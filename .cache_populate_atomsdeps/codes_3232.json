{
  "65459": {
    "text": "// -*- mode: rust; -*-\r\n//\r\n// This file is part of curve25519-dalek.\r\n// Copyright (c) 2016-2021 isis lovecruft\r\n// Copyright (c) 2016-2019 Henry de Valence\r\n// Portions Copyright 2017 Brian Smith\r\n// See LICENSE for licensing information.\r\n//\r\n// Authors:\r\n// - Isis Agora Lovecruft <isis@patternsinthevoid.net>\r\n// - Henry de Valence <hdevalence@hdevalence.ca>\r\n// - Brian Smith <brian@briansmith.org>\r\n\r\n//! Arithmetic on scalars (integers mod the group order).\r\n//!\r\n//! Both the Ristretto group and the Ed25519 basepoint have prime order\r\n//! \\\\( \\ell = 2\\^{252} + 27742317777372353535851937790883648493 \\\\).\r\n//!\r\n//! This code is intended to be useful with both the Ristretto group\r\n//! (where everything is done modulo \\\\( \\ell \\\\)), and the X/Ed25519\r\n//! setting, which mandates specific bit-twiddles that are not\r\n//! well-defined modulo \\\\( \\ell \\\\).\r\n//!\r\n//! All arithmetic on `Scalars` is done modulo \\\\( \\ell \\\\).\r\n//!\r\n//! # Constructing a scalar\r\n//!\r\n//! To create a [`Scalar`](struct.Scalar.html) from a supposedly canonical encoding, use\r\n//! [`Scalar::from_canonical_bytes`](struct.Scalar.html#method.from_canonical_bytes).\r\n//!\r\n//! This function does input validation, ensuring that the input bytes\r\n//! are the canonical encoding of a `Scalar`.\r\n//! If they are, we'll get\r\n//! `Some(Scalar)` in return:\r\n//!\r\n//! ```\r\n//! use curve25519_dalek::scalar::Scalar;\r\n//!\r\n//! let one_as_bytes: [u8; 32] = Scalar::ONE.to_bytes();\r\n//! let a: Option<Scalar> = Scalar::from_canonical_bytes(one_as_bytes).into();\r\n//!\r\n//! assert!(a.is_some());\r\n//! ```\r\n//!\r\n//! However, if we give it bytes representing a scalar larger than \\\\( \\ell \\\\)\r\n//! (in this case, \\\\( \\ell + 2 \\\\)), we'll get `None` back:\r\n//!\r\n//! ```\r\n//! use curve25519_dalek::scalar::Scalar;\r\n//!\r\n//! let l_plus_two_bytes: [u8; 32] = [\r\n//!    0xef, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,\r\n//!    0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,\r\n//!    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\r\n//!    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10,\r\n//! ];\r\n//! let a: Option<Scalar> = Scalar::from_canonical_bytes(l_plus_two_bytes).into();\r\n//!\r\n//! assert!(a.is_none());\r\n//! ```\r\n//!\r\n//! Another way to create a `Scalar` is by reducing a \\\\(256\\\\)-bit integer mod\r\n//! \\\\( \\ell \\\\), for which one may use the\r\n//! [`Scalar::from_bytes_mod_order`](struct.Scalar.html#method.from_bytes_mod_order)\r\n//! method.  In the case of the second example above, this would reduce the\r\n//! resultant scalar \\\\( \\mod \\ell \\\\), producing \\\\( 2 \\\\):\r\n//!\r\n//! ```\r\n//! use curve25519_dalek::scalar::Scalar;\r\n//!\r\n//! let l_plus_two_bytes: [u8; 32] = [\r\n//!    0xef, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,\r\n//!    0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,\r\n//!    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\r\n//!    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10,\r\n//! ];\r\n//! let a: Scalar = Scalar::from_bytes_mod_order(l_plus_two_bytes);\r\n//!\r\n//! let two: Scalar = Scalar::ONE + Scalar::ONE;\r\n//!\r\n//! assert!(a == two);\r\n//! ```\r\n//!\r\n//! There is also a constructor that reduces a \\\\(512\\\\)-bit integer,\r\n//! [`Scalar::from_bytes_mod_order_wide`].\r\n//!\r\n//! To construct a `Scalar` as the hash of some input data, use\r\n//! [`Scalar::hash_from_bytes`], which takes a buffer, or\r\n//! [`Scalar::from_hash`], which allows an IUF API.\r\n//!\r\n// #![cfg_attr(feature = \"digest\", doc = \"```\")]\r\n// #![cfg_attr(not(feature = \"digest\"), doc = \"```ignore\")]\r\n// //! # fn main() {\r\n// //! use sha2::{Digest, Sha512};\r\n// //! use curve25519_dalek::scalar::Scalar;\r\n// //!\r\n// //! // Hashing a single byte slice\r\n// //! let a = Scalar::hash_from_bytes::<Sha512>(b\"Abolish ICE\");\r\n// //!\r\n// //! // Streaming data into a hash object\r\n// //! let mut hasher = Sha512::default();\r\n// //! hasher.update(b\"Abolish \");\r\n// //! hasher.update(b\"ICE\");\r\n// //! let a2 = Scalar::from_hash(hasher);\r\n// //!\r\n// //! assert_eq!(a, a2);\r\n// //! # }\r\n// //! ```\r\n//!\r\n//! See also `Scalar::hash_from_bytes` and `Scalar::from_hash` that\r\n//! reduces a \\\\(512\\\\)-bit integer, if the optional `digest` feature\r\n//! has been enabled.\r\n\r\nuse core::borrow::Borrow;\r\nuse core::fmt::Debug;\r\nuse core::iter::{Product, Sum};\r\nuse core::ops::Index;\r\nuse core::ops::Neg;\r\nuse core::ops::{Add, AddAssign};\r\nuse core::ops::{Mul, MulAssign};\r\nuse core::ops::{Sub, SubAssign};\r\n\r\nuse cfg_if::cfg_if;\r\n\r\n#[cfg(feature = \"group\")]\r\nuse group::ff::{Field, FromUniformBytes, PrimeField};\r\n#[cfg(feature = \"group-bits\")]\r\nuse group::ff::{FieldBits, PrimeFieldBits};\r\n\r\n#[cfg(feature = \"group\")]\r\nuse rand_core::TryRngCore;\r\n\r\n#[cfg(any(test, feature = \"rand_core\"))]\r\nuse rand_core::CryptoRng;\r\n\r\n// #[cfg(feature = \"digest\")]\r\n// use digest::Digest;\r\n// #[cfg(feature = \"digest\")]\r\n// use digest::array::typenum::U64;\r\n\r\nuse subtle::Choice;\r\nuse subtle::ConditionallySelectable;\r\nuse subtle::ConstantTimeEq;\r\nuse subtle::CtOption;\r\n\r\n// #[cfg(feature = \"zeroize\")]\r\n// use zeroize::Zeroize;\r\n\r\nuse crate::backend;\r\nuse crate::constants;\r\n\r\ncfg_if! {\r\n    if #[cfg(curve25519_dalek_backend = \"fiat\")] {\r\n        /// An `UnpackedScalar` represents an element of the field GF(l), optimized for speed.\r\n        ///\r\n        /// This is a type alias for one of the scalar types in the `backend`\r\n        /// module.\r\n        #[cfg(curve25519_dalek_bits = \"32\")]\r\n        #[cfg_attr(\r\n            docsrs,\r\n            doc(cfg(all(feature = \"fiat_backend\", curve25519_dalek_bits = \"32\")))\r\n        )]\r\n        type UnpackedScalar = backend::serial::fiat_u32::scalar::Scalar29;\r\n\r\n        /// An `UnpackedScalar` represents an element of the field GF(l), optimized for speed.\r\n        ///\r\n        /// This is a type alias for one of the scalar types in the `backend`\r\n        /// module.\r\n        #[cfg(curve25519_dalek_bits = \"64\")]\r\n        #[cfg_attr(\r\n            docsrs,\r\n            doc(cfg(all(feature = \"fiat_backend\", curve25519_dalek_bits = \"64\")))\r\n        )]\r\n        type UnpackedScalar = backend::serial::fiat_u64::scalar::Scalar52;\r\n    } else if #[cfg(curve25519_dalek_bits = \"64\")] {\r\n        /// An `UnpackedScalar` represents an element of the field GF(l), optimized for speed.\r\n        ///\r\n        /// This is a type alias for one of the scalar types in the `backend`\r\n        /// module.\r\n        #[cfg_attr(docsrs, doc(cfg(curve25519_dalek_bits = \"64\")))]\r\n        type UnpackedScalar = backend::serial::u64::scalar::Scalar52;\r\n    } else {\r\n        /// An `UnpackedScalar` represents an element of the field GF(l), optimized for speed.\r\n        ///\r\n        /// This is a type alias for one of the scalar types in the `backend`\r\n        /// module.\r\n        #[cfg_attr(docsrs, doc(cfg(curve25519_dalek_bits = \"64\")))]\r\n        type UnpackedScalar = backend::serial::u32::scalar::Scalar29;\r\n    }\r\n}\r\n\r\n/// The `Scalar` struct holds an element of \\\\(\\mathbb Z / \\ell\\mathbb Z \\\\).\r\n#[allow(clippy::derived_hash_with_manual_eq)]\r\n#[derive(Copy, Clone, Hash)]\r\npub struct Scalar {\r\n    /// `bytes` is a little-endian byte encoding of an integer representing a scalar modulo the\r\n    /// group order.\r\n    ///\r\n    /// # Invariant #1\r\n    ///\r\n    /// The integer representing this scalar is less than \\\\(2\\^{255}\\\\). That is, the most\r\n    /// significant bit of `bytes[31]` is 0.\r\n    ///\r\n    /// This is required for `EdwardsPoint` variable- and fixed-base multiplication, because most\r\n    /// integers above 2^255 are unrepresentable in our radix-16 NAF (see [`Self::as_radix_16`]).\r\n    /// The invariant is also required because our `MontgomeryPoint` multiplication assumes the MSB\r\n    /// is 0 (see `MontgomeryPoint::mul`).\r\n    ///\r\n    /// # Invariant #2 (weak)\r\n    ///\r\n    /// The integer representing this scalar is less than \\\\(2\\^{255} - 19 \\\\), i.e., it represents\r\n    /// a canonical representative of an element of \\\\( \\mathbb Z / \\ell\\mathbb Z \\\\). This is\r\n    /// stronger than invariant #1. It also sometimes has to be broken.\r\n    ///\r\n    /// This invariant is deliberately broken in the implementation of `EdwardsPoint::{mul_clamped,\r\n    /// mul_base_clamped}`, `MontgomeryPoint::{mul_clamped, mul_base_clamped}`, and\r\n    /// `BasepointTable::mul_base_clamped`. This is not an issue though. As mentioned above,\r\n    /// scalar-point multiplication is defined for any choice of `bytes` that satisfies invariant\r\n    /// #1. Since clamping guarantees invariant #1 is satisfied, these operations are well defined.\r\n    ///\r\n    /// Note: Scalar-point mult is the _only_ thing you can do safely with an unreduced scalar.\r\n    /// Scalar-scalar addition and subtraction are NOT correct when using unreduced scalars.\r\n    /// Multiplication is correct, but this is only due to a quirk of our implementation, and not\r\n    /// guaranteed to hold in general in the future.\r\n    ///\r\n    /// Note: It is not possible to construct an unreduced `Scalar` from the public API unless the\r\n    /// `legacy_compatibility` is enabled (thus making `Scalar::from_bits` public). Thus, for all\r\n    /// public non-legacy uses, invariant #2\r\n    /// always holds.\r\n    ///\r\n    pub(crate) bytes: [u8; 32],\r\n}\r\n\r\nimpl Scalar {\r\n    /// Construct a `Scalar` by reducing a 256-bit little-endian integer\r\n    /// modulo the group order \\\\( \\ell \\\\).\r\n    pub fn from_bytes_mod_order(bytes: [u8; 32]) -> Scalar {\r\n        // Temporarily allow s_unreduced.bytes > 2^255 ...\r\n        let s_unreduced = Scalar { bytes };\r\n\r\n        // Then reduce mod the group order and return the reduced representative.\r\n        let s = s_unreduced.reduce();\r\n        debug_assert_eq!(0u8, s[31] >> 7);\r\n\r\n        s\r\n    }\r\n\r\n    /// Construct a `Scalar` by reducing a 512-bit little-endian integer\r\n    /// modulo the group order \\\\( \\ell \\\\).\r\n    pub fn from_bytes_mod_order_wide(input: &[u8; 64]) -> Scalar {\r\n        UnpackedScalar::from_bytes_wide(input).pack()\r\n    }\r\n\r\n    /// Attempt to construct a `Scalar` from a canonical byte representation.\r\n    ///\r\n    /// # Return\r\n    ///\r\n    /// - `Some(s)`, where `s` is the `Scalar` corresponding to `bytes`,\r\n    ///   if `bytes` is a canonical byte representation modulo the group order \\\\( \\ell \\\\);\r\n    /// - `None` if `bytes` is not a canonical byte representation.\r\n    pub fn from_canonical_bytes(bytes: [u8; 32]) -> CtOption<Scalar> {\r\n        let high_bit_unset = (bytes[31] >> 7).ct_eq(&0);\r\n        let candidate = Scalar { bytes };\r\n        CtOption::new(candidate, high_bit_unset & candidate.is_canonical())\r\n    }\r\n\r\n    /// Construct a `Scalar` from the low 255 bits of a 256-bit integer. This breaks the invariant\r\n    /// that scalars are always reduced. Scalar-scalar arithmetic, i.e., addition, subtraction,\r\n    /// multiplication, **does not work** on scalars produced from this function. You may only use\r\n    /// the output of this function for `EdwardsPoint::mul`, `MontgomeryPoint::mul`, and\r\n    /// `EdwardsPoint::vartime_double_scalar_mul_basepoint`. **Do not use this function** unless\r\n    /// you absolutely have to.\r\n    #[cfg(feature = \"legacy_compatibility\")]\r\n    pub const fn from_bits(bytes: [u8; 32]) -> Scalar {\r\n        let mut s = Scalar { bytes };\r\n        // Ensure invariant #1 holds. That is, make s < 2^255 by masking the high bit.\r\n        s.bytes[31] &= 0b0111_1111;\r\n\r\n        s\r\n    }\r\n}\r\n\r\nimpl Debug for Scalar {\r\n    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {\r\n        write!(f, \"Scalar{{\\n\\tbytes: {:?},\\n}}\", &self.bytes)\r\n    }\r\n}\r\n\r\nimpl Eq for Scalar {}\r\nimpl PartialEq for Scalar {\r\n    fn eq(&self, other: &Self) -> bool {\r\n        self.ct_eq(other).into()\r\n    }\r\n}\r\n\r\nimpl ConstantTimeEq for Scalar {\r\n    fn ct_eq(&self, other: &Self) -> Choice {\r\n        self.bytes.ct_eq(&other.bytes)\r\n    }\r\n}\r\n\r\nimpl Index<usize> for Scalar {\r\n    type Output = u8;\r\n\r\n    /// Index the bytes of the representative for this `Scalar`.  Mutation is not permitted.\r\n    fn index(&self, _index: usize) -> &u8 {\r\n        &(self.bytes[_index])\r\n    }\r\n}\r\n\r\nimpl<'a> MulAssign<&'a Scalar> for Scalar {\r\n    fn mul_assign(&mut self, _rhs: &'a Scalar) {\r\n        *self = UnpackedScalar::mul(&self.unpack(), &_rhs.unpack()).pack();\r\n    }\r\n}\r\n\r\ndefine_mul_assign_variants!(LHS = Scalar, RHS = Scalar);\r\n\r\nimpl<'a> Mul<&'a Scalar> for &Scalar {\r\n    type Output = Scalar;\r\n    fn mul(self, _rhs: &'a Scalar) -> Scalar {\r\n        UnpackedScalar::mul(&self.unpack(), &_rhs.unpack()).pack()\r\n    }\r\n}\r\n\r\ndefine_mul_variants!(LHS = Scalar, RHS = Scalar, Output = Scalar);\r\n\r\nimpl<'a> AddAssign<&'a Scalar> for Scalar {\r\n    fn add_assign(&mut self, _rhs: &'a Scalar) {\r\n        *self = *self + _rhs;\r\n    }\r\n}\r\n\r\ndefine_add_assign_variants!(LHS = Scalar, RHS = Scalar);\r\n\r\nimpl<'a> Add<&'a Scalar> for &Scalar {\r\n    type Output = Scalar;\r\n    #[allow(non_snake_case)]\r\n    fn add(self, _rhs: &'a Scalar) -> Scalar {\r\n        // The UnpackedScalar::add function produces reduced outputs if the inputs are reduced. By\r\n        // Scalar invariant #1, this is always the case.\r\n        UnpackedScalar::add(&self.unpack(), &_rhs.unpack()).pack()\r\n    }\r\n}\r\n\r\ndefine_add_variants!(LHS = Scalar, RHS = Scalar, Output = Scalar);\r\n\r\nimpl<'a> SubAssign<&'a Scalar> for Scalar {\r\n    fn sub_assign(&mut self, _rhs: &'a Scalar) {\r\n        *self = *self - _rhs;\r\n    }\r\n}\r\n\r\ndefine_sub_assign_variants!(LHS = Scalar, RHS = Scalar);\r\n\r\nimpl<'a> Sub<&'a Scalar> for &Scalar {\r\n    type Output = Scalar;\r\n    #[allow(non_snake_case)]\r\n    fn sub(self, rhs: &'a Scalar) -> Scalar {\r\n        // The UnpackedScalar::sub function produces reduced outputs if the inputs are reduced. By\r\n        // Scalar invariant #1, this is always the case.\r\n        UnpackedScalar::sub(&self.unpack(), &rhs.unpack()).pack()\r\n    }\r\n}\r\n\r\ndefine_sub_variants!(LHS = Scalar, RHS = Scalar, Output = Scalar);\r\n\r\nimpl Neg for &Scalar {\r\n    type Output = Scalar;\r\n    #[allow(non_snake_case)]\r\n    fn neg(self) -> Scalar {\r\n        let self_R = UnpackedScalar::mul_internal(&self.unpack(), &constants::R);\r\n        let self_mod_l = UnpackedScalar::montgomery_reduce(&self_R);\r\n        UnpackedScalar::sub(&UnpackedScalar::ZERO, &self_mod_l).pack()\r\n    }\r\n}\r\n\r\nimpl Neg for Scalar {\r\n    type Output = Scalar;\r\n    fn neg(self) -> Scalar {\r\n        -&self\r\n    }\r\n}\r\n\r\nimpl ConditionallySelectable for Scalar {\r\n    fn conditional_select(a: &Self, b: &Self, choice: Choice) -> Self {\r\n        let mut bytes = [0u8; 32];\r\n        #[allow(clippy::needless_range_loop)]\r\n        for i in 0..32 {\r\n            bytes[i] = u8::conditional_select(&a.bytes[i], &b.bytes[i], choice);\r\n        }\r\n        Scalar { bytes }\r\n    }\r\n}\r\n\r\n#[cfg(feature = \"serde\")]\r\nuse serde::de::Visitor;\r\n#[cfg(feature = \"serde\")]\r\nuse serde::{Deserialize, Deserializer, Serialize, Serializer};\r\n\r\n#[cfg(feature = \"serde\")]\r\n#[cfg_attr(docsrs, doc(cfg(feature = \"serde\")))]\r\nimpl Serialize for Scalar {\r\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\r\n    where\r\n        S: Serializer,\r\n    {\r\n        use serde::ser::SerializeTuple;\r\n        let mut tup = serializer.serialize_tuple(32)?;\r\n        for byte in self.as_bytes().iter() {\r\n            tup.serialize_element(byte)?;\r\n        }\r\n        tup.end()\r\n    }\r\n}\r\n\r\n#[cfg(feature = \"serde\")]\r\n#[cfg_attr(docsrs, doc(cfg(feature = \"serde\")))]\r\nimpl<'de> Deserialize<'de> for Scalar {\r\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\r\n    where\r\n        D: Deserializer<'de>,\r\n    {\r\n        struct ScalarVisitor;\r\n\r\n        impl<'de> Visitor<'de> for ScalarVisitor {\r\n            type Value = Scalar;\r\n\r\n            fn expecting(&self, formatter: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {\r\n                formatter.write_str(\r\n                    \"a sequence of 32 bytes whose little-endian interpretation is less than the \\\r\n                    basepoint order â„“\",\r\n                )\r\n            }\r\n\r\n            fn visit_seq<A>(self, mut seq: A) -> Result<Scalar, A::Error>\r\n            where\r\n                A: serde::de::SeqAccess<'de>,\r\n            {\r\n                let mut bytes = [0u8; 32];\r\n                #[allow(clippy::needless_range_loop)]\r\n                for i in 0..32 {\r\n                    bytes[i] = seq\r\n                        .next_element()?\r\n                        .ok_or_else(|| serde::de::Error::invalid_length(i, &\"expected 32 bytes\"))?;\r\n                }\r\n                Option::from(Scalar::from_canonical_bytes(bytes))\r\n                    .ok_or_else(|| serde::de::Error::custom(\"scalar was not canonically encoded\"))\r\n            }\r\n        }\r\n\r\n        deserializer.deserialize_tuple(32, ScalarVisitor)\r\n    }\r\n}\r\n\r\nimpl<T> Product<T> for Scalar\r\nwhere\r\n    T: Borrow<Scalar>,\r\n{\r\n    fn product<I>(iter: I) -> Self\r\n    where\r\n        I: Iterator<Item = T>,\r\n    {\r\n        iter.fold(Scalar::ONE, |acc, item| acc * item.borrow())\r\n    }\r\n}\r\n\r\nimpl<T> Sum<T> for Scalar\r\nwhere\r\n    T: Borrow<Scalar>,\r\n{\r\n    fn sum<I>(iter: I) -> Self\r\n    where\r\n        I: Iterator<Item = T>,\r\n    {\r\n        iter.fold(Scalar::ZERO, |acc, item| acc + item.borrow())\r\n    }\r\n}\r\n\r\nimpl Default for Scalar {\r\n    fn default() -> Scalar {\r\n        Scalar::ZERO\r\n    }\r\n}\r\n\r\nimpl From<u8> for Scalar {\r\n    fn from(x: u8) -> Scalar {\r\n        let mut s_bytes = [0u8; 32];\r\n        s_bytes[0] = x;\r\n        Scalar { bytes: s_bytes }\r\n    }\r\n}\r\n\r\nimpl From<u16> for Scalar {\r\n    fn from(x: u16) -> Scalar {\r\n        let mut s_bytes = [0u8; 32];\r\n        let x_bytes = x.to_le_bytes();\r\n        s_bytes[0..x_bytes.len()].copy_from_slice(&x_bytes);\r\n        Scalar { bytes: s_bytes }\r\n    }\r\n}\r\n\r\nimpl From<u32> for Scalar {\r\n    fn from(x: u32) -> Scalar {\r\n        let mut s_bytes = [0u8; 32];\r\n        let x_bytes = x.to_le_bytes();\r\n        s_bytes[0..x_bytes.len()].copy_from_slice(&x_bytes);\r\n        Scalar { bytes: s_bytes }\r\n    }\r\n}\r\n\r\nimpl From<u64> for Scalar {\r\n    /// Construct a scalar from the given `u64`.\r\n    ///\r\n    /// # Inputs\r\n    ///\r\n    /// An `u64` to convert to a `Scalar`.\r\n    ///\r\n    /// # Returns\r\n    ///\r\n    /// A `Scalar` corresponding to the input `u64`.\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// use curve25519_dalek::scalar::Scalar;\r\n    ///\r\n    /// let fourtytwo = Scalar::from(42u64);\r\n    /// let six = Scalar::from(6u64);\r\n    /// let seven = Scalar::from(7u64);\r\n    ///\r\n    /// assert!(fourtytwo == six * seven);\r\n    /// ```\r\n    fn from(x: u64) -> Scalar {\r\n        let mut s_bytes = [0u8; 32];\r\n        let x_bytes = x.to_le_bytes();\r\n        s_bytes[0..x_bytes.len()].copy_from_slice(&x_bytes);\r\n        Scalar { bytes: s_bytes }\r\n    }\r\n}\r\n\r\nimpl From<u128> for Scalar {\r\n    fn from(x: u128) -> Scalar {\r\n        let mut s_bytes = [0u8; 32];\r\n        let x_bytes = x.to_le_bytes();\r\n        s_bytes[0..x_bytes.len()].copy_from_slice(&x_bytes);\r\n        Scalar { bytes: s_bytes }\r\n    }\r\n}\r\n\r\n// #[cfg(feature = \"zeroize\")]\r\n// impl Zeroize for Scalar {\r\n//     fn zeroize(&mut self) {\r\n//         self.bytes.zeroize();\r\n//     }\r\n// }\r\n\r\nimpl Scalar {\r\n    /// The scalar \\\\( 0 \\\\).\r\n    pub const ZERO: Self = Self { bytes: [0u8; 32] };\r\n\r\n    /// The scalar \\\\( 1 \\\\).\r\n    pub const ONE: Self = Self {\r\n        bytes: [\r\n            1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n            0, 0, 0,\r\n        ],\r\n    };\r\n\r\n    #[cfg(any(test, feature = \"rand_core\"))]\r\n    /// Return a `Scalar` chosen uniformly at random using a user-provided RNG.\r\n    ///\r\n    /// # Inputs\r\n    ///\r\n    /// * `rng`: any RNG which implements `CryptoRng` interface.\r\n    ///\r\n    /// # Returns\r\n    ///\r\n    /// A random scalar within \\\\(\\mathbb{Z} / \\ell\\mathbb{Z}\\\\).\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// # fn main() {\r\n    /// use curve25519_dalek::scalar::Scalar;\r\n    ///\r\n    /// use rand_core::{OsRng, TryRngCore};\r\n    ///\r\n    /// let mut csprng = OsRng.unwrap_err();\r\n    /// let a: Scalar = Scalar::random(&mut csprng);\r\n    /// # }\r\n    pub fn random<R: CryptoRng + ?Sized>(rng: &mut R) -> Self {\r\n        let mut scalar_bytes = [0u8; 64];\r\n        rng.fill_bytes(&mut scalar_bytes);\r\n        Scalar::from_bytes_mod_order_wide(&scalar_bytes)\r\n    }\r\n\r\n    // #[cfg(feature = \"digest\")]\r\n    // /// Hash a slice of bytes into a scalar.\r\n    // ///\r\n    // /// Takes a type parameter `D`, which is any `Digest` producing 64\r\n    // /// bytes (512 bits) of output.\r\n    // ///\r\n    // /// Convenience wrapper around `from_hash`.\r\n    // ///\r\n    // /// # Example\r\n    // ///\r\n    // #[cfg_attr(feature = \"digest\", doc = \"```\")]\r\n    // #[cfg_attr(not(feature = \"digest\"), doc = \"```ignore\")]\r\n    // /// # use curve25519_dalek::scalar::Scalar;\r\n    // /// use sha2::Sha512;\r\n    // ///\r\n    // /// # // Need fn main() here in comment so the doctest compiles\r\n    // /// # // See https://doc.rust-lang.org/book/documentation.html#documentation-as-tests\r\n    // /// # fn main() {\r\n    // /// let msg = \"To really appreciate architecture, you may even need to commit a murder\";\r\n    // /// let s = Scalar::hash_from_bytes::<Sha512>(msg.as_bytes());\r\n    // /// # }\r\n    // /// ```\r\n    // pub fn hash_from_bytes<D>(input: &[u8]) -> Scalar\r\n    // where\r\n    //     D: Digest<OutputSize = U64> + Default,\r\n    // {\r\n    //     let mut hash = D::default();\r\n    //     hash.update(input);\r\n    //     Scalar::from_hash(hash)\r\n    // }\r\n\r\n    // #[cfg(feature = \"digest\")]\r\n    // /// Construct a scalar from an existing `Digest` instance.\r\n    // ///\r\n    // /// Use this instead of `hash_from_bytes` if it is more convenient\r\n    // /// to stream data into the `Digest` than to pass a single byte\r\n    // /// slice.\r\n    // ///\r\n    // /// # Example\r\n    // ///\r\n    // /// ```\r\n    // /// # use curve25519_dalek::scalar::Scalar;\r\n    // /// use curve25519_dalek::digest::Update;\r\n    // ///\r\n    // /// use sha2::Digest;\r\n    // /// use sha2::Sha512;\r\n    // ///\r\n    // /// # fn main() {\r\n    // /// let mut h = Sha512::new()\r\n    // ///     .chain(\"To really appreciate architecture, you may even need to commit a murder.\")\r\n    // ///     .chain(\"While the programs used for The Manhattan Transcripts are of the most extreme\")\r\n    // ///     .chain(\"nature, they also parallel the most common formula plot: the archetype of\")\r\n    // ///     .chain(\"murder. Other phantasms were occasionally used to underline the fact that\")\r\n    // ///     .chain(\"perhaps all architecture, rather than being about functional standards, is\")\r\n    // ///     .chain(\"about love and death.\");\r\n    // ///\r\n    // /// let s = Scalar::from_hash(h);\r\n    // ///\r\n    // /// println!(\"{:?}\", s.to_bytes());\r\n    // /// assert_eq!(\r\n    // ///     s.to_bytes(),\r\n    // ///     [  21,  88, 208, 252,  63, 122, 210, 152,\r\n    // ///       154,  38,  15,  23,  16, 167,  80, 150,\r\n    // ///       192, 221,  77, 226,  62,  25, 224, 148,\r\n    // ///       239,  48, 176,  10, 185,  69, 168,  11, ],\r\n    // /// );\r\n    // /// # }\r\n    // /// ```\r\n    // pub fn from_hash<D>(hash: D) -> Scalar\r\n    // where\r\n    //     D: Digest<OutputSize = U64>,\r\n    // {\r\n    //     let mut output = [0u8; 64];\r\n    //     output.copy_from_slice(hash.finalize().as_slice());\r\n    //     Scalar::from_bytes_mod_order_wide(&output)\r\n    // }\r\n\r\n    /// Convert this `Scalar` to its underlying sequence of bytes.\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// use curve25519_dalek::scalar::Scalar;\r\n    ///\r\n    /// let s: Scalar = Scalar::ZERO;\r\n    ///\r\n    /// assert!(s.to_bytes() == [0u8; 32]);\r\n    /// ```\r\n    pub const fn to_bytes(&self) -> [u8; 32] {\r\n        self.bytes\r\n    }\r\n\r\n    /// View the little-endian byte encoding of the integer representing this Scalar.\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// use curve25519_dalek::scalar::Scalar;\r\n    ///\r\n    /// let s: Scalar = Scalar::ZERO;\r\n    ///\r\n    /// assert!(s.as_bytes() == &[0u8; 32]);\r\n    /// ```\r\n    pub const fn as_bytes(&self) -> &[u8; 32] {\r\n        &self.bytes\r\n    }\r\n\r\n    /// Given a nonzero `Scalar`, compute its multiplicative inverse.\r\n    ///\r\n    /// # Warning\r\n    ///\r\n    /// `self` **MUST** be nonzero.  If you cannot\r\n    /// *prove* that this is the case, you **SHOULD NOT USE THIS\r\n    /// FUNCTION**.\r\n    ///\r\n    /// # Returns\r\n    ///\r\n    /// The multiplicative inverse of the this `Scalar`.\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// use curve25519_dalek::scalar::Scalar;\r\n    ///\r\n    /// // x = 2238329342913194256032495932344128051776374960164957527413114840482143558222\r\n    /// let X: Scalar = Scalar::from_bytes_mod_order([\r\n    ///         0x4e, 0x5a, 0xb4, 0x34, 0x5d, 0x47, 0x08, 0x84,\r\n    ///         0x59, 0x13, 0xb4, 0x64, 0x1b, 0xc2, 0x7d, 0x52,\r\n    ///         0x52, 0xa5, 0x85, 0x10, 0x1b, 0xcc, 0x42, 0x44,\r\n    ///         0xd4, 0x49, 0xf4, 0xa8, 0x79, 0xd9, 0xf2, 0x04,\r\n    ///     ]);\r\n    /// // 1/x = 6859937278830797291664592131120606308688036382723378951768035303146619657244\r\n    /// let XINV: Scalar = Scalar::from_bytes_mod_order([\r\n    ///         0x1c, 0xdc, 0x17, 0xfc, 0xe0, 0xe9, 0xa5, 0xbb,\r\n    ///         0xd9, 0x24, 0x7e, 0x56, 0xbb, 0x01, 0x63, 0x47,\r\n    ///         0xbb, 0xba, 0x31, 0xed, 0xd5, 0xa9, 0xbb, 0x96,\r\n    ///         0xd5, 0x0b, 0xcd, 0x7a, 0x3f, 0x96, 0x2a, 0x0f,\r\n    ///     ]);\r\n    ///\r\n    /// let inv_X: Scalar = X.invert();\r\n    /// assert!(XINV == inv_X);\r\n    /// let should_be_one: Scalar = &inv_X * &X;\r\n    /// assert!(should_be_one == Scalar::ONE);\r\n    /// ```\r\n    pub fn invert(&self) -> Scalar {\r\n        self.unpack().invert().pack()\r\n    }\r\n\r\n    /// Given a slice of nonzero (possibly secret) `Scalar`s,\r\n    /// compute their inverses in a batch.\r\n    ///\r\n    /// # Return\r\n    ///\r\n    /// Each element of `inputs` is replaced by its inverse.\r\n    ///\r\n    /// The product of all inverses is returned.\r\n    ///\r\n    /// # Warning\r\n    ///\r\n    /// All input `Scalars` **MUST** be nonzero.  If you cannot\r\n    /// *prove* that this is the case, you **SHOULD NOT USE THIS\r\n    /// FUNCTION**.\r\n    ///\r\n    /// # Example\r\n    ///\r\n    /// ```\r\n    /// # use curve25519_dalek::scalar::Scalar;\r\n    /// # fn main() {\r\n    /// let mut scalars = [\r\n    ///     Scalar::from(3u64),\r\n    ///     Scalar::from(5u64),\r\n    ///     Scalar::from(7u64),\r\n    ///     Scalar::from(11u64),\r\n    /// ];\r\n    ///\r\n    /// let allinv = Scalar::batch_invert(&mut scalars);\r\n    ///\r\n    /// assert_eq!(allinv, Scalar::from(3*5*7*11u64).invert());\r\n    /// assert_eq!(scalars[0], Scalar::from(3u64).invert());\r\n    /// assert_eq!(scalars[1], Scalar::from(5u64).invert());\r\n    /// assert_eq!(scalars[2], Scalar::from(7u64).invert());\r\n    /// assert_eq!(scalars[3], Scalar::from(11u64).invert());\r\n    /// # }\r\n    /// ```\r\n    #[cfg(feature = \"alloc\")]\r\n    pub fn batch_invert(inputs: &mut [Scalar]) -> Scalar {\r\n        // This code is essentially identical to the FieldElement\r\n        // implementation, and is documented there.  Unfortunately,\r\n        // it's not easy to write it generically, since here we want\r\n        // to use `UnpackedScalar`s internally, and `Scalar`s\r\n        // externally, but there's no corresponding distinction for\r\n        // field elements.\r\n\r\n        let n = inputs.len();\r\n        let one: UnpackedScalar = Scalar::ONE.unpack().as_montgomery();\r\n\r\n        let mut scratch = vec![one; n];\r\n\r\n        // Keep an accumulator of all of the previous products\r\n        let mut acc = Scalar::ONE.unpack().as_montgomery();\r\n\r\n        // Pass through the input vector, recording the previous\r\n        // products in the scratch space\r\n        for (input, scratch) in inputs.iter_mut().zip(scratch.iter_mut()) {\r\n            *scratch = acc;\r\n\r\n            // Avoid unnecessary Montgomery multiplication in second pass by\r\n            // keeping inputs in Montgomery form\r\n            let tmp = input.unpack().as_montgomery();\r\n            *input = tmp.pack();\r\n            acc = UnpackedScalar::montgomery_mul(&acc, &tmp);\r\n        }\r\n\r\n        // acc is nonzero iff all inputs are nonzero\r\n        debug_assert!(acc.pack() != Scalar::ZERO);\r\n\r\n        // Compute the inverse of all products\r\n        acc = acc.montgomery_invert().from_montgomery();\r\n\r\n        // We need to return the product of all inverses later\r\n        let ret = acc.pack();\r\n\r\n        // Pass through the vector backwards to compute the inverses\r\n        // in place\r\n        for (input, scratch) in inputs.iter_mut().rev().zip(scratch.iter().rev()) {\r\n            let tmp = UnpackedScalar::montgomery_mul(&acc, &input.unpack());\r\n            *input = UnpackedScalar::montgomery_mul(&acc, scratch).pack();\r\n            acc = tmp;\r\n        }\r\n\r\n        // #[cfg(feature = \"zeroize\")]\r\n        // Zeroize::zeroize(&mut scratch);\r\n\r\n        ret\r\n    }\r\n\r\n    /// Get the bits of the scalar, in little-endian order\r\n    pub(crate) fn bits_le(&self) -> impl DoubleEndedIterator<Item = bool> + '_ {\r\n        (0..256).map(|i| {\r\n            // As i runs from 0..256, the bottom 3 bits index the bit, while the upper bits index\r\n            // the byte. Since self.bytes is little-endian at the byte level, this iterator is\r\n            // little-endian on the bit level\r\n            ((self.bytes[i >> 3] >> (i & 7)) & 1u8) == 1\r\n        })\r\n    }\r\n\r\n    /// Compute a width-\\\\(w\\\\) \"Non-Adjacent Form\" of this scalar.\r\n    ///\r\n    /// A width-\\\\(w\\\\) NAF of a positive integer \\\\(k\\\\) is an expression\r\n    /// $$\r\n    /// k = \\sum_{i=0}\\^m n\\_i 2\\^i,\r\n    /// $$\r\n    /// where each nonzero\r\n    /// coefficient \\\\(n\\_i\\\\) is odd and bounded by \\\\(|n\\_i| < 2\\^{w-1}\\\\),\r\n    /// \\\\(n\\_{m-1}\\\\) is nonzero, and at most one of any \\\\(w\\\\) consecutive\r\n    /// coefficients is nonzero.  (Hankerson, Menezes, Vanstone; def 3.32).\r\n    ///\r\n    /// The length of the NAF is at most one more than the length of\r\n    /// the binary representation of \\\\(k\\\\).  This is why the\r\n    /// `Scalar` type maintains an invariant (invariant #1) that the top bit is\r\n    /// \\\\(0\\\\), so that the NAF of a scalar has at most 256 digits.\r\n    ///\r\n    /// Intuitively, this is like a binary expansion, except that we\r\n    /// allow some coefficients to grow in magnitude up to\r\n    /// \\\\(2\\^{w-1}\\\\) so that the nonzero coefficients are as sparse\r\n    /// as possible.\r\n    ///\r\n    /// When doing scalar multiplication, we can then use a lookup\r\n    /// table of precomputed multiples of a point to add the nonzero\r\n    /// terms \\\\( k_i P \\\\).  Using signed digits cuts the table size\r\n    /// in half, and using odd digits cuts the table size in half\r\n    /// again.\r\n    ///\r\n    /// To compute a \\\\(w\\\\)-NAF, we use a modification of Algorithm 3.35 of HMV:\r\n    ///\r\n    /// 1. \\\\( i \\gets 0 \\\\)\r\n    /// 2. While \\\\( k \\ge 1 \\\\):\r\n    ///     1. If \\\\(k\\\\) is odd, \\\\( n_i \\gets k \\operatorname{mods} 2^w \\\\), \\\\( k \\gets k - n_i \\\\).\r\n    ///     2. If \\\\(k\\\\) is even, \\\\( n_i \\gets 0 \\\\).\r\n    ///     3. \\\\( k \\gets k / 2 \\\\), \\\\( i \\gets i + 1 \\\\).\r\n    /// 3. Return \\\\( n_0, n_1, ... , \\\\)\r\n    ///\r\n    /// Here \\\\( \\bar x = x \\operatorname{mods} 2^w \\\\) means the\r\n    /// \\\\( \\bar x \\\\) with \\\\( \\bar x \\equiv x \\pmod{2^w} \\\\) and\r\n    /// \\\\( -2^{w-1} \\leq \\bar x < 2^{w-1} \\\\).\r\n    ///\r\n    /// We implement this by scanning across the bits of \\\\(k\\\\) from\r\n    /// least-significant bit to most-significant-bit.\r\n    /// Write the bits of \\\\(k\\\\) as\r\n    /// $$\r\n    /// k = \\sum\\_{i=0}\\^m k\\_i 2^i,\r\n    /// $$\r\n    /// and split the sum as\r\n    /// $$\r\n    /// k = \\sum\\_{i=0}^{w-1} k\\_i 2^i + 2^w \\sum\\_{i=0} k\\_{i+w} 2^i\r\n    /// $$\r\n    /// where the first part is \\\\( k \\mod 2^w \\\\).\r\n    ///\r\n    /// If \\\\( k \\mod 2^w\\\\) is odd, and \\\\( k \\mod 2^w < 2^{w-1} \\\\), then we emit\r\n    /// \\\\( n_0 = k \\mod 2^w \\\\).  Instead of computing\r\n    /// \\\\( k - n_0 \\\\), we just advance \\\\(w\\\\) bits and reindex.\r\n    ///\r\n    /// If \\\\( k \\mod 2^w\\\\) is odd, and \\\\( k \\mod 2^w \\ge 2^{w-1} \\\\), then\r\n    /// \\\\( n_0 = k \\operatorname{mods} 2^w = k \\mod 2^w - 2^w \\\\).\r\n    /// The quantity \\\\( k - n_0 \\\\) is\r\n    /// $$\r\n    /// \\begin{aligned}\r\n    /// k - n_0 &= \\sum\\_{i=0}^{w-1} k\\_i 2^i + 2^w \\sum\\_{i=0} k\\_{i+w} 2^i\r\n    ///          - \\sum\\_{i=0}^{w-1} k\\_i 2^i + 2^w \\\\\\\\\r\n    /// &= 2^w + 2^w \\sum\\_{i=0} k\\_{i+w} 2^i\r\n    /// \\end{aligned}\r\n    /// $$\r\n    /// so instead of computing the subtraction, we can set a carry\r\n    /// bit, advance \\\\(w\\\\) bits, and reindex.\r\n    ///\r\n    /// If \\\\( k \\mod 2^w\\\\) is even, we emit \\\\(0\\\\), advance 1 bit\r\n    /// and reindex.  In fact, by setting all digits to \\\\(0\\\\)\r\n    /// initially, we don't need to emit anything.\r\n    pub(crate) fn non_adjacent_form(&self, w: usize) -> [i8; 256] {\r\n        // required by the NAF definition\r\n        debug_assert!(w >= 2);\r\n        // required so that the NAF digits fit in i8\r\n        debug_assert!(w <= 8);\r\n\r\n        let mut naf = [0i8; 256];\r\n\r\n        let mut x_u64 = [0u64; 5];\r\n        read_le_u64_into(&self.bytes, &mut x_u64[0..4]);\r\n\r\n        let width = 1 << w;\r\n        let window_mask = width - 1;\r\n\r\n        let mut pos = 0;\r\n        let mut carry = 0;\r\n        while pos < 256 {\r\n            // Construct a buffer of bits of the scalar, starting at bit `pos`\r\n            let u64_idx = pos / 64;\r\n            let bit_idx = pos % 64;\r\n            let bit_buf: u64 = if bit_idx < 64 - w {\r\n                // This window's bits are contained in a single u64\r\n                x_u64[u64_idx] >> bit_idx\r\n            } else {\r\n                // Combine the current u64's bits with the bits from the next u64\r\n                (x_u64[u64_idx] >> bit_idx) | (x_u64[1 + u64_idx] << (64 - bit_idx))\r\n            };\r\n\r\n            // Add the carry into the current window\r\n            let window = carry + (bit_buf & window_mask);\r\n\r\n            if window & 1 == 0 {\r\n                // If the window value is even, preserve the carry and continue.\r\n                // Why is the carry preserved?\r\n                // If carry == 0 and window & 1 == 0, then the next carry should be 0\r\n                // If carry == 1 and window & 1 == 0, then bit_buf & 1 == 1 so the next carry should be 1\r\n                pos += 1;\r\n                continue;\r\n            }\r\n\r\n            if window < width / 2 {\r\n                carry = 0;\r\n                naf[pos] = window as i8;\r\n            } else {\r\n                carry = 1;\r\n                naf[pos] = (window as i8).wrapping_sub(width as i8);\r\n            }\r\n\r\n            pos += w;\r\n        }\r\n\r\n        naf\r\n    }\r\n\r\n    /// Write this scalar in radix 16, with coefficients in \\\\([-8,8)\\\\),\r\n    /// i.e., compute \\\\(a\\_i\\\\) such that\r\n    /// $$\r\n    ///    a = a\\_0 + a\\_1 16\\^1 + \\cdots + a_{63} 16\\^{63},\r\n    /// $$\r\n    /// with \\\\(-8 \\leq a_i < 8\\\\) for \\\\(0 \\leq i < 63\\\\) and \\\\(-8 \\leq a_{63} \\leq 8\\\\).\r\n    ///\r\n    /// The largest value that can be decomposed like this is just over \\\\(2^{255}\\\\). Thus, in\r\n    /// order to not error, the top bit MUST NOT be set, i.e., `Self` MUST be less than\r\n    /// \\\\(2^{255}\\\\).\r\n    pub(crate) fn as_radix_16(&self) -> [i8; 64] {\r\n        debug_assert!(self[31] <= 127);\r\n        let mut output = [0i8; 64];\r\n\r\n        // Step 1: change radix.\r\n        // Convert from radix 256 (bytes) to radix 16 (nibbles)\r\n        #[allow(clippy::identity_op)]\r\n        #[inline(always)]\r\n        fn bot_half(x: u8) -> u8 {\r\n            (x >> 0) & 15\r\n        }\r\n        #[inline(always)]\r\n        fn top_half(x: u8) -> u8 {\r\n            (x >> 4) & 15\r\n        }\r\n\r\n        for i in 0..32 {\r\n            output[2 * i] = bot_half(self[i]) as i8;\r\n            output[2 * i + 1] = top_half(self[i]) as i8;\r\n        }\r\n        // Precondition note: since self[31] <= 127, output[63] <= 7\r\n\r\n        // Step 2: recenter coefficients from [0,16) to [-8,8)\r\n        for i in 0..63 {\r\n            let carry = (output[i] + 8) >> 4;\r\n            output[i] -= carry << 4;\r\n            output[i + 1] += carry;\r\n        }\r\n        // Precondition note: output[63] is not recentered.  It\r\n        // increases by carry <= 1.  Thus output[63] <= 8.\r\n\r\n        output\r\n    }\r\n\r\n    /// Returns a size hint indicating how many entries of the return\r\n    /// value of `to_radix_2w` are nonzero.\r\n    #[cfg(any(feature = \"alloc\", all(test, feature = \"precomputed-tables\")))]\r\n    pub(crate) fn to_radix_2w_size_hint(w: usize) -> usize {\r\n        debug_assert!(w >= 4);\r\n        debug_assert!(w <= 8);\r\n\r\n        let digits_count = match w {\r\n            4..=7 => 256_usize.div_ceil(w),\r\n            // See comment in to_radix_2w on handling the terminal carry.\r\n            8 => 256_usize.div_ceil(w) + 1_usize,\r\n            _ => panic!(\"invalid radix parameter\"),\r\n        };\r\n\r\n        debug_assert!(digits_count <= 64);\r\n        digits_count\r\n    }\r\n\r\n    /// Creates a representation of a Scalar in radix \\\\( 2^w \\\\) with \\\\(w = 4, 5, 6, 7, 8\\\\) for\r\n    /// use with the Pippenger algorithm. Higher radixes are not supported to save cache space.\r\n    /// Radix 256 is near-optimal even for very large inputs.\r\n    ///\r\n    /// Radix below 16 or above 256 is prohibited.\r\n    /// This method returns digits in a fixed-sized array, excess digits are zeroes.\r\n    ///\r\n    /// For radix 16, `Self` must be less than \\\\(2^{255}\\\\). This is because most integers larger\r\n    /// than \\\\(2^{255}\\\\) are unrepresentable in the form described below for \\\\(w = 4\\\\). This\r\n    /// would be true for \\\\(w = 8\\\\) as well, but it is compensated for by increasing the size\r\n    /// hint by 1.\r\n    ///\r\n    /// ## Scalar representation\r\n    ///\r\n    /// Radix \\\\(2\\^w\\\\), with \\\\(n = ceil(256/w)\\\\) coefficients in \\\\([-(2\\^w)/2,(2\\^w)/2)\\\\),\r\n    /// i.e., scalar is represented using digits \\\\(a\\_i\\\\) such that\r\n    /// $$\r\n    ///    a = a\\_0 + a\\_1 2\\^1w + \\cdots + a_{n-1} 2\\^{w*(n-1)},\r\n    /// $$\r\n    /// with \\\\(-2\\^w/2 \\leq a_i < 2\\^w/2\\\\) for \\\\(0 \\leq i < (n-1)\\\\) and \\\\(-2\\^w/2 \\leq a_{n-1} \\leq 2\\^w/2\\\\).\r\n    ///\r\n    #[cfg(any(feature = \"alloc\", feature = \"precomputed-tables\"))]\r\n    pub(crate) fn as_radix_2w(&self, w: usize) -> [i8; 64] {\r\n        debug_assert!(w >= 4);\r\n        debug_assert!(w <= 8);\r\n\r\n        if w == 4 {\r\n            return self.as_radix_16();\r\n        }\r\n\r\n        // Scalar formatted as four `u64`s with carry bit packed into the highest bit.\r\n        let mut scalar64x4 = [0u64; 4];\r\n        read_le_u64_into(&self.bytes, &mut scalar64x4[0..4]);\r\n\r\n        let radix: u64 = 1 << w;\r\n        let window_mask: u64 = radix - 1;\r\n\r\n        let mut carry = 0u64;\r\n        let mut digits = [0i8; 64];\r\n        let digits_count = 256_usize.div_ceil(w);\r\n        #[allow(clippy::needless_range_loop)]\r\n        for i in 0..digits_count {\r\n            // Construct a buffer of bits of the scalar, starting at `bit_offset`.\r\n            let bit_offset = i * w;\r\n            let u64_idx = bit_offset / 64;\r\n            let bit_idx = bit_offset % 64;\r\n\r\n            // Read the bits from the scalar\r\n            let bit_buf: u64 = if bit_idx < 64 - w || u64_idx == 3 {\r\n                // This window's bits are contained in a single u64,\r\n                // or it's the last u64 anyway.\r\n                scalar64x4[u64_idx] >> bit_idx\r\n            } else {\r\n                // Combine the current u64's bits with the bits from the next u64\r\n                (scalar64x4[u64_idx] >> bit_idx) | (scalar64x4[1 + u64_idx] << (64 - bit_idx))\r\n            };\r\n\r\n            // Read the actual coefficient value from the window\r\n            let coef = carry + (bit_buf & window_mask); // coef = [0, 2^r)\r\n\r\n            // Recenter coefficients from [0,2^w) to [-2^w/2, 2^w/2)\r\n            carry = (coef + (radix / 2)) >> w;\r\n            digits[i] = ((coef as i64) - (carry << w) as i64) as i8;\r\n        }\r\n\r\n        // When 4 < w < 8, we can fold the final carry onto the last digit d,\r\n        // because d < 2^w/2 so d + carry*2^w = d + 1*2^w < 2^(w+1) < 2^8.\r\n        //\r\n        // When w = 8, we can't fit carry*2^w into an i8.  This should\r\n        // not happen anyways, because the final carry will be 0 for\r\n        // reduced scalars, but Scalar invariant #1 allows 255-bit scalars.\r\n        // To handle this, we expand the size_hint by 1 when w=8,\r\n        // and accumulate the final carry onto another digit.\r\n        match w {\r\n            8 => digits[digits_count] += carry as i8,\r\n            _ => digits[digits_count - 1] += (carry << w) as i8,\r\n        }\r\n\r\n        digits\r\n    }\r\n\r\n    /// Unpack this `Scalar` to an `UnpackedScalar` for faster arithmetic.\r\n    pub(crate) fn unpack(&self) -> UnpackedScalar {\r\n        UnpackedScalar::from_bytes(&self.bytes)\r\n    }\r\n\r\n    /// Reduce this `Scalar` modulo \\\\(\\ell\\\\).\r\n    #[allow(non_snake_case)]\r\n    fn reduce(&self) -> Scalar {\r\n        let x = self.unpack();\r\n        let xR = UnpackedScalar::mul_internal(&x, &constants::R);\r\n        let x_mod_l = UnpackedScalar::montgomery_reduce(&xR);\r\n        x_mod_l.pack()\r\n    }\r\n\r\n    /// Check whether this `Scalar` is the canonical representative mod \\\\(\\ell\\\\). This is not\r\n    /// public because any `Scalar` that is publicly observed is reduced, by scalar invariant #2.\r\n    fn is_canonical(&self) -> Choice {\r\n        self.ct_eq(&self.reduce())\r\n    }\r\n}\r\n\r\nimpl UnpackedScalar {\r\n    /// Pack the limbs of this `UnpackedScalar` into a `Scalar`.\r\n    fn pack(&self) -> Scalar {\r\n        Scalar {\r\n            bytes: self.to_bytes(),\r\n        }\r\n    }\r\n\r\n    /// Inverts an UnpackedScalar in Montgomery form.\r\n    #[rustfmt::skip] // keep alignment of addition chain and squarings\r\n    #[allow(clippy::just_underscores_and_digits)]\r\n    pub fn montgomery_invert(&self) -> UnpackedScalar {\r\n        // Uses the addition chain from\r\n        // https://briansmith.org/ecc-inversion-addition-chains-01#curve25519_scalar_inversion\r\n        let    _1 = *self;\r\n        let   _10 = _1.montgomery_square();\r\n        let  _100 = _10.montgomery_square();\r\n        let   _11 = UnpackedScalar::montgomery_mul(&_10,     &_1);\r\n        let  _101 = UnpackedScalar::montgomery_mul(&_10,    &_11);\r\n        let  _111 = UnpackedScalar::montgomery_mul(&_10,   &_101);\r\n        let _1001 = UnpackedScalar::montgomery_mul(&_10,   &_111);\r\n        let _1011 = UnpackedScalar::montgomery_mul(&_10,  &_1001);\r\n        let _1111 = UnpackedScalar::montgomery_mul(&_100, &_1011);\r\n\r\n        // _10000\r\n        let mut y = UnpackedScalar::montgomery_mul(&_1111, &_1);\r\n\r\n        #[inline]\r\n        fn square_multiply(y: &mut UnpackedScalar, squarings: usize, x: &UnpackedScalar) {\r\n            for _ in 0..squarings {\r\n                *y = y.montgomery_square();\r\n            }\r\n            *y = UnpackedScalar::montgomery_mul(y, x);\r\n        }\r\n\r\n        square_multiply(&mut y, 123 + 3, &_101);\r\n        square_multiply(&mut y,   2 + 2, &_11);\r\n        square_multiply(&mut y,   1 + 4, &_1111);\r\n        square_multiply(&mut y,   1 + 4, &_1111);\r\n        square_multiply(&mut y,       4, &_1001);\r\n        square_multiply(&mut y,       2, &_11);\r\n        square_multiply(&mut y,   1 + 4, &_1111);\r\n        square_multiply(&mut y,   1 + 3, &_101);\r\n        square_multiply(&mut y,   3 + 3, &_101);\r\n        square_multiply(&mut y,       3, &_111);\r\n        square_multiply(&mut y,   1 + 4, &_1111);\r\n        square_multiply(&mut y,   2 + 3, &_111);\r\n        square_multiply(&mut y,   2 + 2, &_11);\r\n        square_multiply(&mut y,   1 + 4, &_1011);\r\n        square_multiply(&mut y,   2 + 4, &_1011);\r\n        square_multiply(&mut y,   6 + 4, &_1001);\r\n        square_multiply(&mut y,   2 + 2, &_11);\r\n        square_multiply(&mut y,   3 + 2, &_11);\r\n        square_multiply(&mut y,   3 + 2, &_11);\r\n        square_multiply(&mut y,   1 + 4, &_1001);\r\n        square_multiply(&mut y,   1 + 3, &_111);\r\n        square_multiply(&mut y,   2 + 4, &_1111);\r\n        square_multiply(&mut y,   1 + 4, &_1011);\r\n        square_multiply(&mut y,       3, &_101);\r\n        square_multiply(&mut y,   2 + 4, &_1111);\r\n        square_multiply(&mut y,       3, &_101);\r\n        square_multiply(&mut y,   1 + 2, &_11);\r\n\r\n        y\r\n    }\r\n\r\n    /// Inverts an UnpackedScalar not in Montgomery form.\r\n    pub fn invert(&self) -> UnpackedScalar {\r\n        self.as_montgomery().montgomery_invert().from_montgomery()\r\n    }\r\n}\r\n\r\n#[cfg(feature = \"group\")]\r\nimpl Field for Scalar {\r\n    const ZERO: Self = Self::ZERO;\r\n    const ONE: Self = Self::ONE;\r\n\r\n    fn try_from_rng<R: TryRngCore + ?Sized>(rng: &mut R) -> Result<Self, R::Error> {\r\n        // NOTE: this is duplicated due to different `rng` bounds\r\n        let mut scalar_bytes = [0u8; 64];\r\n        rng.try_fill_bytes(&mut scalar_bytes)?;\r\n        Ok(Self::from_bytes_mod_order_wide(&scalar_bytes))\r\n    }\r\n\r\n    fn square(&self) -> Self {\r\n        self * self\r\n    }\r\n\r\n    fn double(&self) -> Self {\r\n        self + self\r\n    }\r\n\r\n    fn invert(&self) -> CtOption<Self> {\r\n        CtOption::new(self.invert(), !self.is_zero())\r\n    }\r\n\r\n    fn sqrt_ratio(num: &Self, div: &Self) -> (Choice, Self) {\r\n        #[allow(unused_qualifications)]\r\n        group::ff::helpers::sqrt_ratio_generic(num, div)\r\n    }\r\n\r\n    fn sqrt(&self) -> CtOption<Self> {\r\n        #[allow(unused_qualifications)]\r\n        group::ff::helpers::sqrt_tonelli_shanks(\r\n            self,\r\n            [\r\n                0xcb02_4c63_4b9e_ba7d,\r\n                0x029b_df3b_d45e_f39a,\r\n                0x0000_0000_0000_0000,\r\n                0x0200_0000_0000_0000,\r\n            ],\r\n        )\r\n    }\r\n}\r\n\r\n#[cfg(feature = \"group\")]\r\nimpl PrimeField for Scalar {\r\n    type Repr = [u8; 32];\r\n\r\n    fn from_repr(repr: Self::Repr) -> CtOption<Self> {\r\n        Self::from_canonical_bytes(repr)\r\n    }\r\n\r\n    fn from_repr_vartime(repr: Self::Repr) -> Option<Self> {\r\n        // Check that the high bit is not set\r\n        if (repr[31] >> 7) != 0u8 {\r\n            return None;\r\n        }\r\n\r\n        let candidate = Scalar { bytes: repr };\r\n\r\n        if candidate == candidate.reduce() {\r\n            Some(candidate)\r\n        } else {\r\n            None\r\n        }\r\n    }\r\n\r\n    fn to_repr(&self) -> Self::Repr {\r\n        self.to_bytes()\r\n    }\r\n\r\n    fn is_odd(&self) -> Choice {\r\n        Choice::from(self.as_bytes()[0] & 1)\r\n    }\r\n\r\n    const MODULUS: &'static str =\r\n        \"0x1000000000000000000000000000000014def9dea2f79cd65812631a5cf5d3ed\";\r\n    const NUM_BITS: u32 = 253;\r\n    const CAPACITY: u32 = 252;\r\n\r\n    const TWO_INV: Self = Self {\r\n        bytes: [\r\n            0xf7, 0xe9, 0x7a, 0x2e, 0x8d, 0x31, 0x09, 0x2c, 0x6b, 0xce, 0x7b, 0x51, 0xef, 0x7c,\r\n            0x6f, 0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\r\n            0x00, 0x00, 0x00, 0x08,\r\n        ],\r\n    };\r\n    const MULTIPLICATIVE_GENERATOR: Self = Self {\r\n        bytes: [\r\n            2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n            0, 0, 0,\r\n        ],\r\n    };\r\n    const S: u32 = 2;\r\n    const ROOT_OF_UNITY: Self = Self {\r\n        bytes: [\r\n            0xd4, 0x07, 0xbe, 0xeb, 0xdf, 0x75, 0x87, 0xbe, 0xfe, 0x83, 0xce, 0x42, 0x53, 0x56,\r\n            0xf0, 0x0e, 0x7a, 0xc2, 0xc1, 0xab, 0x60, 0x6d, 0x3d, 0x7d, 0xe7, 0x81, 0x79, 0xe0,\r\n            0x10, 0x73, 0x4a, 0x09,\r\n        ],\r\n    };\r\n    const ROOT_OF_UNITY_INV: Self = Self {\r\n        bytes: [\r\n            0x19, 0xcc, 0x37, 0x71, 0x3a, 0xed, 0x8a, 0x99, 0xd7, 0x18, 0x29, 0x60, 0x8b, 0xa3,\r\n            0xee, 0x05, 0x86, 0x3d, 0x3e, 0x54, 0x9f, 0x92, 0xc2, 0x82, 0x18, 0x7e, 0x86, 0x1f,\r\n            0xef, 0x8c, 0xb5, 0x06,\r\n        ],\r\n    };\r\n    const DELTA: Self = Self {\r\n        bytes: [\r\n            16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n            0, 0, 0,\r\n        ],\r\n    };\r\n}\r\n\r\n#[cfg(feature = \"group-bits\")]\r\nimpl PrimeFieldBits for Scalar {\r\n    type ReprBits = [u8; 32];\r\n\r\n    fn to_le_bits(&self) -> FieldBits<Self::ReprBits> {\r\n        self.to_repr().into()\r\n    }\r\n\r\n    fn char_le_bits() -> FieldBits<Self::ReprBits> {\r\n        constants::BASEPOINT_ORDER.to_bytes().into()\r\n    }\r\n}\r\n\r\n#[cfg(feature = \"group\")]\r\nimpl FromUniformBytes<64> for Scalar {\r\n    fn from_uniform_bytes(bytes: &[u8; 64]) -> Self {\r\n        Scalar::from_bytes_mod_order_wide(bytes)\r\n    }\r\n}\r\n\r\n/// Read one or more u64s stored as little endian bytes.\r\n///\r\n/// ## Panics\r\n/// Panics if `src.len() != 8 * dst.len()`.\r\nfn read_le_u64_into(src: &[u8], dst: &mut [u64]) {\r\n    assert!(\r\n        src.len() == 8 * dst.len(),\r\n        \"src.len() = {}, dst.len() = {}\",\r\n        src.len(),\r\n        dst.len()\r\n    );\r\n    for (bytes, val) in src.chunks(8).zip(dst.iter_mut()) {\r\n        *val = u64::from_le_bytes(\r\n            bytes\r\n                .try_into()\r\n                .expect(\"Incorrect src length, should be 8 * dst.len()\"),\r\n        );\r\n    }\r\n}\r\n\r\n/// _Clamps_ the given little-endian representation of a 32-byte integer. Clamping the value puts\r\n/// it in the range:\r\n///\r\n/// **n âˆˆ 2^254 + 8\\*{0, 1, 2, 3, . . ., 2^251 âˆ’ 1}**\r\n///\r\n/// # Explanation of clamping\r\n///\r\n/// For Curve25519, h = 8, and multiplying by 8 is the same as a binary left-shift by 3 bits.\r\n/// If you take a secret scalar value between 2^251 and 2^252 â€“ 1 and left-shift by 3 bits\r\n/// then you end up with a 255-bit number with the most significant bit set to 1 and\r\n/// the least-significant three bits set to 0.\r\n///\r\n/// The Curve25519 clamping operation takes **an arbitrary 256-bit random value** and\r\n/// clears the most-significant bit (making it a 255-bit number), sets the next bit, and then\r\n/// clears the 3 least-significant bits. In other words, it directly creates a scalar value that is\r\n/// in the right form and pre-multiplied by the cofactor.\r\n///\r\n/// See [here](https://neilmadden.blog/2020/05/28/whats-the-curve25519-clamping-all-about/) for\r\n/// more details.\r\n#[must_use]\r\npub const fn clamp_integer(mut bytes: [u8; 32]) -> [u8; 32] {\r\n    bytes[0] &= 0b1111_1000;\r\n    bytes[31] &= 0b0111_1111;\r\n    bytes[31] |= 0b0100_0000;\r\n    bytes\r\n}\r\n\r\n// #[cfg(test)]\r\n// pub(crate) mod test {\r\n//     use super::*;\r\n//     use rand_core::RngCore;\r\n\r\n//     #[cfg(feature = \"alloc\")]\r\n//     use alloc::vec::Vec;\r\n\r\n//     /// x = 2238329342913194256032495932344128051776374960164957527413114840482143558222\r\n//     pub static X: Scalar = Scalar {\r\n//         bytes: [\r\n//             0x4e, 0x5a, 0xb4, 0x34, 0x5d, 0x47, 0x08, 0x84, 0x59, 0x13, 0xb4, 0x64, 0x1b, 0xc2,\r\n//             0x7d, 0x52, 0x52, 0xa5, 0x85, 0x10, 0x1b, 0xcc, 0x42, 0x44, 0xd4, 0x49, 0xf4, 0xa8,\r\n//             0x79, 0xd9, 0xf2, 0x04,\r\n//         ],\r\n//     };\r\n//     /// 1/x = 6859937278830797291664592131120606308688036382723378951768035303146619657244\r\n//     pub static XINV: Scalar = Scalar {\r\n//         bytes: [\r\n//             0x1c, 0xdc, 0x17, 0xfc, 0xe0, 0xe9, 0xa5, 0xbb, 0xd9, 0x24, 0x7e, 0x56, 0xbb, 0x01,\r\n//             0x63, 0x47, 0xbb, 0xba, 0x31, 0xed, 0xd5, 0xa9, 0xbb, 0x96, 0xd5, 0x0b, 0xcd, 0x7a,\r\n//             0x3f, 0x96, 0x2a, 0x0f,\r\n//         ],\r\n//     };\r\n//     /// y = 2592331292931086675770238855846338635550719849568364935475441891787804997264\r\n//     pub static Y: Scalar = Scalar {\r\n//         bytes: [\r\n//             0x90, 0x76, 0x33, 0xfe, 0x1c, 0x4b, 0x66, 0xa4, 0xa2, 0x8d, 0x2d, 0xd7, 0x67, 0x83,\r\n//             0x86, 0xc3, 0x53, 0xd0, 0xde, 0x54, 0x55, 0xd4, 0xfc, 0x9d, 0xe8, 0xef, 0x7a, 0xc3,\r\n//             0x1f, 0x35, 0xbb, 0x05,\r\n//         ],\r\n//     };\r\n\r\n//     /// The largest scalar that satisfies invariant #1, i.e., the largest scalar with the top bit\r\n//     /// set to 0. Since this scalar violates invariant #2, i.e., it's greater than the modulus `l`,\r\n//     /// addition and subtraction are broken. The only thing you can do with this is scalar-point\r\n//     /// multiplication (and actually also scalar-scalar multiplication, but that's just a quirk of\r\n//     /// our implementation).\r\n//     pub(crate) static LARGEST_UNREDUCED_SCALAR: Scalar = Scalar {\r\n//         bytes: [\r\n//             0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\r\n//             0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\r\n//             0xff, 0xff, 0xff, 0x7f,\r\n//         ],\r\n//     };\r\n\r\n//     /// x*y = 5690045403673944803228348699031245560686958845067437804563560795922180092780\r\n//     static X_TIMES_Y: Scalar = Scalar {\r\n//         bytes: [\r\n//             0x6c, 0x33, 0x74, 0xa1, 0x89, 0x4f, 0x62, 0x21, 0x0a, 0xaa, 0x2f, 0xe1, 0x86, 0xa6,\r\n//             0xf9, 0x2c, 0xe0, 0xaa, 0x75, 0xc2, 0x77, 0x95, 0x81, 0xc2, 0x95, 0xfc, 0x08, 0x17,\r\n//             0x9a, 0x73, 0x94, 0x0c,\r\n//         ],\r\n//     };\r\n\r\n//     /// sage: l = 2^252 + 27742317777372353535851937790883648493\r\n//     /// sage: big = 2^256 - 1\r\n//     /// sage: repr((big % l).digits(256))\r\n//     static CANONICAL_2_256_MINUS_1: Scalar = Scalar {\r\n//         bytes: [\r\n//             28, 149, 152, 141, 116, 49, 236, 214, 112, 207, 125, 115, 244, 91, 239, 198, 254, 255,\r\n//             255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 15,\r\n//         ],\r\n//     };\r\n\r\n//     static A_SCALAR: Scalar = Scalar {\r\n//         bytes: [\r\n//             0x1a, 0x0e, 0x97, 0x8a, 0x90, 0xf6, 0x62, 0x2d, 0x37, 0x47, 0x02, 0x3f, 0x8a, 0xd8,\r\n//             0x26, 0x4d, 0xa7, 0x58, 0xaa, 0x1b, 0x88, 0xe0, 0x40, 0xd1, 0x58, 0x9e, 0x7b, 0x7f,\r\n//             0x23, 0x76, 0xef, 0x09,\r\n//         ],\r\n//     };\r\n\r\n//     static A_NAF: [i8; 256] = [\r\n//         0, 13, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, -9, 0, 0, 0, 0, -11, 0, 0, 0, 0, 3, 0, 0,\r\n//         0, 0, 1, 0, 0, 0, 0, 9, 0, 0, 0, 0, -5, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 0, 0, 0,\r\n//         11, 0, 0, 0, 0, 0, -9, 0, 0, 0, 0, 0, -3, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\r\n//         0, -1, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, -15, 0, 0, 0, 0, -7, 0, 0, 0, 0, -9, 0, 0, 0, 0, 0, 5,\r\n//         0, 0, 0, 0, 13, 0, 0, 0, 0, 0, -3, 0, 0, 0, 0, -11, 0, 0, 0, 0, -7, 0, 0, 0, 0, -13, 0, 0,\r\n//         0, 0, 11, 0, 0, 0, 0, -9, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -15, 0, 0, 0, 0, 1, 0, 0, 0, 0,\r\n//         7, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 15,\r\n//         0, 0, 0, 0, 0, -9, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, -15, 0,\r\n//         0, 0, 0, 0, 15, 0, 0, 0, 0, 15, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\r\n//     ];\r\n\r\n//     const BASEPOINT_ORDER_MINUS_ONE: Scalar = Scalar {\r\n//         bytes: [\r\n//             0xec, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58, 0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9,\r\n//             0xde, 0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\r\n//             0x00, 0x00, 0x00, 0x10,\r\n//         ],\r\n//     };\r\n\r\n//     /// The largest clamped integer\r\n//     static LARGEST_CLAMPED_INTEGER: [u8; 32] = clamp_integer(LARGEST_UNREDUCED_SCALAR.bytes);\r\n\r\n//     #[test]\r\n//     fn fuzzer_testcase_reduction() {\r\n//         // LE bytes of 24519928653854221733733552434404946937899825954937634815\r\n//         let a_bytes = [\r\n//             255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\r\n//             255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n//         ];\r\n//         // LE bytes of 4975441334397345751130612518500927154628011511324180036903450236863266160640\r\n//         let b_bytes = [\r\n//             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 210, 210,\r\n//             210, 255, 255, 255, 255, 10,\r\n//         ];\r\n//         // LE bytes of 6432735165214683820902750800207468552549813371247423777071615116673864412038\r\n//         let c_bytes = [\r\n//             134, 171, 119, 216, 180, 128, 178, 62, 171, 132, 32, 62, 34, 119, 104, 193, 47, 215,\r\n//             181, 250, 14, 207, 172, 93, 75, 207, 211, 103, 144, 204, 56, 14,\r\n//         ];\r\n\r\n//         let a = Scalar::from_bytes_mod_order(a_bytes);\r\n//         let b = Scalar::from_bytes_mod_order(b_bytes);\r\n//         let c = Scalar::from_bytes_mod_order(c_bytes);\r\n\r\n//         let mut tmp = [0u8; 64];\r\n\r\n//         // also_a = (a mod l)\r\n//         tmp[0..32].copy_from_slice(&a_bytes[..]);\r\n//         let also_a = Scalar::from_bytes_mod_order_wide(&tmp);\r\n\r\n//         // also_b = (b mod l)\r\n//         tmp[0..32].copy_from_slice(&b_bytes[..]);\r\n//         let also_b = Scalar::from_bytes_mod_order_wide(&tmp);\r\n\r\n//         let expected_c = a * b;\r\n//         let also_expected_c = also_a * also_b;\r\n\r\n//         assert_eq!(c, expected_c);\r\n//         assert_eq!(c, also_expected_c);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn non_adjacent_form_test_vector() {\r\n//         let naf = A_SCALAR.non_adjacent_form(5);\r\n//         for i in 0..256 {\r\n//             assert_eq!(naf[i], A_NAF[i]);\r\n//         }\r\n//     }\r\n\r\n//     fn non_adjacent_form_iter(w: usize, x: &Scalar) {\r\n//         let naf = x.non_adjacent_form(w);\r\n\r\n//         // Reconstruct the scalar from the computed NAF\r\n//         let mut y = Scalar::ZERO;\r\n//         for i in (0..256).rev() {\r\n//             y += y;\r\n//             let digit = if naf[i] < 0 {\r\n//                 -Scalar::from((-naf[i]) as u64)\r\n//             } else {\r\n//                 Scalar::from(naf[i] as u64)\r\n//             };\r\n//             y += digit;\r\n//         }\r\n\r\n//         assert_eq!(*x, y);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn non_adjacent_form_random() {\r\n//         let mut rng = rand::rng();\r\n//         for _ in 0..1_000 {\r\n//             let x = Scalar::random(&mut rng);\r\n//             for w in &[5, 6, 7, 8] {\r\n//                 non_adjacent_form_iter(*w, &x);\r\n//             }\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn from_u64() {\r\n//         let val: u64 = 0xdeadbeefdeadbeef;\r\n//         let s = Scalar::from(val);\r\n//         assert_eq!(s[7], 0xde);\r\n//         assert_eq!(s[6], 0xad);\r\n//         assert_eq!(s[5], 0xbe);\r\n//         assert_eq!(s[4], 0xef);\r\n//         assert_eq!(s[3], 0xde);\r\n//         assert_eq!(s[2], 0xad);\r\n//         assert_eq!(s[1], 0xbe);\r\n//         assert_eq!(s[0], 0xef);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn scalar_mul_by_one() {\r\n//         let test_scalar = X * Scalar::ONE;\r\n//         for i in 0..32 {\r\n//             assert!(test_scalar[i] == X[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn add_reduces() {\r\n//         // Check that addition wraps around the modulus\r\n//         assert_eq!(BASEPOINT_ORDER_MINUS_ONE + Scalar::ONE, Scalar::ZERO);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn sub_reduces() {\r\n//         // Check that subtraction wraps around the modulus\r\n//         assert_eq!(Scalar::ZERO - Scalar::ONE, BASEPOINT_ORDER_MINUS_ONE);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn impl_add() {\r\n//         let two = Scalar::from(2u64);\r\n//         let one = Scalar::ONE;\r\n//         let should_be_two = one + one;\r\n//         assert_eq!(should_be_two, two);\r\n//     }\r\n\r\n//     #[allow(non_snake_case)]\r\n//     #[test]\r\n//     fn impl_mul() {\r\n//         let should_be_X_times_Y = X * Y;\r\n//         assert_eq!(should_be_X_times_Y, X_TIMES_Y);\r\n//     }\r\n\r\n//     #[allow(non_snake_case)]\r\n//     #[test]\r\n//     #[cfg(feature = \"alloc\")]\r\n//     fn impl_product() {\r\n//         // Test that product works for non-empty iterators\r\n//         let X_Y_vector = [X, Y];\r\n//         let should_be_X_times_Y: Scalar = X_Y_vector.iter().product();\r\n//         assert_eq!(should_be_X_times_Y, X_TIMES_Y);\r\n\r\n//         // Test that product works for the empty iterator\r\n//         let one = Scalar::ONE;\r\n//         let empty_vector = [];\r\n//         let should_be_one: Scalar = empty_vector.iter().product();\r\n//         assert_eq!(should_be_one, one);\r\n\r\n//         // Test that product works for iterators where Item = Scalar\r\n//         let xs = [Scalar::from(2u64); 10];\r\n//         let ys = [Scalar::from(3u64); 10];\r\n//         // now zs is an iterator with Item = Scalar\r\n//         let zs = xs.iter().zip(ys.iter()).map(|(x, y)| x * y);\r\n\r\n//         let x_prod: Scalar = xs.iter().product();\r\n//         let y_prod: Scalar = ys.iter().product();\r\n//         let z_prod: Scalar = zs.product();\r\n\r\n//         assert_eq!(x_prod, Scalar::from(1024u64));\r\n//         assert_eq!(y_prod, Scalar::from(59049u64));\r\n//         assert_eq!(z_prod, Scalar::from(60466176u64));\r\n//         assert_eq!(x_prod * y_prod, z_prod);\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"alloc\")]\r\n//     fn impl_sum() {\r\n//         // Test that sum works for non-empty iterators\r\n//         let two = Scalar::from(2u64);\r\n//         let one_vector = [Scalar::ONE, Scalar::ONE];\r\n//         let should_be_two: Scalar = one_vector.iter().sum();\r\n//         assert_eq!(should_be_two, two);\r\n\r\n//         // Test that sum works for the empty iterator\r\n//         let zero = Scalar::ZERO;\r\n//         let empty_vector = [];\r\n//         let should_be_zero: Scalar = empty_vector.iter().sum();\r\n//         assert_eq!(should_be_zero, zero);\r\n\r\n//         // Test that sum works for owned types\r\n//         let xs = [Scalar::from(1u64); 10];\r\n//         let ys = [Scalar::from(2u64); 10];\r\n//         // now zs is an iterator with Item = Scalar\r\n//         let zs = xs.iter().zip(ys.iter()).map(|(x, y)| x + y);\r\n\r\n//         let x_sum: Scalar = xs.iter().sum();\r\n//         let y_sum: Scalar = ys.iter().sum();\r\n//         let z_sum: Scalar = zs.sum();\r\n\r\n//         assert_eq!(x_sum, Scalar::from(10u64));\r\n//         assert_eq!(y_sum, Scalar::from(20u64));\r\n//         assert_eq!(z_sum, Scalar::from(30u64));\r\n//         assert_eq!(x_sum + y_sum, z_sum);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn square() {\r\n//         let expected = X * X;\r\n//         let actual = X.unpack().square().pack();\r\n//         for i in 0..32 {\r\n//             assert!(expected[i] == actual[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn reduce() {\r\n//         let biggest = Scalar::from_bytes_mod_order([0xff; 32]);\r\n//         assert_eq!(biggest, CANONICAL_2_256_MINUS_1);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn from_bytes_mod_order_wide() {\r\n//         let mut bignum = [0u8; 64];\r\n//         // set bignum = x + 2^256x\r\n//         for i in 0..32 {\r\n//             bignum[i] = X[i];\r\n//             bignum[32 + i] = X[i];\r\n//         }\r\n//         // 3958878930004874126169954872055634648693766179881526445624823978500314864344\r\n//         // = x + 2^256x (mod l)\r\n//         let reduced = Scalar {\r\n//             bytes: [\r\n//                 216, 154, 179, 139, 210, 121, 2, 71, 69, 99, 158, 216, 23, 173, 63, 100, 204, 0,\r\n//                 91, 50, 219, 153, 57, 249, 28, 82, 31, 197, 100, 165, 192, 8,\r\n//             ],\r\n//         };\r\n//         let test_red = Scalar::from_bytes_mod_order_wide(&bignum);\r\n//         for i in 0..32 {\r\n//             assert!(test_red[i] == reduced[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[allow(non_snake_case)]\r\n//     #[test]\r\n//     fn invert() {\r\n//         let inv_X = X.invert();\r\n//         assert_eq!(inv_X, XINV);\r\n//         let should_be_one = inv_X * X;\r\n//         assert_eq!(should_be_one, Scalar::ONE);\r\n//     }\r\n\r\n//     // Negating a scalar twice should result in the original scalar.\r\n//     #[allow(non_snake_case)]\r\n//     #[test]\r\n//     fn neg_twice_is_identity() {\r\n//         let negative_X = -&X;\r\n//         let should_be_X = -&negative_X;\r\n\r\n//         assert_eq!(should_be_X, X);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn to_bytes_from_bytes_roundtrips() {\r\n//         let unpacked = X.unpack();\r\n//         let bytes = unpacked.to_bytes();\r\n//         let should_be_unpacked = UnpackedScalar::from_bytes(&bytes);\r\n\r\n//         assert_eq!(should_be_unpacked.limbs, unpacked.limbs);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn montgomery_reduce_matches_from_bytes_mod_order_wide() {\r\n//         let mut bignum = [0u8; 64];\r\n\r\n//         // set bignum = x + 2^256x\r\n//         for i in 0..32 {\r\n//             bignum[i] = X[i];\r\n//             bignum[32 + i] = X[i];\r\n//         }\r\n//         // x + 2^256x (mod l)\r\n//         //         = 3958878930004874126169954872055634648693766179881526445624823978500314864344\r\n//         let expected = Scalar {\r\n//             bytes: [\r\n//                 216, 154, 179, 139, 210, 121, 2, 71, 69, 99, 158, 216, 23, 173, 63, 100, 204, 0,\r\n//                 91, 50, 219, 153, 57, 249, 28, 82, 31, 197, 100, 165, 192, 8,\r\n//             ],\r\n//         };\r\n//         let reduced = Scalar::from_bytes_mod_order_wide(&bignum);\r\n\r\n//         // The reduced scalar should match the expected\r\n//         assert_eq!(reduced.bytes, expected.bytes);\r\n\r\n//         //  (x + 2^256x) * R\r\n//         let interim =\r\n//             UnpackedScalar::mul_internal(&UnpackedScalar::from_bytes_wide(&bignum), &constants::R);\r\n//         // ((x + 2^256x) * R) / R  (mod l)\r\n//         let montgomery_reduced = UnpackedScalar::montgomery_reduce(&interim);\r\n\r\n//         // The Montgomery reduced scalar should match the reduced one, as well as the expected\r\n//         assert_eq!(montgomery_reduced.limbs, reduced.unpack().limbs);\r\n//         assert_eq!(montgomery_reduced.limbs, expected.unpack().limbs)\r\n//     }\r\n\r\n//     #[test]\r\n//     fn canonical_decoding() {\r\n//         // canonical encoding of 1667457891\r\n//         let canonical_bytes = [\r\n//             99, 99, 99, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n//             0, 0, 0, 0,\r\n//         ];\r\n\r\n//         // encoding of\r\n//         //   7265385991361016183439748078976496179028704920197054998554201349516117938192\r\n//         // = 28380414028753969466561515933501938171588560817147392552250411230663687203 (mod l)\r\n//         // non_canonical because unreduced mod l\r\n//         let non_canonical_bytes_because_unreduced = [16; 32];\r\n\r\n//         // encoding with high bit set, to check that the parser isn't pre-masking the high bit\r\n//         let non_canonical_bytes_because_highbit = [\r\n//             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n//             0, 0, 128,\r\n//         ];\r\n\r\n//         assert!(bool::from(\r\n//             Scalar::from_canonical_bytes(canonical_bytes).is_some()\r\n//         ));\r\n//         assert!(bool::from(\r\n//             Scalar::from_canonical_bytes(non_canonical_bytes_because_unreduced).is_none()\r\n//         ));\r\n//         assert!(bool::from(\r\n//             Scalar::from_canonical_bytes(non_canonical_bytes_because_highbit).is_none()\r\n//         ));\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"serde\")]\r\n//     fn serde_bincode_scalar_roundtrip() {\r\n//         use bincode;\r\n//         let encoded = bincode::serialize(&X).unwrap();\r\n//         let parsed: Scalar = bincode::deserialize(&encoded).unwrap();\r\n//         assert_eq!(parsed, X);\r\n\r\n//         // Check that the encoding is 32 bytes exactly\r\n//         assert_eq!(encoded.len(), 32);\r\n\r\n//         // Check that the encoding itself matches the usual one\r\n//         assert_eq!(X, bincode::deserialize(X.as_bytes()).unwrap(),);\r\n//     }\r\n\r\n//     #[cfg(all(debug_assertions, feature = \"alloc\"))]\r\n//     #[test]\r\n//     #[should_panic]\r\n//     fn batch_invert_with_a_zero_input_panics() {\r\n//         let mut xs = vec![Scalar::ONE; 16];\r\n//         xs[3] = Scalar::ZERO;\r\n//         // This should panic in debug mode.\r\n//         Scalar::batch_invert(&mut xs);\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"alloc\")]\r\n//     fn batch_invert_empty() {\r\n//         assert_eq!(Scalar::ONE, Scalar::batch_invert(&mut []));\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"alloc\")]\r\n//     fn batch_invert_consistency() {\r\n//         let mut x = Scalar::from(1u64);\r\n//         let mut v1: Vec<_> = (0..16)\r\n//             .map(|_| {\r\n//                 let tmp = x;\r\n//                 x = x + x;\r\n//                 tmp\r\n//             })\r\n//             .collect();\r\n//         let v2 = v1.clone();\r\n\r\n//         let expected: Scalar = v1.iter().product();\r\n//         let expected = expected.invert();\r\n//         let ret = Scalar::batch_invert(&mut v1);\r\n//         assert_eq!(ret, expected);\r\n\r\n//         for (a, b) in v1.iter().zip(v2.iter()) {\r\n//             assert_eq!(a * b, Scalar::ONE);\r\n//         }\r\n//     }\r\n\r\n//     #[cfg(feature = \"precomputed-tables\")]\r\n//     fn test_pippenger_radix_iter(scalar: Scalar, w: usize) {\r\n//         let digits_count = Scalar::to_radix_2w_size_hint(w);\r\n//         let digits = scalar.as_radix_2w(w);\r\n\r\n//         let radix = Scalar::from((1 << w) as u64);\r\n//         let mut term = Scalar::ONE;\r\n//         let mut recovered_scalar = Scalar::ZERO;\r\n//         for digit in &digits[0..digits_count] {\r\n//             let digit = *digit;\r\n//             if digit != 0 {\r\n//                 let sdigit = if digit < 0 {\r\n//                     -Scalar::from((-(digit as i64)) as u64)\r\n//                 } else {\r\n//                     Scalar::from(digit as u64)\r\n//                 };\r\n//                 recovered_scalar += term * sdigit;\r\n//             }\r\n//             term *= radix;\r\n//         }\r\n//         // When the input is unreduced, we may only recover the scalar mod l.\r\n//         assert_eq!(recovered_scalar, scalar.reduce());\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"precomputed-tables\")]\r\n//     fn test_pippenger_radix() {\r\n//         use core::iter;\r\n//         // For each valid radix it tests that 1000 random-ish scalars can be restored\r\n//         // from the produced representation precisely.\r\n//         let cases = (2..100)\r\n//             .map(|s| Scalar::from(s as u64).invert())\r\n//             // The largest unreduced scalar, s = 2^255-1. This is not reduced mod l. Scalar mult\r\n//             // still works though.\r\n//             .chain(iter::once(LARGEST_UNREDUCED_SCALAR));\r\n\r\n//         for scalar in cases {\r\n//             test_pippenger_radix_iter(scalar, 6);\r\n//             test_pippenger_radix_iter(scalar, 7);\r\n//             test_pippenger_radix_iter(scalar, 8);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     #[cfg(feature = \"alloc\")]\r\n//     fn test_read_le_u64_into() {\r\n//         let cases: &[(&[u8], &[u64])] = &[\r\n//             (\r\n//                 &[0xFE, 0xEF, 0x10, 0x01, 0x1F, 0xF1, 0x0F, 0xF0],\r\n//                 &[0xF00F_F11F_0110_EFFE],\r\n//             ),\r\n//             (\r\n//                 &[\r\n//                     0xFE, 0xEF, 0x10, 0x01, 0x1F, 0xF1, 0x0F, 0xF0, 0x12, 0x34, 0x56, 0x78, 0x9A,\r\n//                     0xBC, 0xDE, 0xF0,\r\n//                 ],\r\n//                 &[0xF00F_F11F_0110_EFFE, 0xF0DE_BC9A_7856_3412],\r\n//             ),\r\n//         ];\r\n\r\n//         for (src, expected) in cases {\r\n//             let mut dst = vec![0; expected.len()];\r\n//             read_le_u64_into(src, &mut dst);\r\n\r\n//             assert_eq!(&dst, expected, \"Expected {:x?} got {:x?}\", expected, dst);\r\n//         }\r\n//     }\r\n\r\n//     // Tests consistency of From<{integer}> impls for Scalar\r\n//     #[test]\r\n//     fn test_scalar_from_int() {\r\n//         let s1 = Scalar::ONE;\r\n\r\n//         // For `x` in `u8`, `u16`, `u32`, `u64`, and `u128`, check that\r\n//         // `Scalar::from(x + 1) == Scalar::from(x) + Scalar::from(1)`\r\n\r\n//         let x = 0x23u8;\r\n//         let sx = Scalar::from(x);\r\n//         assert_eq!(sx + s1, Scalar::from(x + 1));\r\n\r\n//         let x = 0x2323u16;\r\n//         let sx = Scalar::from(x);\r\n//         assert_eq!(sx + s1, Scalar::from(x + 1));\r\n\r\n//         let x = 0x2323_2323u32;\r\n//         let sx = Scalar::from(x);\r\n//         assert_eq!(sx + s1, Scalar::from(x + 1));\r\n\r\n//         let x = 0x2323_2323_2323_2323u64;\r\n//         let sx = Scalar::from(x);\r\n//         assert_eq!(sx + s1, Scalar::from(x + 1));\r\n\r\n//         let x = 0x2323_2323_2323_2323_2323_2323_2323_2323u128;\r\n//         let sx = Scalar::from(x);\r\n//         assert_eq!(sx + s1, Scalar::from(x + 1));\r\n//     }\r\n\r\n//     #[cfg(feature = \"group\")]\r\n//     #[test]\r\n//     fn ff_constants() {\r\n//         assert_eq!(Scalar::from(2u64) * Scalar::TWO_INV, Scalar::ONE);\r\n\r\n//         assert_eq!(\r\n//             Scalar::ROOT_OF_UNITY * Scalar::ROOT_OF_UNITY_INV,\r\n//             Scalar::ONE,\r\n//         );\r\n\r\n//         // ROOT_OF_UNITY^{2^s} mod m == 1\r\n//         assert_eq!(\r\n//             Scalar::ROOT_OF_UNITY.pow(&[1u64 << Scalar::S, 0, 0, 0]),\r\n//             Scalar::ONE,\r\n//         );\r\n\r\n//         // DELTA^{t} mod m == 1\r\n//         assert_eq!(\r\n//             Scalar::DELTA.pow(&[\r\n//                 0x9604_98c6_973d_74fb,\r\n//                 0x0537_be77_a8bd_e735,\r\n//                 0x0000_0000_0000_0000,\r\n//                 0x0400_0000_0000_0000,\r\n//             ]),\r\n//             Scalar::ONE,\r\n//         );\r\n//     }\r\n\r\n//     #[cfg(feature = \"group\")]\r\n//     #[test]\r\n//     fn ff_impls() {\r\n//         assert!(bool::from(Scalar::ZERO.is_even()));\r\n//         assert!(bool::from(Scalar::ONE.is_odd()));\r\n//         assert!(bool::from(Scalar::from(2u64).is_even()));\r\n//         assert!(bool::from(Scalar::DELTA.is_even()));\r\n\r\n//         assert!(bool::from(Field::invert(&Scalar::ZERO).is_none()));\r\n//         assert_eq!(Field::invert(&X).unwrap(), XINV);\r\n\r\n//         let x_sq = X.square();\r\n//         // We should get back either the positive or negative root.\r\n//         assert!([X, -X].contains(&x_sq.sqrt().unwrap()));\r\n\r\n//         assert_eq!(Scalar::from_repr_vartime(X.to_repr()), Some(X));\r\n//         assert_eq!(Scalar::from_repr_vartime([0xff; 32]), None);\r\n\r\n//         assert_eq!(Scalar::from_repr(X.to_repr()).unwrap(), X);\r\n//         assert!(bool::from(Scalar::from_repr([0xff; 32]).is_none()));\r\n//     }\r\n\r\n//     #[test]\r\n//     #[should_panic]\r\n//     fn test_read_le_u64_into_should_panic_on_bad_input() {\r\n//         let mut dst = [0_u64; 1];\r\n//         // One byte short\r\n//         read_le_u64_into(&[0xFE, 0xEF, 0x10, 0x01, 0x1F, 0xF1, 0x0F], &mut dst);\r\n//     }\r\n\r\n//     #[test]\r\n//     fn test_scalar_clamp() {\r\n//         let input = A_SCALAR.bytes;\r\n//         let expected = [\r\n//             0x18, 0x0e, 0x97, 0x8a, 0x90, 0xf6, 0x62, 0x2d, 0x37, 0x47, 0x02, 0x3f, 0x8a, 0xd8,\r\n//             0x26, 0x4d, 0xa7, 0x58, 0xaa, 0x1b, 0x88, 0xe0, 0x40, 0xd1, 0x58, 0x9e, 0x7b, 0x7f,\r\n//             0x23, 0x76, 0xef, 0x49,\r\n//         ];\r\n//         let actual = clamp_integer(input);\r\n//         assert_eq!(actual, expected);\r\n\r\n//         let expected = [\r\n//             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n//             0, 0, 0x40,\r\n//         ];\r\n//         let actual = clamp_integer([0; 32]);\r\n//         assert_eq!(expected, actual);\r\n//         let expected = [\r\n//             0xf8, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\r\n//             0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\r\n//             0xff, 0xff, 0xff, 0x7f,\r\n//         ];\r\n//         let actual = clamp_integer([0xff; 32]);\r\n//         assert_eq!(actual, expected);\r\n\r\n//         assert_eq!(\r\n//             LARGEST_CLAMPED_INTEGER,\r\n//             clamp_integer(LARGEST_CLAMPED_INTEGER)\r\n//         );\r\n//     }\r\n\r\n//     // Check that a * b == a.reduce() * a.reduce() for ANY scalars a,b, even ones that violate\r\n//     // invariant #1, i.e., a,b > 2^255. Old versions of ed25519-dalek did multiplication where a\r\n//     // was reduced and b was clamped and unreduced. This checks that was always well-defined.\r\n//     #[test]\r\n//     fn test_mul_reduction_invariance() {\r\n//         let mut rng = rand::rng();\r\n\r\n//         for _ in 0..10 {\r\n//             // Also define c that's clamped. We'll make sure that clamping doesn't affect\r\n//             // computation\r\n//             let (a, b, c) = {\r\n//                 let mut a_bytes = [0u8; 32];\r\n//                 let mut b_bytes = [0u8; 32];\r\n//                 let mut c_bytes = [0u8; 32];\r\n//                 rng.fill_bytes(&mut a_bytes);\r\n//                 rng.fill_bytes(&mut b_bytes);\r\n//                 rng.fill_bytes(&mut c_bytes);\r\n//                 (\r\n//                     Scalar { bytes: a_bytes },\r\n//                     Scalar { bytes: b_bytes },\r\n//                     Scalar {\r\n//                         bytes: clamp_integer(c_bytes),\r\n//                     },\r\n//                 )\r\n//             };\r\n\r\n//             // Make sure this is the same product no matter how you cut it\r\n//             let reduced_mul_ab = a.reduce() * b.reduce();\r\n//             let reduced_mul_ac = a.reduce() * c.reduce();\r\n//             assert_eq!(a * b, reduced_mul_ab);\r\n//             assert_eq!(a.reduce() * b, reduced_mul_ab);\r\n//             assert_eq!(a * b.reduce(), reduced_mul_ab);\r\n//             assert_eq!(a * c, reduced_mul_ac);\r\n//             assert_eq!(a.reduce() * c, reduced_mul_ac);\r\n//             assert_eq!(a * c.reduce(), reduced_mul_ac);\r\n//         }\r\n//     }\r\n// }",
    "filename": "scalar_toptest.rs",
    "filepath": null,
    "folder_id": null,
    "user_id": 460184
  }
}