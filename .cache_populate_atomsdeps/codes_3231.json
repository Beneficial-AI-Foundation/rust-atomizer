{
  "65458": {
    "text": "//! Arithmetic mod \\\\(2\\^{252} + 27742317777372353535851937790883648493\\\\)\r\n//! with five \\\\(52\\\\)-bit unsigned limbs.\r\n//!\r\n//! \\\\(51\\\\)-bit limbs would cover the desired bit range (\\\\(253\\\\)\r\n//! bits), but isn't large enough to reduce a \\\\(512\\\\)-bit number with\r\n//! Montgomery multiplication, so \\\\(52\\\\) bits is used instead.  To see\r\n//! that this is safe for intermediate results, note that the largest\r\n//! limb in a \\\\(5\\times 5\\\\) product of \\\\(52\\\\)-bit limbs will be\r\n//!\r\n//! ```text\r\n//! (0xfffffffffffff^2) * 5 = 0x4ffffffffffff60000000000005 (107 bits).\r\n//! ```\r\n\r\nuse core::fmt::Debug;\r\nuse core::ops::{Index, IndexMut};\r\nuse subtle::Choice;\r\n\r\n// #[cfg(feature = \"zeroize\")]\r\n// use zeroize::Zeroize;\r\n\r\nuse crate::constants;\r\n\r\n#[allow(unused_imports)]\r\nuse super::scalar_lemmas::*;\r\n#[allow(unused_imports)]\r\nuse super::scalar_specs::*;\r\nuse super::subtle_assumes::*;\r\n#[allow(unused_imports)]\r\nuse vstd::arithmetic::div_mod::*;\r\n#[allow(unused_imports)]\r\nuse vstd::arithmetic::power2::*;\r\nuse vstd::prelude::*;\r\n\r\nverus! {\r\n/// The `Scalar52` struct represents an element in\r\n/// \\\\(\\mathbb Z / \\ell \\mathbb Z\\\\) as 5 \\\\(52\\\\)-bit limbs.\r\n#[derive(Copy, Clone)]\r\npub struct Scalar52 {\r\n    pub limbs: [u64; 5],\r\n}\r\n\r\n} // verus!\r\n\r\nimpl Debug for Scalar52 {\r\n    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {\r\n        write!(f, \"Scalar52: {:?}\", self.limbs)\r\n    }\r\n}\r\n\r\n// #[cfg(feature = \"zeroize\")]\r\n// impl Zeroize for Scalar52 {\r\n//     fn zeroize(&mut self) {\r\n//         self.limbs.zeroize();\r\n//     }\r\n// }\r\n\r\nverus! {\r\nimpl Index<usize> for Scalar52 {\r\n    type Output = u64;\r\n    // TODO Verify this\r\n    #[verifier::external_body]\r\n    fn index(&self, _index: usize) -> &u64 {\r\n        &(self.limbs[_index])\r\n    }\r\n}\r\n} // verus!\r\n\r\nimpl IndexMut<usize> for Scalar52 {\r\n    fn index_mut(&mut self, _index: usize) -> &mut u64 {\r\n        &mut (self.limbs[_index])\r\n    }\r\n}\r\n\r\nverus! {\r\n\r\n/// u64 * u64 = u128 multiply helper\r\n#[inline(always)]\r\nfn m(x: u64, y: u64) -> (z: u128)\r\nrequires\r\n    x < (1u64 << 52),\r\n    y < (1u64 << 52),\r\nensures\r\n    z < (1u128 << 104),\r\n    z == x * y\r\n{\r\n    proof {lemma_52_52(x, y);}\r\n    (x as u128) * (y as u128)\r\n}\r\n\r\nimpl Scalar52 {\r\n    /// The scalar \\\\( 0 \\\\).\r\n    pub const ZERO: Scalar52 = Scalar52 { limbs: [0, 0, 0, 0, 0] };\r\n\r\n    /// Unpack a 32 byte / 256 bit scalar into 5 52-bit limbs.\r\n    #[rustfmt::skip] // keep alignment of s[*] calculations\r\n    pub fn from_bytes(bytes: &[u8; 32]) -> (s: Scalar52)\r\n    ensures bytes_to_nat(bytes) == to_nat(&s.limbs)\r\n    {\r\n        let mut words = [0u64; 4];\r\n        for i in 0..4\r\n            invariant 0 <= i <= 4 // proof\r\n        {\r\n            for j in 0..8\r\n                invariant 0 <= j <= 8 && i < 4\r\n            {\r\n                proof {\r\n                    assert(i < 4 && j < 8);\r\n                    assert((i as u64)*8u64 < 32u64);\r\n                    let idx = (i as u64) * 8 + (j as u64);\r\n                    assert(idx < 32);\r\n                }\r\n                words[i] |= (bytes[(i * 8) + j] as u64) << (j * 8);\r\n            }\r\n        }\r\n        //TODO: prove that bytes_to_nat(bytes) == words_to_nat(&words)\r\n        assume(bytes_to_nat(bytes) == words_to_nat(&words));\r\n        proof {\r\n            assert(1u64 << 52 > 0) by (bit_vector);\r\n            assert(1u64 << 48 > 0) by (bit_vector);\r\n            // TODO: prove property about words array\r\n        }\r\n\r\n        let mask = (1u64 << 52) - 1;\r\n        let top_mask = (1u64 << 48) - 1;\r\n        let mut s = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\r\n        //test workflow graphs\r\n        s.limbs[0] =   words[0]                            & mask;\r\n        s.limbs[1] = ((words[0] >> 52) | (words[1] << 12)) & mask;\r\n        s.limbs[2] = ((words[1] >> 40) | (words[2] << 24)) & mask;\r\n        s.limbs[3] = ((words[2] >> 28) | (words[3] << 36)) & mask;\r\n        s.limbs[4] =  (words[3] >> 16)                     & top_mask;\r\n\r\n        assume(false); // TODO: complete the proof\r\n\r\n        s\r\n    }\r\n\r\n    /// Reduce a 64 byte / 512 bit scalar mod l\r\n    #[rustfmt::skip] // keep alignment of lo[*] and hi[*] calculations\r\n    #[verifier::external_body] // TODO Verify this function\r\n    pub fn from_bytes_wide(bytes: &[u8; 64]) -> (s: Scalar52)\r\n    ensures\r\n        limbs_bounded(&s),\r\n        to_nat(&s.limbs) == bytes_wide_to_nat(bytes) % group_order(),\r\n    {\r\n        assume(false); // TODO: complete the proof\r\n        let mut words = [0u64; 8];\r\n        for i in 0..8 {\r\n            for j in 0..8 {\r\n                words[i] |= (bytes[(i * 8) + j] as u64) << (j * 8);\r\n            }\r\n        }\r\n\r\n        let mask = (1u64 << 52) - 1;\r\n        let mut lo = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\r\n        let mut hi = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\r\n\r\n        lo[0] =   words[0]                             & mask;\r\n        lo[1] = ((words[0] >> 52) | (words[ 1] << 12)) & mask;\r\n        lo[2] = ((words[1] >> 40) | (words[ 2] << 24)) & mask;\r\n        lo[3] = ((words[2] >> 28) | (words[ 3] << 36)) & mask;\r\n        lo[4] = ((words[3] >> 16) | (words[ 4] << 48)) & mask;\r\n        hi[0] =  (words[4] >>  4)                      & mask;\r\n        hi[1] = ((words[4] >> 56) | (words[ 5] <<  8)) & mask;\r\n        hi[2] = ((words[5] >> 44) | (words[ 6] << 20)) & mask;\r\n        hi[3] = ((words[6] >> 32) | (words[ 7] << 32)) & mask;\r\n        hi[4] =   words[7] >> 20                             ;\r\n\r\n        lo = Scalar52::montgomery_mul(&lo, &constants::R);  // (lo * R) / R = lo\r\n        hi = Scalar52::montgomery_mul(&hi, &constants::RR); // (hi * R^2) / R = hi * R\r\n\r\n        Scalar52::add(&hi, &lo)\r\n    }\r\n\r\n    /// Pack the limbs of this `Scalar52` into 32 bytes\r\n    #[rustfmt::skip] // keep alignment of s[*] calculations\r\n    #[allow(clippy::identity_op)]\r\n    #[allow(clippy::wrong_self_convention)]\r\n    pub fn to_bytes(self) -> (s: [u8; 32])\r\n    ensures bytes_to_nat(&s) == to_nat(&self.limbs)\r\n    {\r\n        let mut s = [0u8; 32];\r\n\r\n        s[ 0] =  (self.limbs[ 0] >>  0)                      as u8;\r\n        s[ 1] =  (self.limbs[ 0] >>  8)                      as u8;\r\n        s[ 2] =  (self.limbs[ 0] >> 16)                      as u8;\r\n        s[ 3] =  (self.limbs[ 0] >> 24)                      as u8;\r\n        s[ 4] =  (self.limbs[ 0] >> 32)                      as u8;\r\n        s[ 5] =  (self.limbs[ 0] >> 40)                      as u8;\r\n        s[ 6] = ((self.limbs[ 0] >> 48) | (self.limbs[ 1] << 4)) as u8;\r\n        s[ 7] =  (self.limbs[ 1] >>  4)                      as u8;\r\n        s[ 8] =  (self.limbs[ 1] >> 12)                      as u8;\r\n        s[ 9] =  (self.limbs[ 1] >> 20)                      as u8;\r\n        s[10] =  (self.limbs[ 1] >> 28)                      as u8;\r\n        s[11] =  (self.limbs[ 1] >> 36)                      as u8;\r\n        s[12] =  (self.limbs[ 1] >> 44)                      as u8;\r\n        s[13] =  (self.limbs[ 2] >>  0)                      as u8;\r\n        s[14] =  (self.limbs[ 2] >>  8)                      as u8;\r\n        s[15] =  (self.limbs[ 2] >> 16)                      as u8;\r\n        s[16] =  (self.limbs[ 2] >> 24)                      as u8;\r\n        s[17] =  (self.limbs[ 2] >> 32)                      as u8;\r\n        s[18] =  (self.limbs[ 2] >> 40)                      as u8;\r\n        s[19] = ((self.limbs[ 2] >> 48) | (self.limbs[ 3] << 4)) as u8;\r\n        s[20] =  (self.limbs[ 3] >>  4)                      as u8;\r\n        s[21] =  (self.limbs[ 3] >> 12)                      as u8;\r\n        s[22] =  (self.limbs[ 3] >> 20)                      as u8;\r\n        s[23] =  (self.limbs[ 3] >> 28)                      as u8;\r\n        s[24] =  (self.limbs[ 3] >> 36)                      as u8;\r\n        s[25] =  (self.limbs[ 3] >> 44)                      as u8;\r\n        s[26] =  (self.limbs[ 4] >>  0)                      as u8;\r\n        s[27] =  (self.limbs[ 4] >>  8)                      as u8;\r\n        s[28] =  (self.limbs[ 4] >> 16)                      as u8;\r\n        s[29] =  (self.limbs[ 4] >> 24)                      as u8;\r\n        s[30] =  (self.limbs[ 4] >> 32)                      as u8;\r\n        s[31] =  (self.limbs[ 4] >> 40)                      as u8;\r\n\r\n        assume(false); // TODO: complete the proof\r\n\r\n        s\r\n    }\r\n\r\n    /// Compute `a + b` (mod l)\r\n    pub fn add(a: &Scalar52, b: &Scalar52) -> (s: Scalar52)\r\n    requires\r\n        limbs_bounded(a),\r\n        limbs_bounded(b),\r\n        to_nat(&a.limbs) < group_order(),\r\n        to_nat(&b.limbs) < group_order(),\r\n    ensures\r\n        to_nat(&s.limbs) == (to_nat(&a.limbs) + to_nat(&b.limbs)) % group_order(),\r\n    {\r\n        let mut sum = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\r\n        proof { assert(1u64 << 52 > 0) by (bit_vector); }\r\n        let mask = (1u64 << 52) - 1;\r\n\r\n        // a + b\r\n        let mut carry: u64 = 0;\r\n        proof {\r\n            // Base case: empty subrange has value 0\r\n            assert(seq_u64_to_nat(a.limbs@.subrange(0, 0 as int)) == 0);\r\n            assert(seq_u64_to_nat(b.limbs@.subrange(0, 0 as int)) == 0);\r\n            assert(seq_u64_to_nat(sum.limbs@.subrange(0, 0 as int)) == 0);\r\n            assert((carry >> 52) == 0) by (bit_vector) requires carry == 0;\r\n            lemma2_to64();\r\n            assert(pow2(0) == 1);\r\n        }\r\n        for i in 0..5\r\n           invariant\r\n                    forall|j: int| 0 <= j < i ==> sum.limbs[j] < 1u64 << 52,\r\n                    limbs_bounded(a),\r\n                    limbs_bounded(b),\r\n                    mask == (1u64 << 52) - 1,\r\n                    i == 0 ==> carry == 0,\r\n                    i >= 1 ==> (carry >> 52) < 2,\r\n                    seq_u64_to_nat(a.limbs@.subrange(0, i as int)) + seq_u64_to_nat(b.limbs@.subrange(0, i as int)) ==\r\n                    seq_u64_to_nat(sum.limbs@.subrange(0, i as int)) + (carry >> 52) * pow2((52 * (i) as nat))\r\n        {\r\n            proof {lemma_add_loop_bounds(i as int, carry, a.limbs[i as int], b.limbs[i as int]);}\r\n            let ghost old_carry = carry;\r\n            carry = a.limbs[i] + b.limbs[i] + (carry >> 52);\r\n            let ghost sum_loop_start = sum;\r\n            sum.limbs[i] = carry & mask;\r\n            assert(sum_loop_start.limbs@.subrange(0, i as int) == sum.limbs@.subrange(0, i as int));\r\n            proof {\r\n                lemma_add_loop_invariant(sum, carry, i, a, b, old_carry, mask, sum_loop_start);\r\n            }\r\n            proof {lemma_add_carry_and_sum_bounds(carry, mask);}\r\n        }\r\n\r\n        assert(seq_u64_to_nat(a.limbs@.subrange(0, 5 as int)) + seq_u64_to_nat(b.limbs@.subrange(0, 5 as int)) ==\r\n               seq_u64_to_nat(sum.limbs@.subrange(0, 5 as int)) + (carry >> 52) * pow2((52 * (5) as nat)));\r\n\r\n        proof {lemma_add_sum_simplify(a, b, &sum, carry);}\r\n\r\n        // subtract l if the sum is >= l\r\n        proof { lemma_l_value_properties(&constants::L, &sum); }\r\n        assert(group_order() > to_nat(&sum.limbs) - group_order() >= -group_order());\r\n        proof{lemma_l_equals_group_order();}\r\n        proof{lemma_mod_sub_multiples_vanish(to_nat(&sum.limbs) as int, group_order() as int);}\r\n        Scalar52::sub(&sum, &constants::L)\r\n\r\n    }\r\n\r\n    /// Compute `a - b` (mod l)\r\n    pub fn sub(a: &Scalar52, b: &Scalar52) -> (s: Scalar52)\r\n    requires\r\n        limbs_bounded(a),\r\n        limbs_bounded(b),\r\n        // Without the following condition, all we can prove is something like:\r\n        // to_nat(&a.limbs) >= to_nat(&b.limbs) ==> to_nat(&s.limbs) == to_nat(&a.limbs) - to_nat(&b.limbs),\r\n        // to_nat(&a.limbs) < to_nat(&b.limbs) ==> to_nat(&s.limbs) == (to_nat(&a.limbs) - to_nat(&b.limbs) + pow2(260) + group_order()) % (pow2(260) as int),\r\n        // In the 2nd case, `sub` doesn't always do subtraction mod group_order\r\n        -group_order() <= to_nat(&a.limbs) - to_nat(&b.limbs) < group_order(),\r\n    ensures\r\n        to_nat(&s.limbs) == (to_nat(&a.limbs) - to_nat(&b.limbs)) % (group_order() as int),\r\n        limbs_bounded(&s),\r\n    {\r\n        let mut difference = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\r\n        proof { assert(1u64 << 52 > 0) by (bit_vector);}\r\n        let mask = (1u64 << 52) - 1;\r\n\r\n        // a - b\r\n        let mut borrow: u64 = 0;\r\n        assert(seq_u64_to_nat(a.limbs@.subrange(0, 0 as int)) - seq_u64_to_nat(b.limbs@.subrange(0, 0 as int )) ==\r\n                seq_u64_to_nat(difference.limbs@.subrange(0, 0 as int )));\r\n        assert( (borrow >> 63) == 0 ) by (bit_vector)\r\n            requires borrow == 0;\r\n        assert(seq_u64_to_nat(a.limbs@.subrange(0, 0 as int)) - seq_u64_to_nat(b.limbs@.subrange(0, 0 as int )) ==\r\n                seq_u64_to_nat(difference.limbs@.subrange(0, 0 as int )) - (borrow >> 63) * pow2((52 * (0) as nat)));\r\n        for i in 0..5\r\n            invariant\r\n                      limbs_bounded(b),\r\n                      limbs_bounded(a),\r\n                      forall|j: int| 0 <= j < i ==> difference.limbs[j] < (1u64 << 52),\r\n                      mask == (1u64 << 52) - 1,\r\n                      seq_u64_to_nat(a.limbs@.subrange(0, i as int)) - seq_u64_to_nat(b.limbs@.subrange(0, i as int )) ==\r\n                                    seq_u64_to_nat(difference.limbs@.subrange(0, i as int )) - (borrow >> 63) * pow2((52 * (i) as nat))\r\n        {\r\n            proof { assert ((borrow >> 63) < 2) by (bit_vector); }\r\n            let ghost old_borrow = borrow;\r\n            borrow = a.limbs[i].wrapping_sub(b.limbs[i] + (borrow >> 63));\r\n            let ghost difference_loop1_start = difference;\r\n            difference.limbs[i] = borrow & mask;\r\n            assert(difference_loop1_start.limbs@.subrange(0, i as int) == difference.limbs@.subrange(0, i as int));\r\n            assert(\r\n            seq_u64_to_nat(a.limbs@.subrange(0, i as int)) - seq_u64_to_nat(b.limbs@.subrange(0, i as int )) ==\r\n                        seq_u64_to_nat(difference_loop1_start.limbs@.subrange(0, i as int )) - (old_borrow >> 63) * pow2((52 * (i) as nat)));\r\n            proof{\r\n                lemma_sub_loop1_invariant(difference, borrow, i, a, b, old_borrow, mask, difference_loop1_start);\r\n            }\r\n            proof { lemma_borrow_and_mask_bounded(borrow, mask); }\r\n        }\r\n\r\n        assert(seq_u64_to_nat(a.limbs@.subrange(0, 5 as int)) - seq_u64_to_nat(b.limbs@.subrange(0, 5 as int )) ==\r\n                seq_u64_to_nat(difference.limbs@.subrange(0, 5 as int )) - (borrow >> 63) * pow2((52 * (5) as nat)) );\r\n        // conditionally add l if the difference is negative\r\n        assert(borrow >> 63 == 1 || borrow >> 63 == 0) by (bit_vector);\r\n        let mut carry: u64 = 0;\r\n        let ghost difference_after_loop1 = difference;\r\n        assert(seq_u64_to_nat(difference_after_loop1.limbs@.subrange(0, 0 as int)) == 0);\r\n        assert(seq_u64_to_nat(constants::L.limbs@.subrange(0, 0 as int)) == 0);\r\n        assert(seq_u64_to_nat(difference.limbs@.subrange(0, 0 as int)) == 0);\r\n        assert(carry >> 52 == 0) by (bit_vector)\r\n            requires carry == 0;\r\n        for i in 0..5\r\n            invariant\r\n                      forall|j: int| 0 <= j < 5 ==> difference.limbs[j] < (1u64 << 52),  // from first loop\r\n                      forall|j: int| i <= j < 5 ==> difference.limbs[j] == difference_after_loop1.limbs[j],\r\n                      mask == (1u64 << 52) - 1,\r\n                      i == 0 ==> carry == 0,\r\n                      i >= 1 ==> (carry >> 52) < 2,\r\n                      (i >=1 && borrow >> 63 == 0) ==> carry == difference.limbs[i-1],\r\n                      borrow >> 63 == 0 ==> difference_after_loop1 == difference,\r\n                      borrow >> 63 == 1 ==>\r\n                          seq_u64_to_nat(difference_after_loop1.limbs@.subrange(0, i as int)) + seq_u64_to_nat(constants::L.limbs@.subrange(0, i as int)) ==\r\n                          seq_u64_to_nat(difference.limbs@.subrange(0, i as int)) + (carry >> 52) * pow2(52 * i as nat)\r\n\r\n        {\r\n            let ghost old_carry = carry;\r\n            let underflow = Choice::from((borrow >> 63) as u8);\r\n            let addend = select(&0, &constants::L.limbs[i], underflow);\r\n            if borrow >> 63 == 0 {\r\n                assert(addend == 0);\r\n            }\r\n            if borrow >> 63 == 1 {\r\n                assert(addend == constants::L.limbs[i as int]);\r\n            }\r\n            proof {lemma_scalar_subtract_no_overflow(carry, difference.limbs[i as int], addend, i as u32, &constants::L);}\r\n            carry = (carry >> 52) + difference.limbs[i] + addend;\r\n            let ghost difference_loop2_start = difference;\r\n            difference.limbs[i] = carry & mask;\r\n            proof {\r\n                lemma_carry_bounded_after_mask(carry, mask);\r\n                assert(difference_loop2_start.limbs@.subrange(0, i as int) == difference.limbs@.subrange(0, i as int));\r\n                lemma_sub_loop2_invariant(difference, i, a, b, mask, difference_after_loop1, difference_loop2_start, carry, old_carry, addend, borrow);\r\n            }\r\n        }\r\n        proof { lemma_sub_correct_after_loops(difference, carry, a, b, difference_after_loop1, borrow);}\r\n        difference\r\n    }\r\n\r\n    /// Compute `a * b`\r\n    #[inline(always)]\r\n    #[rustfmt::skip] // keep alignment of z[*] calculations\r\n    pub (crate) fn mul_internal(a: &Scalar52, b: &Scalar52) -> (z: [u128; 9])\r\n    requires\r\n        limbs_bounded(a),\r\n        limbs_bounded(b),\r\n    ensures\r\n        slice128_to_nat(&z) == to_nat(&a.limbs) * to_nat(&b.limbs),\r\n    {\r\n        proof {lemma_mul_internal_no_overflow()}\r\n\r\n        let mut z = [0u128; 9];\r\n\r\n        z[0] = m(a.limbs[0], b.limbs[0]);\r\n        z[1] = m(a.limbs[0], b.limbs[1]) + m(a.limbs[1], b.limbs[0]);\r\n        z[2] = m(a.limbs[0], b.limbs[2]) + m(a.limbs[1], b.limbs[1]) + m(a.limbs[2], b.limbs[0]);\r\n        z[3] = m(a.limbs[0], b.limbs[3]) + m(a.limbs[1], b.limbs[2]) + m(a.limbs[2], b.limbs[1]) + m(a.limbs[3], b.limbs[0]);\r\n        z[4] = m(a.limbs[0], b.limbs[4]) + m(a.limbs[1], b.limbs[3]) + m(a.limbs[2], b.limbs[2]) + m(a.limbs[3], b.limbs[1]) + m(a.limbs[4], b.limbs[0]);\r\n        z[5] =                 m(a.limbs[1], b.limbs[4]) + m(a.limbs[2], b.limbs[3]) + m(a.limbs[3], b.limbs[2]) + m(a.limbs[4], b.limbs[1]);\r\n        z[6] =                                 m(a.limbs[2], b.limbs[4]) + m(a.limbs[3], b.limbs[3]) + m(a.limbs[4], b.limbs[2]);\r\n        z[7] =                                                 m(a.limbs[3], b.limbs[4]) + m(a.limbs[4], b.limbs[3]);\r\n        z[8] =                                                                 m(a.limbs[4], b.limbs[4]);\r\n\r\n        proof {lemma_mul_internal_correct(&a.limbs, &b.limbs, &z);}\r\n\r\n        z\r\n    }\r\n\r\n\r\n    // TODO Make this function more like the original?\r\n    /// Compute `a^2`\r\n    #[inline(always)]\r\n    #[rustfmt::skip] // keep alignment of calculations\r\n    pub (crate) fn square_internal(a: &Scalar52) -> (z: [u128; 9])\r\n    requires\r\n        limbs_bounded(a),\r\n    ensures\r\n        slice128_to_nat(&z) == to_nat(&a.limbs) * to_nat(&a.limbs),\r\n    {\r\n        proof {lemma_square_internal_no_overflow()}\r\n\r\n        let mut z = [0u128; 9];\r\n        z[0] = m(a.limbs[0], a.limbs[0]);\r\n        z[1] = m(a.limbs[0], a.limbs[1]) * 2;\r\n        z[2] = m(a.limbs[0], a.limbs[2]) * 2 + m(a.limbs[1], a.limbs[1]);\r\n        z[3] = m(a.limbs[0], a.limbs[3]) * 2 + m(a.limbs[1], a.limbs[2]) * 2;\r\n        z[4] = m(a.limbs[0], a.limbs[4]) * 2 + m(a.limbs[1], a.limbs[3]) * 2 + m(a.limbs[2], a.limbs[2]);\r\n        z[5] =                 m(a.limbs[1], a.limbs[4]) * 2 + m(a.limbs[2], a.limbs[3]) * 2;\r\n        z[6] =                                 m(a.limbs[2], a.limbs[4]) * 2 + m(a.limbs[3], a.limbs[3]);\r\n        z[7] =                                                 m(a.limbs[3], a.limbs[4]) * 2;\r\n        z[8] =                                                                 m(a.limbs[4], a.limbs[4]);\r\n\r\n        proof {lemma_square_internal_correct(&a.limbs, &z);}\r\n\r\n        z\r\n    }\r\n\r\n    /// Compute `limbs/R` (mod l), where R is the Montgomery modulus 2^260\r\n    #[inline(always)]\r\n    #[rustfmt::skip] // keep alignment of n* and r* calculations\r\n    pub (crate) fn montgomery_reduce(limbs: &[u128; 9]) -> (result: Scalar52)\r\n    ensures\r\n        (to_nat(&result.limbs) * montgomery_radix()) % group_order() == slice128_to_nat(limbs) % group_order(),\r\n        limbs_bounded(&result),\r\n    {\r\n        assume(false); // TODO: Add proper bounds checking and proofs\r\n\r\n\r\n        // note: l[3] is zero, so its multiples can be skipped\r\n        let l = &constants::L;\r\n\r\n        // the first half computes the Montgomery adjustment factor n, and begins adding n*l to make limbs divisible by R\r\n        let (carry, n0) = Self::part1(limbs[0]);\r\n        let (carry, n1) = Self::part1(carry + limbs[1] + m(n0, l.limbs[1]));\r\n        let (carry, n2) = Self::part1(carry + limbs[2] + m(n0, l.limbs[2]) + m(n1, l.limbs[1]));\r\n        let (carry, n3) = Self::part1(carry + limbs[3] + m(n1, l.limbs[2]) + m(n2, l.limbs[1]));\r\n        let (carry, n4) = Self::part1(carry + limbs[4] + m(n0, l.limbs[4]) + m(n2, l.limbs[2]) + m(n3, l.limbs[1]));\r\n\r\n        // limbs is divisible by R now, so we can divide by R by simply storing the upper half as the result\r\n        let (carry, r0) = Self::part2(carry + limbs[5] + m(n1, l.limbs[4]) + m(n3, l.limbs[2]) + m(n4, l.limbs[1]));\r\n        let (carry, r1) = Self::part2(carry + limbs[6] + m(n2, l.limbs[4]) + m(n4, l.limbs[2]));\r\n        let (carry, r2) = Self::part2(carry + limbs[7] + m(n3, l.limbs[4]));\r\n        let (carry, r3) = Self::part2(carry + limbs[8] + m(n4, l.limbs[4]));\r\n        let r4 = carry as u64;\r\n\r\n        // result may be >= l, so attempt to subtract l\r\n        Scalar52::sub(&Scalar52 { limbs: [r0, r1, r2, r3, r4] }, l)\r\n    }\r\n\r\n\r\n    /// Helper function for Montgomery reduction\r\n    #[inline(always)]\r\n    fn part1(sum: u128) -> (res: (u128, u64))\r\n    {\r\n        assume(false); // TODO: Add proper bounds checking and proofs\r\n        let p = (sum as u64).wrapping_mul(constants::LFACTOR) & ((1u64 << 52) - 1);\r\n        let carry = (sum + m(p, constants::L.limbs[0])) >> 52;\r\n        (carry, p)\r\n    }\r\n\r\n    /// Helper function for Montgomery reduction\r\n    #[inline(always)]\r\n    fn part2(sum: u128) -> (res: (u128, u64))\r\n    {\r\n        assume(false); // TODO: Add proper bounds checking and proofs\r\n        let w = (sum as u64) & ((1u64 << 52) - 1);\r\n        let carry = sum >> 52;\r\n        (carry, w)\r\n    }\r\n\r\n    /// Compute `a * b` (mod l)\r\n    #[inline(never)]\r\n    pub fn mul(a: &Scalar52, b: &Scalar52) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(a),\r\n        limbs_bounded(b),\r\n    ensures\r\n        to_nat(&result.limbs) == (to_nat(&a.limbs) * to_nat(&b.limbs)) % group_order(),\r\n    {\r\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\r\n        let ab = Scalar52::montgomery_reduce(&Scalar52::mul_internal(a, b));\r\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(&ab, &constants::RR))\r\n    }\r\n\r\n    /// Compute `a^2` (mod l)\r\n    #[inline(never)]\r\n    #[allow(dead_code)] // XXX we don't expose square() via the Scalar API\r\n    pub fn square(&self) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(self),\r\n    ensures\r\n        to_nat(&result.limbs) == (to_nat(&self.limbs) * to_nat(&self.limbs)) % group_order(),\r\n    {\r\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\r\n        let aa = Scalar52::montgomery_reduce(&Scalar52::square_internal(self));\r\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(&aa, &constants::RR))\r\n    }\r\n\r\n    /// Compute `(a * b) / R` (mod l), where R is the Montgomery modulus 2^260\r\n    #[inline(never)]\r\n    pub fn montgomery_mul(a: &Scalar52, b: &Scalar52) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(a),\r\n        limbs_bounded(b),\r\n    ensures\r\n        limbs_bounded(&result),\r\n        (to_nat(&result.limbs) * montgomery_radix()) % group_order() == (to_nat(&a.limbs) * to_nat(&b.limbs)) % group_order(),\r\n    {\r\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(a, b))\r\n    }\r\n\r\n    /// Compute `(a^2) / R` (mod l) in Montgomery form, where R is the Montgomery modulus 2^260\r\n    #[inline(never)]\r\n    pub fn montgomery_square(&self) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(self),\r\n    ensures\r\n        limbs_bounded(&result),\r\n        (to_nat(&result.limbs) * montgomery_radix()) % group_order() == (to_nat(&self.limbs) * to_nat(&self.limbs)) % group_order(),\r\n    {\r\n        Scalar52::montgomery_reduce(&Scalar52::square_internal(self))\r\n    }\r\n\r\n    /// Puts a Scalar52 in to Montgomery form, i.e. computes `a*R (mod l)`\r\n    #[inline(never)]\r\n    pub fn as_montgomery(&self) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(self),\r\n    ensures\r\n        limbs_bounded(&result),\r\n        to_nat(&result.limbs) == (to_nat(&self.limbs) * montgomery_radix()) % group_order(),\r\n    {\r\n        proof {\r\n            lemma_rr_limbs_bounded();\r\n        }\r\n        let result = Scalar52::montgomery_mul(self, &constants::RR);\r\n        assume(to_nat(&result.limbs) == (to_nat(&self.limbs) * montgomery_radix()) % group_order());\r\n        result\r\n    }\r\n\r\n    /// Takes a Scalar52 out of Montgomery form, i.e. computes `a/R (mod l)`\r\n    #[allow(clippy::wrong_self_convention)]\r\n    #[inline(never)]\r\n    pub fn from_montgomery(&self) -> (result: Scalar52)\r\n    requires\r\n        limbs_bounded(self),\r\n    ensures\r\n        limbs_bounded(&result),\r\n        (to_nat(&result.limbs) * montgomery_radix()) % group_order() == to_nat(&self.limbs) % group_order(),\r\n    {\r\n        let mut limbs = [0u128; 9];\r\n        #[allow(clippy::needless_range_loop)]\r\n        for i in 0..5\r\n            invariant\r\n                forall|j: int| #![auto] 0 <= j < i ==> limbs[j] == self.limbs[j] as u128,\r\n                forall|j: int| #![auto] i <= j < 9 ==> limbs[j] == 0,\r\n        {\r\n            limbs[i] = self.limbs[i] as u128;\r\n        }\r\n        let result = Scalar52::montgomery_reduce(&limbs);\r\n        proof {\r\n            lemma_from_montgomery_limbs_conversion(&limbs, &self.limbs);\r\n        }\r\n        result\r\n    }\r\n}\r\n\r\n\r\n} // verus!\r\n\r\n// #[cfg(test)]\r\n// mod test {\r\n//     use super::*;\r\n\r\n//     /// Note: x is 2^253-1 which is slightly larger than the largest scalar produced by\r\n//     /// this implementation (l-1), and should show there are no overflows for valid scalars\r\n//     ///\r\n//     /// x = 14474011154664524427946373126085988481658748083205070504932198000989141204991\r\n//     /// x = 7237005577332262213973186563042994240801631723825162898930247062703686954002 mod l\r\n//     /// x = 3057150787695215392275360544382990118917283750546154083604586903220563173085*R mod l in Montgomery form\r\n//     pub static X: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000fffffffffffff,\r\n//             0x000fffffffffffff,\r\n//             0x000fffffffffffff,\r\n//             0x000fffffffffffff,\r\n//             0x00001fffffffffff,\r\n//         ],\r\n//     };\r\n\r\n//     /// x^2 = 3078544782642840487852506753550082162405942681916160040940637093560259278169 mod l\r\n//     pub static XX: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x0001668020217559,\r\n//             0x000531640ffd0ec0,\r\n//             0x00085fd6f9f38a31,\r\n//             0x000c268f73bb1cf4,\r\n//             0x000006ce65046df0,\r\n//         ],\r\n//     };\r\n\r\n//     /// x^2 = 4413052134910308800482070043710297189082115023966588301924965890668401540959*R mod l in Montgomery form\r\n//     pub static XX_MONT: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000c754eea569a5c,\r\n//             0x00063b6ed36cb215,\r\n//             0x0008ffa36bf25886,\r\n//             0x000e9183614e7543,\r\n//             0x0000061db6c6f26f,\r\n//         ],\r\n//     };\r\n\r\n//     /// y = 6145104759870991071742105800796537629880401874866217824609283457819451087098\r\n//     pub static Y: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000b75071e1458fa,\r\n//             0x000bf9d75e1ecdac,\r\n//             0x000433d2baf0672b,\r\n//             0x0005fffcc11fad13,\r\n//             0x00000d96018bb825,\r\n//         ],\r\n//     };\r\n\r\n//     /// x*y = 36752150652102274958925982391442301741 mod l\r\n//     pub static XY: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000ee6d76ba7632d,\r\n//             0x000ed50d71d84e02,\r\n//             0x00000000001ba634,\r\n//             0x0000000000000000,\r\n//             0x0000000000000000,\r\n//         ],\r\n//     };\r\n\r\n//     /// x*y = 658448296334113745583381664921721413881518248721417041768778176391714104386*R mod l in Montgomery form\r\n//     pub static XY_MONT: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x0006d52bf200cfd5,\r\n//             0x00033fb1d7021570,\r\n//             0x000f201bc07139d8,\r\n//             0x0001267e3e49169e,\r\n//             0x000007b839c00268,\r\n//         ],\r\n//     };\r\n\r\n//     /// a = 2351415481556538453565687241199399922945659411799870114962672658845158063753\r\n//     pub static A: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x0005236c07b3be89,\r\n//             0x0001bc3d2a67c0c4,\r\n//             0x000a4aa782aae3ee,\r\n//             0x0006b3f6e4fec4c4,\r\n//             0x00000532da9fab8c,\r\n//         ],\r\n//     };\r\n\r\n//     /// b = 4885590095775723760407499321843594317911456947580037491039278279440296187236\r\n//     pub static B: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000d3fae55421564,\r\n//             0x000c2df24f65a4bc,\r\n//             0x0005b5587d69fb0b,\r\n//             0x00094c091b013b3b,\r\n//             0x00000acd25605473,\r\n//         ],\r\n//     };\r\n\r\n//     /// a+b = 0\r\n//     /// a-b = 4702830963113076907131374482398799845891318823599740229925345317690316127506\r\n//     pub static AB: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000a46d80f677d12,\r\n//             0x0003787a54cf8188,\r\n//             0x0004954f0555c7dc,\r\n//             0x000d67edc9fd8989,\r\n//             0x00000a65b53f5718,\r\n//         ],\r\n//     };\r\n\r\n//     // c = (2^512 - 1) % l = 1627715501170711445284395025044413883736156588369414752970002579683115011840\r\n//     pub static C: Scalar52 = Scalar52 {\r\n//         limbs: [\r\n//             0x000611e3449c0f00,\r\n//             0x000a768859347a40,\r\n//             0x0007f5be65d00e1b,\r\n//             0x0009a3dceec73d21,\r\n//             0x00000399411b7c30,\r\n//         ],\r\n//     };\r\n\r\n//     #[test]\r\n//     fn mul_max() {\r\n//         let res = Scalar52::mul(&X, &X);\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XX[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn square_max() {\r\n//         let res = X.square();\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XX[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn montgomery_mul_max() {\r\n//         let res = Scalar52::montgomery_mul(&X, &X);\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XX_MONT[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn montgomery_square_max() {\r\n//         let res = X.montgomery_square();\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XX_MONT[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn mul() {\r\n//         let res = Scalar52::mul(&X, &Y);\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XY[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn montgomery_mul() {\r\n//         let res = Scalar52::montgomery_mul(&X, &Y);\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == XY_MONT[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn add() {\r\n//         let res = Scalar52::add(&A, &B);\r\n//         let zero = Scalar52::ZERO;\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == zero[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn sub() {\r\n//         let res = Scalar52::sub(&A, &B);\r\n//         for i in 0..5 {\r\n//             assert!(res[i] == AB[i]);\r\n//         }\r\n//     }\r\n\r\n//     #[test]\r\n//     fn from_bytes_wide() {\r\n//         let bignum = [255u8; 64]; // 2^512 - 1\r\n//         let reduced = Scalar52::from_bytes_wide(&bignum);\r\n//         for i in 0..5 {\r\n//             assert!(reduced[i] == C[i]);\r\n//         }\r\n//     }\r\n// }",
    "filename": "scalartest.rs",
    "filepath": null,
    "folder_id": null,
    "user_id": 460184
  }
}