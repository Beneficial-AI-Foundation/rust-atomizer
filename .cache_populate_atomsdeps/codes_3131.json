{
  "51719": {
    "text": "// scalar64_verus.rs\n#![allow(unused)]\nuse subtle::{Choice, ConditionallySelectable};\nuse vstd::arithmetic::mul::*;\nuse vstd::arithmetic::power2::*;\nuse vstd::calc;\nuse vstd::prelude::*;\n// use crate::constants; // We manually import needed constants\n\nverus! {\n\n        #[verifier::external_type_specification]\n        #[verifier::external_body]\n        pub struct ExChoice(Choice);\n\n        pub uninterp spec fn boolify(c: Choice) -> bool;\n\n        pub assume_specification [Choice::from](u: u8) -> (c: Choice)\n            ensures u == 0 ==> boolify(c) == false,\n                    u == 1 ==> boolify(c) == true;\n\n        #[verifier::external_body]\n        fn select(x: &u64, y: &u64, c: Choice) -> (res: u64)\n            ensures boolify(c) ==> res == x,\n                    ! boolify(c) ==> res == y\n        {\n            u64::conditional_select(x, y, c)\n        }\n\n\n        /* MANUALLY IMPORTED FROM curve25519-dalek/src/backend/serial/u64/constants.rs */\n        /// `L` is the order of base point, i.e. 2^252 + 27742317777372353535851937790883648493\n        pub const L: Scalar52 = Scalar52 { limbs: [\n            0x0002631a5cf5d3ed,\n            0x000dea2f79cd6581,\n            0x000000000014def9,\n            0x0000000000000000,\n            0x0000100000000000,\n        ]};\n\n        /// `RR` = (R^2) mod L where R = 2^260\n        pub const RR: Scalar52 = Scalar52 { limbs: [\n            0x0009d265e952d13b,\n            0x000d63c715bea69f,\n            0x0005ea65f25dd3d5,\n            0x000e571d6372e9c5,\n            0x0000039da6b19ca7,\n        ]};\n\n        /// `LFACTOR` = (-(L^(-1))) mod 2^52\n        pub const LFACTOR: u64 = 0x51da312547e1b;\n\n        pub open spec fn seq_to_nat(limbs: Seq<nat>) -> nat\n        decreases limbs.len()\n        {\n            if limbs.len() == 0 {\n                0\n            } else {\n                limbs[0] + seq_to_nat(limbs.subrange(1, limbs.len() as int)) * pow2(52)\n            }\n        }\n\n        pub open spec fn slice128_to_nat(limbs: &[u128]) -> nat\n        {\n            seq_to_nat(limbs@.map(|i, x| x as nat))\n        }\n\n        pub open spec fn to_nat(limbs: &[u64]) -> nat\n        {\n            seq_to_nat(limbs@.map(|i, x| x as nat))\n        }\n\n        pub open spec fn nine_limbs_to_nat_aux(limbs: &[u128; 9]) -> nat {\n            (limbs[0] as nat) +\n            (limbs[1] as nat) * pow2(52) +\n            (limbs[2] as nat) * pow2(104) +\n            (limbs[3] as nat) * pow2(156) +\n            (limbs[4] as nat) * pow2(208) +\n            (limbs[5] as nat) * pow2(260) +\n            (limbs[6] as nat) * pow2(312) +\n            (limbs[7] as nat) * pow2(364) +\n            (limbs[8] as nat) * pow2(416)\n        }\n\n        pub open spec fn five_limbs_to_nat_aux(limbs: [u64; 5]) -> nat {\n            (limbs[0] as nat) +\n            pow2(52) * (limbs[1] as nat) +\n            pow2(104) * (limbs[2] as nat) +\n            pow2(156) * (limbs[3] as nat) +\n            pow2(208) * (limbs[4] as nat)\n        }\n\n        proof fn lemma_nine_limbs_equals_slice128_to_nat(limbs: &[u128; 9])\n        ensures\n            nine_limbs_to_nat_aux(limbs) == slice128_to_nat(limbs)\n        {\n\n            let seq = limbs@.map(|i, x| x as nat);\n\n            calc! {\n                (==)\n                slice128_to_nat(limbs); {\n                }\n                seq_to_nat(seq); {\n                    reveal_with_fuel(seq_to_nat, 10);\n                }\n                (limbs[0] as nat) +\n                ((limbs[1] as nat) +\n                 ((limbs[2] as nat) +\n                  ((limbs[3] as nat) +\n                   ((limbs[4] as nat) +\n                    ((limbs[5] as nat) +\n                     ((limbs[6] as nat) +\n                      ((limbs[7] as nat) +\n                       (limbs[8] as nat) * pow2(52)\n                      ) * pow2(52)\n                     ) * pow2(52)\n                    ) * pow2(52)\n                   ) * pow2(52)\n                  ) * pow2(52)\n                 ) * pow2(52)\n                ) * pow2(52); {\n                lemma_pow2_adds(52, 52);\n                lemma_pow2_adds(104, 52);\n                lemma_pow2_adds(156, 52);\n                lemma_pow2_adds(208, 52);\n                lemma_pow2_adds(260, 52);\n                lemma_pow2_adds(312, 52);\n                lemma_pow2_adds(364, 52);\n                broadcast use group_mul_is_distributive;\n                broadcast use lemma_mul_is_associative;\n                }\n                nine_limbs_to_nat_aux(limbs);\n            }\n        }\n\n        proof fn lemma_five_limbs_equals_to_nat(limbs: &[u64; 5])\n        ensures\n            five_limbs_to_nat_aux(*limbs) == to_nat(limbs)\n        {\n            let seq = limbs@.map(|i, x| x as nat);\n\n            calc! {\n                (==)\n                to_nat(limbs); {\n                }\n                seq_to_nat(seq); {\n                    reveal_with_fuel(seq_to_nat, 6);\n                }\n                (limbs[0] as nat) +\n                ((limbs[1] as nat) +\n                 ((limbs[2] as nat) +\n                  ((limbs[3] as nat) +\n                   (limbs[4] as nat) * pow2(52)\n                  ) * pow2(52)\n                 ) * pow2(52)\n                ) * pow2(52); {\n                lemma_pow2_adds(52, 52);\n                lemma_pow2_adds(104, 52);\n                lemma_pow2_adds(156, 52);\n                broadcast use group_mul_is_distributive;\n                broadcast use lemma_mul_is_associative;\n                }\n                (limbs[0] as nat) +\n                pow2(52) * (limbs[1] as nat) +\n                pow2(104) * (limbs[2] as nat) +\n                pow2(156) * (limbs[3] as nat) +\n                pow2(208) * (limbs[4] as nat); {\n                }\n                five_limbs_to_nat_aux(*limbs);\n            }\n        }\n\n\n\n        // Modular reduction of to_nat mod L\n        spec fn to_scalar(limbs: &[u64; 5]) -> nat {\n            to_nat(limbs) % group_order()\n        }\n\n        /// natural value of a 256 bit bitstring represented as array of 32 bytes\n        pub open spec fn bytes_to_nat(bytes: &[u8; 32]) -> nat {\n            // Convert bytes to nat in little-endian order using recursive helper\n            bytes_to_nat_rec(bytes, 0)\n        }\n\n        pub open spec fn bytes_to_nat_rec(bytes: &[u8; 32], index: int) -> nat\n        decreases 32 - index\n        {\n            if index >= 32 {\n                0\n            } else {\n                (bytes[index] as nat) * pow2(index as nat) + bytes_to_nat_rec(bytes, index + 1)\n            }\n        }\n\n        // Generic function to convert array of words to natural number\n        // Takes: array of words, number of words, bits per word\n        // Note: This is a specification function that works with concrete types\n        pub open spec fn words_to_nat_gen_u64(words: &[u64], num_words: int, bits_per_word: int) -> nat\n        decreases num_words\n        {\n            if num_words <= 0 {\n                0\n            } else {\n                let word_value = (words[num_words - 1] as nat) * pow2(((num_words - 1) * bits_per_word) as nat);\n                word_value + words_to_nat_gen_u64(words, num_words - 1, bits_per_word)\n            }\n        }\n\n        pub open spec fn words_to_nat_gen_u32(words: &[u32], num_words: int, bits_per_word: int) -> nat\n        decreases num_words\n        {\n            if num_words <= 0 {\n                0\n            } else {\n                let word_value = (words[num_words - 1] as nat) * pow2(((num_words - 1) * bits_per_word) as nat);\n                word_value + words_to_nat_gen_u32(words, num_words - 1, bits_per_word)\n            }\n        }\n\n        // natural value of a 256 bit bitstring represented as an array of 4 words of 64 bits\n        // Now implemented using the generic function\n        pub open spec fn words_to_nat(words: &[u64; 4]) -> nat {\n            words_to_nat_gen_u64(words, 4, 64)\n        }\n\n        // Group order: the value of L as a natural number\n        pub open spec fn group_order() -> nat {\n            (1u64 << 252) as nat + 27742317777372353535851937790883648493nat\n        }\n\n        /// u64 * u64 = u128 multiply helper\n        #[inline(always)]\n        fn m(x: u64, y: u64) -> (z: u128)\n        requires\n            x < (1u64 << 52),\n            y < (1u64 << 52),\n        ensures\n            z < (1u128 << 104),\n            z == x * y\n        {\n            proof {\n                assert(1u128 << 52 == 1u64 << 52) by (bit_vector);\n                calc! {\n                    (<)\n                    (x as u128) * (y as u128); (<=) {\n                        if x > 0 {\n                            lemma_mul_strict_inequality(y as int, (1u128 << 52) as int, x as int);\n                        } else {\n                            assert(x == 0);\n                            assert((x as u128) * (y as u128) == 0);\n                        }\n                    }\n                    (x as u128) * (1u128 << 52); (<) {\n                        lemma_mul_strict_inequality(x as int, (1u128 << 52) as int, (1u128 << 52) as int);\n                    }\n                    (1u128 << 52) * (1u128 << 52);\n                }\n                assert((1u128 << 52) * (1u128 << 52) == (1u128 << 104)) by (compute);\n            }\n            (x as u128) * (y as u128)\n        }\n\n        pub struct Scalar52 {\n            // ADAPTED CODE LINE: we give a name to the field: \"limbs\"\n            pub limbs: [u64; 5],\n        }\n\n    impl Scalar52 {\n\n        /****** IMPLEMENTATION CONSTANTS AND FUNCTIONS ********/\n        pub const ZERO: Scalar52 = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\n\n        /// Unpack a 32 byte / 256 bit scalar into 5 52-bit limbs.\n        #[rustfmt::skip] // keep alignment of s[*] calculations\n        /* ADAPTED CODE LINE: we give a name to the output: \"s\" */\n        pub fn from_bytes(bytes: &[u8; 32]) -> (s: Scalar52)\n        // SPECIFICATION: unpacking keeps the same nat value\n        ensures bytes_to_nat(bytes) == to_nat(&s.limbs)\n        {\n            let mut words = [0u64; 4];\n            for i in 0..4\n                invariant 0 <= i <= 4 // proof\n            {\n                for j in 0..8\n                    invariant 0 <= j <= 8 && i < 4\n                {\n                    proof {\n                        assert(i < 4 && j < 8);\n                        assert((i as u64)*8u64 < 32u64);\n                        let idx = (i as u64) * 8 + (j as u64);\n                        assert(idx < 32);\n                    }\n                    words[i] |= (bytes[(i * 8) + j] as u64) << (j * 8);\n                }\n            }\n            assume(bytes_to_nat(bytes) == words_to_nat(&words));\n            proof {\n                assert(1u64 << 52 > 0) by (bit_vector);\n                assert(1u64 << 48 > 0) by (bit_vector);\n                // TODO: prove property about words array\n            }\n\n        let mask = (1u64 << 52) - 1;\n        let top_mask = (1u64 << 48) - 1;\n        // let mut s = Scalar52::ZERO; // ORIGINAL IMPLEMENTATION\n        let mut s = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\n        proof {\n            assert(Scalar52::ZERO == Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] });\n            assert(s == Scalar52::ZERO); // PROVES EQUIVALENCE TO ORIGINAL IMPLEMENTATION\n        }\n\n        s.limbs[0] =   words[0]                            & mask;\n        s.limbs[1] = ((words[0] >> 52) | (words[1] << 12)) & mask;\n        s.limbs[2] = ((words[1] >> 40) | (words[2] << 24)) & mask;\n        s.limbs[3] = ((words[2] >> 28) | (words[3] << 36)) & mask;\n        s.limbs[4] =  (words[3] >> 16)                     & top_mask;\n\n        assume(false); // TODO: complete the proof\n\n        s\n    }\n\n    /// Reduce a 64 byte / 512 bit scalar mod l\n    #[rustfmt::skip] // keep alignment of lo[*] and hi[*] calculations\n    pub fn from_bytes_wide(bytes: &[u8; 64]) -> Scalar52 {\n        // TODO; just signature for now\n        Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] }\n    }\n\n    /// Pack the limbs of this `Scalar52` into 32 bytes\n    #[rustfmt::skip] // keep alignment of s[*] calculations\n    #[allow(clippy::identity_op)]\n    #[allow(clippy::wrong_self_convention)]\n    pub fn to_bytes(self) -> (s: [u8; 32])\n    // DIFF-SPEC-3: we give a name to the output: \"s\"\n    // SPECIFICATION: packing keeps the same nat value\n    ensures bytes_to_nat(&s) == to_nat(&self.limbs)\n    {\n        let mut s = [0u8; 32];\n\n        s[ 0] =  (self.limbs[ 0] >>  0)                      as u8;\n        s[ 1] =  (self.limbs[ 0] >>  8)                      as u8;\n        s[ 2] =  (self.limbs[ 0] >> 16)                      as u8;\n        s[ 3] =  (self.limbs[ 0] >> 24)                      as u8;\n        s[ 4] =  (self.limbs[ 0] >> 32)                      as u8;\n        s[ 5] =  (self.limbs[ 0] >> 40)                      as u8;\n        s[ 6] = ((self.limbs[ 0] >> 48) | (self.limbs[ 1] << 4)) as u8;\n        s[ 7] =  (self.limbs[ 1] >>  4)                      as u8;\n        s[ 8] =  (self.limbs[ 1] >> 12)                      as u8;\n        s[ 9] =  (self.limbs[ 1] >> 20)                      as u8;\n        s[10] =  (self.limbs[ 1] >> 28)                      as u8;\n        s[11] =  (self.limbs[ 1] >> 36)                      as u8;\n        s[12] =  (self.limbs[ 1] >> 44)                      as u8;\n        s[13] =  (self.limbs[ 2] >>  0)                      as u8;\n        s[14] =  (self.limbs[ 2] >>  8)                      as u8;\n        s[15] =  (self.limbs[ 2] >> 16)                      as u8;\n        s[16] =  (self.limbs[ 2] >> 24)                      as u8;\n        s[17] =  (self.limbs[ 2] >> 32)                      as u8;\n        s[18] =  (self.limbs[ 2] >> 40)                      as u8;\n        s[19] = ((self.limbs[ 2] >> 48) | (self.limbs[ 3] << 4)) as u8;\n        s[20] =  (self.limbs[ 3] >>  4)                      as u8;\n        s[21] =  (self.limbs[ 3] >> 12)                      as u8;\n        s[22] =  (self.limbs[ 3] >> 20)                      as u8;\n        s[23] =  (self.limbs[ 3] >> 28)                      as u8;\n        s[24] =  (self.limbs[ 3] >> 36)                      as u8;\n        s[25] =  (self.limbs[ 3] >> 44)                      as u8;\n        s[26] =  (self.limbs[ 4] >>  0)                      as u8;\n        s[27] =  (self.limbs[ 4] >>  8)                      as u8;\n        s[28] =  (self.limbs[ 4] >> 16)                      as u8;\n        s[29] =  (self.limbs[ 4] >> 24)                      as u8;\n        s[30] =  (self.limbs[ 4] >> 32)                      as u8;\n        s[31] =  (self.limbs[ 4] >> 40)                      as u8;\n\n        assume(false); // TODO: complete the proof\n\n        s\n    }\n\n    /// Compute `a + b` (mod l)\n    pub fn add(a: &Scalar52, b: &Scalar52) -> (s: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n        forall|i: int| 0 <= i < 5 ==>  b.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&s.limbs) == to_nat(&a.limbs) + to_nat(&b.limbs),\n    {\n        //let mut sum = Scalar52::ZERO;\n        let mut sum = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\n        proof {\n            assert(Scalar52::ZERO == Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] });\n            assert(sum == Scalar52::ZERO);\n            assert(1u64 << 52 > 0) by (bit_vector);\n        }\n        let mask = (1u64 << 52) - 1;\n\n        // a + b\n        let mut carry: u64 = 0;\n        proof {\n            assert(carry == 0u64);\n            assert(1u64 << 54 < u64::MAX) by (bit_vector);\n            assert(0u64 < (1u64 << 54)) by (bit_vector);\n        }\n        for i in 0..5\n           invariant //0 <= i <= 5,\n           // forall|j: int| 0 <= j < i ==> sum.limbs[j] < 1u64 << 52,\n            (0 <= i < 5) ==> a.limbs[i as int] < (1u64 << 52),\n            (0 <= i < 5) ==> b.limbs[i as int] < (1u64 << 52),\n            carry < (1u64 << 54),\n        {\n            proof {\n                assert(0 <= i < 5);\n                assert(a.limbs[i as int] < 1u64 << 52);\n                assert(b.limbs[i as int] < 1u64 << 52);\n                assert((1u64 << 52) + (1u64 << 52) == (1u64 << 53)) by (bit_vector);\n                assert(a.limbs[i as int] + b.limbs[i as int] < 1u64 << 53);\n                assert(carry < (1u64 << 54));\n                assert(carry >> 52 >= 0u64);\n                assert((carry >> 52) < (1u64 << 54)) by (bit_vector);\n                assert((1u64 << 53) + 3 < (1u64 << 54)) by (bit_vector);\n                assert((1u64 << 53) + (1u64 << 54) <= (1u64 << 55)) by (bit_vector);\n                assert((a.limbs[i as int] + b.limbs[i as int] + (carry >> 52)) < (1u64 << 55));\n            }\n            carry = a.limbs[i] + b.limbs[i] + (carry >> 52);\n            sum.limbs[i] = carry & mask;\n            assume( (0 <= i < 5) ==> a.limbs[i as int] < (1u64 << 52));\n            assume( (0 <= i < 5) ==> b.limbs[i as int] < (1u64 << 52));\n            assume(false);\n        }\n\n        // subtract l if the sum is >= l\n\n        /*** BEGIN: ADAPTED CODE BLOCK ***/\n\n        /* ORIGINAL CODE */\n        /*let mut s = Scalar52::sub(&sum, &Self::L);*/\n        /* OUR ADAPTED CODE FOR VERUS; PROVED EQUIVALENT TO ORIGINAL CODE */\n        let l_value = Scalar52 { limbs: [0x0002631a5cf5d3ed, 0x000dea2f79cd6581, 0x000000000014def9, 0x0000000000000000, 0x0000100000000000] };\n        assert(to_nat(&l_value.limbs) == to_nat(&L.limbs));\n        assume(false); // TODO: complete the proof\n\n        Scalar52::sub(&sum, &l_value)\n\n        /*** END: ADAPTED CODE BLOCK ***/\n\n    }\n\n    /// Compute `a - b` (mod l)\n    pub fn sub(a: &Scalar52, b: &Scalar52) -> (s: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n        forall|i: int| 0 <= i < 5 ==> b.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&s.limbs) == to_nat(&a.limbs) - to_nat(&b.limbs),\n    {\n        //let mut difference = Scalar52::ZERO;\n         let mut difference = Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] };\n        proof {\n            assert(Scalar52::ZERO == Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] });\n            assert(difference == Scalar52::ZERO);\n            assert(1u64 << 52 > 0) by (bit_vector);\n        }\n        let mask = (1u64 << 52) - 1;\n\n        // a - b\n        let mut borrow: u64 = 0;\n        for i in 0..5\n            invariant 0 <= i <= 5,\n                      forall|j: int| 0 <= j < 5 ==> b.limbs[j] < (1u64 << 52),\n        {\n            proof {\n                assert ((borrow >> 63) < 2) by (bit_vector);\n            }\n            borrow = a.limbs[i].wrapping_sub(b.limbs[i] + (borrow >> 63));\n            difference.limbs[i] = borrow & mask;\n        }\n\n        // conditionally add l if the difference is negative\n        let mut carry: u64 = 0;\n        for i in 0..5 {\n            let underflow = Choice::from((borrow >> 63) as u8);\n          /*** BEGIN: ADAPTED CODE BLOCK ***/\n          // ORIGINAL CODE\n         //   let addend = u64::conditional_select(&0, &constants::L[i], underflow);\n        // OUR ADAPTED CODE FOR VERUS\n            let addend = select(&0, &L.limbs[i], underflow);\n        /*** END: ADAPTED CODE BLOCK ***/\n            assume (carry >> 52 < 2);\n            assume (difference.limbs[i as int] < 1 << 52);\n            assume (L.limbs[i as int] < 1 << 52);\n            carry = (carry >> 52) + difference.limbs[i] + addend;\n            difference.limbs[i] = carry & mask;\n        }\n        assume(false); // TODO: complete the proof\n        difference\n    }\n\n    /// Compute `a * b`\n    #[inline(always)]\n    #[rustfmt::skip] // keep alignment of z[*] calculations\n    pub (crate) fn mul_internal(a: &Scalar52, b: &Scalar52) -> (z: [u128; 9])\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n        forall|i: int| 0 <= i < 5 ==> b.limbs[i] < (1u64 << 52),\n    ensures\n        slice128_to_nat(&z) == to_nat(&a.limbs) * to_nat(&b.limbs),\n    {\n        let mut z = [0u128; 9];\n\n        z[0] = m(a.limbs[0], b.limbs[0]);\n\n        proof {\n            // Each m() result is < 2^104\n            // Sum: 2^104 + 2^104 = 2^105 < 2^128\n            assert((1u128 << 104) + (1u128 << 104) == (1u128 << 105)) by (bit_vector);\n        }\n        z[1] = m(a.limbs[0], b.limbs[1]) + m(a.limbs[1], b.limbs[0]);\n\n        proof {\n            // Each m() result is < 2^104\n            // Sum: 3 * 2^104 = 3 * 2^104 < 2^106 < 2^128\n            assert(3u128 * (1u128 << 104) < (1u128 << 106)) by (bit_vector);\n        }\n        z[2] = m(a.limbs[0], b.limbs[2]) + m(a.limbs[1], b.limbs[1]) + m(a.limbs[2], b.limbs[0]);\n\n        proof {\n            // Each m() result is < 2^104\n            // Sum: 4 * 2^104 = 2^2 * 2^104 = 2^106 < 2^128\n\n            assert(4u128 * (1u128 << 104) == (1u128 << 2) * (1u128 << 104)) by (bit_vector);\n            assert((1u128 << 2) * (1u128 << 104) == (1u128 << 106)) by (bit_vector);\n        }\n        z[3] = m(a.limbs[0], b.limbs[3]) + m(a.limbs[1], b.limbs[2]) + m(a.limbs[2], b.limbs[1]) + m(a.limbs[3], b.limbs[0]);\n\n        proof {\n            // Each m() result is < 2^104\n            // Sum: 5 * 2^104 < 8 * 2^104 = 2^3 * 2^104 = 2^107 < 2^128\n            assert(8u128 == (1u128 << 3)) by (bit_vector);\n            assert((1u128 << 3) * (1u128 << 104) == (1u128 << 107)) by (bit_vector);\n        }\n        z[4] = m(a.limbs[0], b.limbs[4]) + m(a.limbs[1], b.limbs[3]) + m(a.limbs[2], b.limbs[2]) + m(a.limbs[3], b.limbs[1]) + m(a.limbs[4], b.limbs[0]);\n        z[5] =                 m(a.limbs[1], b.limbs[4]) + m(a.limbs[2], b.limbs[3]) + m(a.limbs[3], b.limbs[2]) + m(a.limbs[4], b.limbs[1]);\n        z[6] =                                 m(a.limbs[2], b.limbs[4]) + m(a.limbs[3], b.limbs[3]) + m(a.limbs[4], b.limbs[2]);\n        z[7] =                                                 m(a.limbs[3], b.limbs[4]) + m(a.limbs[4], b.limbs[3]);\n        z[8] =                                                                 m(a.limbs[4], b.limbs[4]);\n\n        proof {\n            assert(five_limbs_to_nat_aux(a.limbs) * five_limbs_to_nat_aux(b.limbs) == nine_limbs_to_nat_aux(&z)) by {\n                broadcast use group_mul_is_commutative_and_distributive;\n                broadcast use lemma_mul_is_associative;\n\n                lemma_pow2_adds(52, 52);\n                lemma_pow2_adds(52, 104);\n                lemma_pow2_adds(52, 156);\n                lemma_pow2_adds(52, 208);\n                lemma_pow2_adds(104, 104);\n                lemma_pow2_adds(104, 156);\n                lemma_pow2_adds(104, 208);\n                lemma_pow2_adds(156, 156);\n                lemma_pow2_adds(156, 208);\n                lemma_pow2_adds(208, 208);\n            };\n            lemma_nine_limbs_equals_slice128_to_nat(&z);\n            lemma_five_limbs_equals_to_nat(&a.limbs);\n            lemma_five_limbs_equals_to_nat(&b.limbs);\n        }\n\n        z\n    }\n\n    /// Compute `a^2`\n    #[inline(always)]\n    #[rustfmt::skip] // keep alignment of calculations\n    pub (crate) fn square_internal(a: &Scalar52) -> (z: [u128; 9])\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n    ensures\n        slice128_to_nat(&z) == to_nat(&a.limbs) * to_nat(&a.limbs),\n    {\n        let mut z = [0u128; 9];\n\n        z[0] = m(a.limbs[0], a.limbs[0]);\n\n        proof {\n            // m() ensures its result is < 2^104\n            // Since m_result < 2^104, we have m_result * 2 < 2^105\n            // and 2^105 is well within u128 bounds\n            assert((1u128 << 104) * 2 == (1u128 << 105)) by (bit_vector);\n        }\n        z[1] = m(a.limbs[0], a.limbs[1]) * 2;\n\n        proof {\n            // Each m() result is < 2^104\n            // m_term1 * 2 < 2^105\n\n            // Sum: 2^105 + 2^104 = 3 * 2^104 < 2^106 < 2^128\n            assert((1u128 << 105) + (1u128 << 104) < (1u128 << 106)) by (bit_vector);\n        }\n        z[2] = m(a.limbs[0], a.limbs[2]) * 2 + m(a.limbs[1], a.limbs[1]);\n\n        proof {\n            // Each m() result is < 2^104\n            // Each * 2 gives < 2^105\n\n            // Sum: 2^105 + 2^105 = 2^106 < 2^128\n            assert((1u128 << 105) + (1u128 << 105) == (1u128 << 106)) by (bit_vector);\n        }\n        z[3] = m(a.limbs[0], a.limbs[3]) * 2 + m(a.limbs[1], a.limbs[2]) * 2;\n\n        proof {\n            // Each m() result is < 2^104\n            // First two terms * 2 give < 2^105\n\n            // Sum: 2^105 + 2^105 + 2^104 = 2^106 + 2^104 < 2^107 < 2^128\n            assert((1u128 << 106) + (1u128 << 104) < (1u128 << 107)) by (bit_vector);\n        }\n        z[4] = m(a.limbs[0], a.limbs[4]) * 2 + m(a.limbs[1], a.limbs[3]) * 2 + m(a.limbs[2], a.limbs[2]);\n        z[5] =                 m(a.limbs[1], a.limbs[4]) * 2 + m(a.limbs[2], a.limbs[3]) * 2;\n        z[6] =                                 m(a.limbs[2], a.limbs[4]) * 2 + m(a.limbs[3], a.limbs[3]);\n        z[7] =                                                 m(a.limbs[3], a.limbs[4]) * 2;\n        z[8] =                                                                 m(a.limbs[4], a.limbs[4]);\n\n        proof {\n\n            assert(five_limbs_to_nat_aux(a.limbs) * five_limbs_to_nat_aux(a.limbs) == nine_limbs_to_nat_aux(&z)) by {\n                broadcast use group_mul_is_commutative_and_distributive;\n                broadcast use lemma_mul_is_associative;\n\n                lemma_pow2_adds(52, 52);\n                lemma_pow2_adds(52, 104);\n                lemma_pow2_adds(52, 156);\n                lemma_pow2_adds(52, 208);\n                lemma_pow2_adds(104, 104);\n                lemma_pow2_adds(104, 156);\n                lemma_pow2_adds(104, 208);\n                lemma_pow2_adds(156, 156);\n                lemma_pow2_adds(156, 208);\n                lemma_pow2_adds(208, 208);\n            };\n            lemma_nine_limbs_equals_slice128_to_nat(&z);\n            lemma_five_limbs_equals_to_nat(&a.limbs);\n        }\n\n        z\n    }\n\n    /// Compute `a * b` (mod l)\n    #[inline(never)]\n    pub fn mul(a: &Scalar52, b: &Scalar52) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n        forall|i: int| 0 <= i < 5 ==> b.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&result.limbs) == (to_nat(&a.limbs) * to_nat(&b.limbs)) % group_order(),\n    {\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\n        let ab = Scalar52::montgomery_reduce(&Scalar52::mul_internal(a, b));\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(&ab, &RR))\n    }\n\n    /// Compute `a^2` (mod l)\n    #[inline(never)]\n    pub fn square(&self) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> self.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&result.limbs) == (to_nat(&self.limbs) * to_nat(&self.limbs)) % group_order(),\n    {\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\n        let aa = Scalar52::montgomery_reduce(&Scalar52::square_internal(self));\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(&aa, &RR))\n    }\n\n    /// Compute `(a * b) / R` (mod l), where R is the Montgomery modulus 2^260\n    #[inline(never)]\n    pub fn montgomery_mul(a: &Scalar52, b: &Scalar52) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> a.limbs[i] < (1u64 << 52),\n        forall|i: int| 0 <= i < 5 ==> b.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&result.limbs) == (to_nat(&a.limbs) * to_nat(&b.limbs)) % group_order(),\n    {\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\n        Scalar52::montgomery_reduce(&Scalar52::mul_internal(a, b))\n    }\n\n    /// Compute `(a^2) / R` (mod l) in Montgomery form, where R is the Montgomery modulus 2^260\n    #[inline(never)]\n    pub fn montgomery_square(&self) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> self.limbs[i] < (1u64 << 52),\n    ensures\n        to_nat(&result.limbs) == (to_nat(&self.limbs) * to_nat(&self.limbs)) % group_order(),\n    {\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\n        Scalar52::montgomery_reduce(&Scalar52::square_internal(self))\n    }\n\n    /// Helper function for part1 of Montgomery reduction\n    #[inline(always)]\n    fn montgomery_part1(sum: u128) -> (u128, u64)\n    {\n        assume(false); // TODO: Add proper bounds checking and proofs\n        let p = (sum as u64).wrapping_mul(LFACTOR) & ((1u64 << 52) - 1);\n        let carry = (sum + m(p, L.limbs[0])) >> 52;\n        (carry, p)\n    }\n\n    /// Helper function for part2 of Montgomery reduction\n    #[inline(always)]\n    fn montgomery_part2(sum: u128) -> (u128, u64)\n    {\n        assume(false); // TODO: Add proper bounds checking and proofs\n        let w = (sum as u64) & ((1u64 << 52) - 1);\n        let carry = sum >> 52;\n        (carry, w)\n    }\n\n    /// Montgomery reduction: reduces a 9-limb number to a 5-limb scalar\n    /// This is the core of Montgomery arithmetic - it computes (x / R) mod L\n    /// where R = 2^260 and L is the scalar field order\n    pub (crate) fn montgomery_reduce(limbs: &[u128; 9]) -> (result: Scalar52)\n    ensures\n        // TODO: Add proper specification for Montgomery reduction\n        true,\n    {\n        assume(false); // TODO: Add proper bounds checking and proofs\n        // First half: compute Montgomery adjustment factor n and add n*L to make limbs divisible by R\n        let (carry, n0) = Scalar52::montgomery_part1(limbs[0]);\n        let (carry, n1) = Scalar52::montgomery_part1(carry + limbs[1] + m(n0, L.limbs[1]));\n        let (carry, n2) = Scalar52::montgomery_part1(carry + limbs[2] + m(n0, L.limbs[2]) + m(n1, L.limbs[1]));\n        let (carry, n3) = Scalar52::montgomery_part1(carry + limbs[3] + m(n1, L.limbs[2]) + m(n2, L.limbs[1]));\n        let (carry, n4) = Scalar52::montgomery_part1(carry + limbs[4] + m(n0, L.limbs[4]) + m(n2, L.limbs[2]) + m(n3, L.limbs[1]));\n\n        // Second half: limbs is now divisible by R, so divide by R by taking upper half\n        let (carry, r0) = Scalar52::montgomery_part2(carry + limbs[5] + m(n1, L.limbs[4]) + m(n3, L.limbs[2]) + m(n4, L.limbs[1]));\n        let (carry, r1) = Scalar52::montgomery_part2(carry + limbs[6] + m(n2, L.limbs[4]) + m(n4, L.limbs[2]));\n        let (carry, r2) = Scalar52::montgomery_part2(carry + limbs[7] + m(n3, L.limbs[4]));\n        let (carry, r3) = Scalar52::montgomery_part2(carry + limbs[8] + m(n4, L.limbs[4]));\n        let r4 = carry as u64;\n\n        // Result may be >= L, so attempt to subtract L\n        let result = Scalar52 { limbs: [r0, r1, r2, r3, r4] };\n        Scalar52::sub(&result, &L)\n    }\n\n    /// Puts a Scalar52 into Montgomery form, i.e. computes `a*R (mod L)`\n    #[inline(never)]\n    pub fn as_montgomery(&self) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> self.limbs[i] < (1u64 << 52),\n    ensures\n        // TODO: Add proper specification for Montgomery form conversion\n        true,\n    {\n        assume(false); // TODO: Add proper Montgomery arithmetic proofs\n        Scalar52::montgomery_mul(self, &RR)\n    }\n\n    /// Takes a Scalar52 out of Montgomery form, i.e. computes `a/R (mod L)`\n    #[allow(clippy::wrong_self_convention)]\n    #[inline(never)]\n    pub fn from_montgomery(&self) -> (result: Scalar52)\n    requires\n        forall|i: int| 0 <= i < 5 ==> self.limbs[i] < (1u64 << 52),\n    ensures\n        // TODO: Add proper specification for Montgomery form conversion\n        true,\n    {\n        let mut limbs = [0u128; 9];\n        #[allow(clippy::needless_range_loop)]\n        for i in 0..5 {\n            limbs[i] = self.limbs[i] as u128;\n        }\n        Scalar52::montgomery_reduce(&limbs)\n    }\n\n\n    /// Inverts the scalar using Montgomery logic (simplified)\n    pub fn invert(&self) -> Scalar52 {\n        // TODO\n        Scalar52 { limbs: [0u64, 0u64, 0u64, 0u64, 0u64] }\n    }\n\n    /// Verification: scalar * scalar.invert() ≡ 1 mod L\n    proof fn verify_invert_correct(&self)\n   //     requires to_scalar(&self.limbs) != 0\n    //    ensures (to_scalar(&self.limbs) * invert_spec(&self.limbs)) % group_order() == 1\n    {\n        assume(false);\n\n    }\n\n}\n\n\nfn main()\n{\n    // Test scalar creation from bytes\n    let test_bytes: [u8; 32] = [\n        42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    ];\n\n    let scalar = Scalar52::from_bytes(&test_bytes);\n    let inv_scalar = scalar.invert();\n}\n}\n",
    "filename": "src/scalar_verus.rs",
    "filepath": null,
    "folder_id": 12249,
    "user_id": 460179
  },
  "51720": {
    "text": "// field.rs\n#![allow(unused)]\nuse vstd::arithmetic::div_mod::*;\nuse vstd::arithmetic::mul::*;\nuse vstd::arithmetic::power2::*;\nuse vstd::bits::*;\nuse vstd::prelude::*;\n\n// ADAPTED CODE LINES: X.0 globally replaced with X.limbs\n\nverus! {\n\n\n/* MANUALLY moved outside and made explicit */\n// LOW_51_BIT_MASK: u64 = (1u64 << 51) -1; originally\npub const LOW_51_BIT_MASK: u64 = 2251799813685247u64; // 2^51  -1\n\n// Basic properties of LOW_51_BIT_MASK:\n// - It's the value of low_bits_mask (spec function defined in vstd and used in its lemmas)\n// - it's less than 2^51\npub proof fn l51_bit_mask_lt()\n    ensures\n        LOW_51_BIT_MASK == low_bits_mask(51),\n        LOW_51_BIT_MASK < (1u64 << 51) as nat,\n{\n    lemma2_to64_rest();\n    assert(LOW_51_BIT_MASK < (1u64 << 51) as nat) by (compute);\n}\n\n// Auxiliary lemma for multiplication (of nat!)\npub proof fn mul_lt(a1:nat, b1:nat, a2:nat, b2:nat)\n    requires\n        a1 < b1,\n        a2 < b2,\n    ensures\n        a1 * a2 < b1 * b2,\n{\n    if (a2 == 0) {\n        assert(b1 * b2 > 0) by {\n            // a * b != 0 <==> a != 0 /\\ b != 0\n            lemma_mul_nonzero(b1 as int, b2 as int);\n        }\n    }\n    else {\n        // a1 < b1 /\\ a2 > 0 ==> a1 * a2 < b1 * a2\n        lemma_mul_strict_inequality(a1 as int, b1  as int, a2 as int);\n        // a2 < b2 /\\ b2 > 0 ==> a2 * b1 < b2 * b1\n        lemma_mul_strict_inequality(a2 as int, b2 as int, b1 as int);\n    }\n}\n\n// Auxiliary lemma for exponentiation\npub proof fn pow2_le_max64(k: nat)\n    requires\n        k < 64,\n    ensures\n        pow2(k) <= u64::MAX\n    {\n        lemma2_to64();\n        lemma2_to64_rest();\n    }\n\n// Specialization of lemma_u64_shl_is_mul for x = 1\npub proof fn shift_is_pow2(k: nat)\n    requires\n        k < 64,\n    ensures\n        (1u64 << k) == pow2(k)\n{\n    pow2_le_max64(k);\n    lemma_u64_shl_is_mul(1u64, k as u64);\n}\n\n// Masking with low_bits_mask(k) gives a value bounded by 2^k\npub proof fn masked_lt(v: u64)\n    ensures\n        v & LOW_51_BIT_MASK < (1u64 << 51),\n{\n    assert (v & 2251799813685247u64 < (1u64 << 51)) by (bit_vector);\n}\n\n// right-shifting a u64 gives at most 2^13 - 1\npub proof fn shifted_lt(v: u64)\n    ensures\n        v >> 51 < 1u64 << 13\n{\n    shift_is_pow2(13);\n    broadcast use lemma_u64_shr_is_div;\n    lemma_pow2_pos(51);\n    lemma_div_is_ordered(v as int, u64::MAX as int, pow2(51) as int);\n    assert(u64::MAX >> 51 < 1u64 << 13) by (compute);\n}\n\npub open spec fn p() -> nat {\n    (pow2(255) - 19) as nat\n}\n\n// Proof that 2^255 > 19\npub proof fn pow255_gt_19()\n    ensures\n        pow2(255) > 19\n{\n    lemma2_to64(); // 2^5\n    lemma_pow2_strictly_increases(5, 255);\n}\n\n// Evaluation function, given a field element as limbs, reconstruct the nat value it represents.\npub open spec fn as_nat(limbs: [u64; 5]) -> nat {\n    (limbs[0] as nat) +\n    pow2(51) * (limbs[1] as nat) +\n    pow2(102) * (limbs[2] as nat) +\n    pow2(153) * (limbs[3] as nat) +\n    pow2(204) * (limbs[4] as nat)\n}\n\n// Evaluation function, given a field element as limbs, reconstruct the nat value it represents.\npub open spec fn as_nat_32_u8(limbs: [u8; 32]) -> nat {\n    // Verus error: `core::iter::range::impl&%15::fold` is not supported\n    // we write them out manually\n    (limbs[0] as nat) +\n    pow2( 1 * 8) * (limbs[ 1] as nat) +\n    pow2( 2 * 8) * (limbs[ 2] as nat) +\n    pow2( 3 * 8) * (limbs[ 3] as nat) +\n    pow2( 4 * 8) * (limbs[ 4] as nat) +\n    pow2( 5 * 8) * (limbs[ 5] as nat) +\n    pow2( 6 * 8) * (limbs[ 6] as nat) +\n    pow2( 7 * 8) * (limbs[ 7] as nat) +\n    pow2( 8 * 8) * (limbs[ 8] as nat) +\n    pow2( 9 * 8) * (limbs[ 9] as nat) +\n    pow2(10 * 8) * (limbs[10] as nat) +\n    pow2(11 * 8) * (limbs[11] as nat) +\n    pow2(12 * 8) * (limbs[12] as nat) +\n    pow2(13 * 8) * (limbs[13] as nat) +\n    pow2(14 * 8) * (limbs[14] as nat) +\n    pow2(15 * 8) * (limbs[15] as nat) +\n    pow2(16 * 8) * (limbs[16] as nat) +\n    pow2(17 * 8) * (limbs[17] as nat) +\n    pow2(18 * 8) * (limbs[18] as nat) +\n    pow2(19 * 8) * (limbs[19] as nat) +\n    pow2(20 * 8) * (limbs[20] as nat) +\n    pow2(21 * 8) * (limbs[21] as nat) +\n    pow2(22 * 8) * (limbs[22] as nat) +\n    pow2(23 * 8) * (limbs[23] as nat) +\n    pow2(24 * 8) * (limbs[24] as nat) +\n    pow2(25 * 8) * (limbs[25] as nat) +\n    pow2(26 * 8) * (limbs[26] as nat) +\n    pow2(27 * 8) * (limbs[27] as nat) +\n    pow2(28 * 8) * (limbs[28] as nat) +\n    pow2(29 * 8) * (limbs[29] as nat) +\n    pow2(30 * 8) * (limbs[30] as nat) +\n    pow2(31 * 8) * (limbs[31] as nat)\n}\n\n// Lemma: If a > b pointwise, then as_nat(a - b) = as_nat(a) - as_nat(b)\npub proof fn lemma_as_nat_sub(a: [u64;5], b: [u64;5])\n    requires\n        forall |i:int| 0 <= i < 5 ==> b[i] < a[i]\n    ensures\n        as_nat([\n            (a[0] - b[0]) as u64,\n            (a[1] - b[1]) as u64,\n            (a[2] - b[2]) as u64,\n            (a[3] - b[3]) as u64,\n            (a[4] - b[4]) as u64\n        ]) == as_nat(a) - as_nat(b)\n{\n    let c: [u64;5] = [\n        (a[0] - b[0]) as u64,\n        (a[1] - b[1]) as u64,\n        (a[2] - b[2]) as u64,\n        (a[3] - b[3]) as u64,\n        (a[4] - b[4]) as u64\n    ];\n    // distribute pow2\n    assert( as_nat(c) ==\n        (a[0] - b[0]) +\n        pow2(51) * a[1] - pow2(51) * b[1] +\n        pow2(102) * a[2] - pow2(102) * b[2] +\n        pow2(153) * a[3] - pow2(153) * b[3] +\n        pow2(204) * a[4] - pow2(204) * b[4]\n    ) by {\n        broadcast use lemma_mul_is_distributive_sub;\n    }\n}\n\n// Auxiliary lemma; shift is division (for 51 fixed)\npub proof fn lemma_shift(ai: u64, v: u64)\n    requires\n        ai == v >> 51\n    ensures\n        ai == (v as nat) / pow2(51)\n{\n    lemma_u64_shr_is_div(v, 51);\n}\n\n// Auxiliary lemma; mask is mod (for 51 fixed)\npub proof fn lemma_mask(bi: u64, v: u64)\n    requires\n        bi == v & LOW_51_BIT_MASK\n    ensures\n        bi == v % (pow2(51) as u64)\n{\n    l51_bit_mask_lt();\n    lemma_u64_low_bits_mask_is_mod(v, 51);\n}\n\n// Combination of the above lemmas, and the basic div/mod property that a = d * (a/d) + a % d\npub proof fn lemma_div_and_mod(ai:u64, bi: u64, v: u64)\n    requires\n        ai == v >> 51,\n        bi == v & LOW_51_BIT_MASK\n    ensures\n        ai == (v as nat) / pow2(51),\n        bi == v % (pow2(51) as u64),\n        v == ai * pow2(51) + bi\n{\n    lemma_shift(ai, v);\n    lemma_mask(bi, v);\n    lemma_pow2_pos(51); // pow2(51) != 0\n    assert(pow2(51) <= u64::MAX) by {\n        lemma2_to64_rest();\n    }\n    lemma_fundamental_div_mod(v as int, pow2(51) as int);\n}\n\n// Rewriting lemma; 2^((t + 1) * 51) * x = 2^(t*51) * (2^51 * x)\n// Parenthesis placement matters here\npub proof fn lemma_two_factoring(k : nat, ai: u64)\n    ensures\n        pow2(k + 51) * ai == pow2(k) * (pow2(51) * ai)\n{\n    lemma_pow2_adds(k, 51);\n    lemma_mul_is_associative(pow2(k) as int, pow2(51) as int, ai as int);\n}\n\npub open spec fn spec_reduce(limbs: [u64; 5]) -> (r: [u64; 5]) {\n    let r = [\n        ((limbs[0] & LOW_51_BIT_MASK) + (limbs[4] >> 51) * 19) as u64,\n        ((limbs[1] & LOW_51_BIT_MASK) + (limbs[0] >> 51)) as u64,\n        ((limbs[2] & LOW_51_BIT_MASK) + (limbs[1] >> 51)) as u64,\n        ((limbs[3] & LOW_51_BIT_MASK) + (limbs[2] >> 51)) as u64,\n        ((limbs[4] & LOW_51_BIT_MASK) + (limbs[3] >> 51)) as u64,\n    ];\n    r\n}\n\n// Each component of spec_reduce is bounded.\n// The reason we _don't_ write\n// ensures forall |i: int| 0 <= i < 5 ==> spec_reduce(limbs)[i] < (1u64 << 52)\n// is that the solver treats `spec_reduce`` above as symbolic and does _not_ instantiate e.g.\n// ((limbs[4] & LOW_51_BIT_MASK) + (limbs[3] >> 51)) as u64 < (1u64 << 52)\npub proof fn lemma_boundaries(limbs: [u64; 5])\n    ensures\n        ((limbs[0] & LOW_51_BIT_MASK) + (limbs[4] >> 51) * 19) < (1u64 << 52),\n        ((limbs[1] & LOW_51_BIT_MASK) + (limbs[0] >> 51)) < (1u64 << 52),\n        ((limbs[2] & LOW_51_BIT_MASK) + (limbs[1] >> 51)) < (1u64 << 52),\n        ((limbs[3] & LOW_51_BIT_MASK) + (limbs[2] >> 51)) < (1u64 << 52),\n        ((limbs[4] & LOW_51_BIT_MASK) + (limbs[3] >> 51)) < (1u64 << 52)\n\n{\n    // \\A i. limbs[i] < 2^13\n    shifted_lt(limbs[0]);\n    shifted_lt(limbs[1]);\n    shifted_lt(limbs[2]);\n    shifted_lt(limbs[3]);\n    shifted_lt(limbs[4]);\n\n    // \\A i. limbs[i] & LOW_51_BIT_MASK < 2^51\n    masked_lt(limbs[0]);\n    masked_lt(limbs[1]);\n    masked_lt(limbs[2]);\n    masked_lt(limbs[3]);\n    masked_lt(limbs[4]);\n\n    // Since 19 < 2^5 and (limbs[4] >> 51) < 2^13, their product is less than 2^18\n    assert((limbs[4] >> 51) * 19 < (1u64 << 18) as nat) by {\n        assert(19 < (1u64 << 5)) by (bit_vector);\n        shift_is_pow2(5);\n        shift_is_pow2(13);\n        shift_is_pow2(18);\n        lemma_pow2_adds(13, 5);\n        // If (limbs[4] >> 51) < 2^13 and 19 < 2^5 then their product is less than 2^18\n        mul_lt((limbs[4] >> 51) as nat, (1u64 << 13) as nat, 19nat, (1u64 << 5) as nat);\n    }\n\n    // The final values (limbs[i] += cX) are all bounded by 2^51 + eps, for eps \\in {2^18, 2^13}.\n    assert(((1u64 << 18)) + (1u64 << 51) < (1u64 << 52)) by (bit_vector);\n    assert(((1u64 << 13)) + (1u64 << 51) < (1u64 << 52)) by (bit_vector);\n\n    // In summary, they're all bounded by 2^52\n    // The solver can prove this automatically\n}\n\npub proof fn lemma_reduce(limbs: [u64; 5])\n    ensures\n        forall|i: int| 0 <= i < 5 ==> spec_reduce(limbs)[i] < (1u64 << 52),\n        // Suppose l = (l0, l1, l2, l3, l4) are the input limbs.\n        // They represent a number\n        // e(l) =  l0 + l1 * 2^51 + l2 * 2^102 + l3 * 2^153 + l4 * 2^204\n        // in Z_p, for p = 2^255 - 19\n        // reduce(l) returns v = (v0, v1, v2, v3, v4), such that\n        // v0 = 19 * a4 + b0\n        // v1 =      a0 + b1\n        // v2 =      a1 + b2\n        // v3 =      a2 + b3\n        // v4 =      a3 + b4\n        // where ai = li >> 51 and bi = li & LOW_51_BIT_MASK\n        // we can reformulate this as ai = li / 2^51 (flooring division) and bi = li % 2^51\n        // Using the following identity connecting integer division and remainder:\n        // x = y * (x / y) + x % y\n        // we can see that li = ai * 2^51 + bi\n        // Plugging the above identities into the equations for v, we can observe that\n        // e(v) = e(l) - p * (l4 >> 51)\n        // IOW, e(reduce(l)) = e(l) (mod p)\n        // additionally, if all limbs are below 2^51, reduce(l) = l\n        (forall|i: int| 0 <= i < 5 ==> limbs[i] < (1u64 << 51)) ==> (spec_reduce(limbs) =~= limbs),\n        as_nat(spec_reduce(limbs)) == as_nat(limbs) - p() * (limbs[4] >> 51)\n{\n\n    // -----\n    // reduce identity for small limbs\n\n    // Can't seem to reference r within this proof block, we reconstruct it here\n    let rr: [u64; 5] = spec_reduce(limbs);\n\n    assert((forall|i: int| 0 <= i < 5 ==> #[trigger] limbs[i] < (1u64 << 51)) ==> (rr =~= limbs)) by {\n        if (forall|i: int| 0 <= i < 5 ==> #[trigger] limbs[i] < (1u64 << 51)) {\n            assert forall|i: int| 0 <= i < 5 implies #[trigger] limbs[i] & LOW_51_BIT_MASK == limbs[i] by {\n                l51_bit_mask_lt(); // LOW_51_BIT_MASK = low_bits_mask(51)\n                shift_is_pow2(51);\n                lemma_u64_low_bits_mask_is_mod(limbs[i], 51);\n                lemma_small_mod(limbs[i] as nat, pow2(51));\n            }\n            assert forall|i: int| 0 <= i < 5 implies #[trigger] limbs[i] >> 51 == 0 by {\n                l51_bit_mask_lt(); // LOW_51_BIT_MASK = low_bits_mask(51)\n                shift_is_pow2(51);\n                lemma_u64_shr_is_div(limbs[i], 51);\n                lemma_basic_div(limbs[i] as int, pow2(51) as int);\n            }\n        }\n    }\n\n    // -- as_nat identity\n\n    // ai = limbs[i] / 2^52\n    let a0 = (limbs[0] >> 51);\n    let a1 = (limbs[1] >> 51);\n    let a2 = (limbs[2] >> 51);\n    let a3 = (limbs[3] >> 51);\n    let a4 = (limbs[4] >> 51);\n\n    // bi = limbs[i] % 2^52\n    let b0 = (limbs[0] & LOW_51_BIT_MASK);\n    let b1 = (limbs[1] & LOW_51_BIT_MASK);\n    let b2 = (limbs[2] & LOW_51_BIT_MASK);\n    let b3 = (limbs[3] & LOW_51_BIT_MASK);\n    let b4 = (limbs[4] & LOW_51_BIT_MASK);\n\n    lemma_boundaries(limbs);\n\n    // distribute\n    assert(as_nat(rr) ==\n        19 *  a4 + b0 +\n        pow2(51) * a0 + pow2(51) * b1 +\n        pow2(102) * a1 + pow2(102) * b2 +\n        pow2(153) * a2 + pow2(153) * b3 +\n        pow2(204) * a3 + pow2(204) * b4\n    ) by {\n        broadcast use lemma_mul_is_distributive_add;\n    }\n\n    // factor out\n    assert(as_nat(rr) ==\n        19 *  a4 + b0 +\n        pow2(51) * a0 + pow2(51) * b1 +\n        pow2(51) * (pow2(51) * a1) + pow2(102) * b2 +\n        pow2(102) * (pow2(51) * a2) + pow2(153) * b3 +\n        pow2(153) * (pow2(51) * a3) + pow2(204) * b4\n    ) by {\n        lemma_two_factoring(51, a1);\n        lemma_two_factoring(102, a2);\n        lemma_two_factoring(153, a3);\n    }\n\n    // change groupings\n    assert(as_nat(rr) ==\n        (b0 + pow2(51) * a0) +\n        pow2(51) * (b1 + pow2(51) * a1) +\n        pow2(102) * (b2 + pow2(51) * a2) +\n        pow2(153) * (b3 + pow2(51) * a3) +\n        pow2(204) * b4 + 19 * a4\n    ) by {\n        broadcast use lemma_mul_is_distributive_add;\n    }\n\n    // invoke div/mod identity\n    assert(as_nat(rr) ==\n        limbs[0] +\n        pow2(51) * limbs[1] +\n        pow2(102) * limbs[2] +\n        pow2(153) * limbs[3] +\n        pow2(204) * b4 + 19 * a4\n    ) by {\n        lemma_div_and_mod(a0, b0, limbs[0]);\n        lemma_div_and_mod(a1, b1, limbs[1]);\n        lemma_div_and_mod(a2, b2, limbs[2]);\n        lemma_div_and_mod(a3, b3, limbs[3]);\n    }\n\n    // Add missing limbs[4] parts\n    assert(as_nat(rr) ==\n        limbs[0] +\n        pow2(51) * limbs[1] +\n        pow2(102) * limbs[2] +\n        pow2(153) * limbs[3] +\n        pow2(204) * limbs[4] - pow2(204) * (pow2(51) * a4 ) + 19 * a4\n    ) by {\n        lemma_div_and_mod(a4, b4, limbs[4]);\n        assert(pow2(204) * limbs[4] == pow2(204) * b4 + pow2(204)* (pow2(51) * a4)) by {\n            lemma_mul_is_distributive_add(pow2(204) as int, pow2(51) * a4 as int, b4 as int);\n        }\n    }\n\n    // The solver can collect components of as_nat(limbs) automatically:\n    // as_nat(rr) == as_nat(limbs) - pow2(204) * (pow2(51) * a4 ) + 19 * a4\n    // ... as well as pull in minus\n    // as_nat(rr) == as_nat(limbs) - (pow2(204) * (pow2(51) * a4 ) - 19 * a4)\n\n    // collect components of p() * a4\n    assert(pow2(204) * (pow2(51) * a4) - 19 * a4 == p() * a4) by {\n        lemma_mul_is_associative(pow2(204) as int, pow2(51) as int, a4 as int);\n        lemma_pow2_adds(204, 51);\n        lemma_mul_is_distributive_sub_other_way(a4 as int, pow2(255) as int, 19 );\n        pow255_gt_19(); // we need to prove 2^255 - 19 doesn't underflow\n    }\n}\n\npub proof fn lemma_add_then_shift(a: u64, b: u64)\n    requires\n        a < (1u64 << 52),\n        b < (1u64 << 52)\n    ensures\n        (a + b) < (1u64 << 53),\n        ((a + b) as u64 >> 51) < 4\n{\n    lemma2_to64_rest();\n    assert((a + b) < 1u64 << 53) by {\n        assert((1u64 << 52) + (1u64 << 52) == 1u64 << 53) by (compute);\n    }\n    assert(1u64 << 53 == (1u64 << 51) * 4) by (bit_vector);\n    // 0 < b  /\\ a < b * c => a/b < c\n    lemma_multiply_divide_lt((a + b) as int, (1u64 << 51) as int, 4int);\n    shift_is_pow2(51);\n    shift_is_pow2(53);\n    assert((a + b) as u64 >> 51 == (a + b) as u64 / (pow2(51) as u64)) by {\n        lemma_u64_shr_is_div((a + b) as u64, 51);\n    }\n    assert(pow2(53) / pow2(51) == 4) by {\n        lemma_pow2_subtracts(51, 53);\n    }\n}\n\n/* MANUALLY moved outside, named return value */\nconst fn load8_at(input: &[u8], i: usize) -> (r: u64)\n    requires\n        i + 7 < input.len(),\n    ensures\n        0 <= r <= (u64::MAX as nat),\n{\n        (input[i] as u64)\n    | ((input[i + 1] as u64) << 8)\n    | ((input[i + 2] as u64) << 16)\n    | ((input[i + 3] as u64) << 24)\n    | ((input[i + 4] as u64) << 32)\n    | ((input[i + 5] as u64) << 40)\n    | ((input[i + 6] as u64) << 48)\n    | ((input[i + 7] as u64) << 56)\n}\n\n/* MANUALLY moved outside */\n#[inline(always)]\nfn m(x: u64, y: u64) -> (r: u128)\n    requires\n        (x as nat) * (y as nat) < (u128::MAX as nat),\n    ensures\n        (r as nat) == (x as nat) * (y as nat),\n{\n    (x as u128) * (y as u128)\n}\n\npub struct FieldElement51 {\n    // ADAPTED CODE LINE: we give a name to the field: \"limbs\"\n    pub limbs: [u64; 5],\n}\n\nimpl FieldElement51 {\n    pub(crate) const fn from_limbs(limbs: [u64; 5]) -> FieldElement51 {\n        // ADAPTED CODE LINE: limbs is now a named field\n        FieldElement51{limbs}\n    }\n\n    // Modified to use direct struct\n    pub const ZERO: FieldElement51 = FieldElement51{limbs: [0, 0, 0, 0, 0]};\n    pub const ONE: FieldElement51 = FieldElement51{limbs: [1, 0, 0, 0, 0]};\n    pub const MINUS_ONE: FieldElement51 = FieldElement51{limbs: [\n        2251799813685228,\n        2251799813685247,\n        2251799813685247,\n        2251799813685247,\n        2251799813685247,\n    ]};\n\n    /// Invert the sign of this field element\n    pub fn negate(&mut self)\n        requires\n            forall|i: int| 0 <= i < 5 ==> old(self).limbs[i] < (1u64 << 51),\n        ensures\n            forall|i: int| 0 <= i < 5 ==> self.limbs[i] < (1u64 << 52),\n            // Assume we start with l = (l0, l1, l2, l3, l4).\n            // Using c0 = 2^51 - 19 and c = 2^51 - 1, we can see that\n            // ( 36028797018963664u64 - l0,\n            //   36028797018963952u64 - l1,\n            //   36028797018963952u64 - l2,\n            //   36028797018963952u64 - l3,\n            //   36028797018963952u64 - l4 )\n            // is just 16 * (c0, c, c, c, c) - l (in vector notation)\n            // Further, as_nat((c0, c, c, c, c)) = p, so\n            // as_nat(16 * (c0, c, c, c, c) - l) is 16p - as_nat(l)\n            // We know as_nat(reduce(v)) = as_nat(v) - p * (v4 >> 51) for any v.\n            // This gives us the identity\n            // as_nat(negate(l)) = as_nat(reduce(16 * (c0, c, c, c, c) - l))\n            //                   = 16p - as_nat(l) - p * ((16c - l4) >> 51)\n            // Note that (16c - l4) >> 51 is either 14 or 15, in either case < 16.\n            as_nat(self.limbs) == 16 * p() - as_nat(old(self).limbs) - p() * ((36028797018963952u64 - old(self).limbs[4]) as u64 >> 51)\n            // Reducing mod p, this implies `as_nat(self.limbs) == - as_nat(old(self).limbs)`\n    {\n        proof {\n            let c0 = (pow2(51) - 19);\n            let c  = (pow2(51) - 1);\n            lemma2_to64_rest(); // get pow2(51)\n            // solver knows 36028797018963664u64 == 16 * c0\n            // solver knows 36028797018963952u64 == 16 * c;\n\n            assert forall |i: int| 0 <= i < 5 implies old(self).limbs[i] < 16 * c0 by {\n                shift_is_pow2(51);\n            }\n\n            // Introduce 16p as a vector\n            let v = [(16 * c0) as u64,(16 * c) as u64,(16 * c) as u64,(16 * c) as u64,(16 * c) as u64];\n\n            assert(as_nat(v) == 16 * p()) by {\n                // by definition of as_nat\n                assert( as_nat(v) ==\n                    16 * c0 +\n                    pow2(51) * (16 * c) +\n                    pow2(102) * (16 * c) +\n                    pow2(153) * (16 * c) +\n                    pow2(204) * (16 * c)\n                );\n\n                // solver can reorder factors and pull out 16 on its own\n                // ...\n\n                // Write out `c`s and sum up powers\n                assert( p() ==\n                    c0 +\n                    pow2(51) * c +\n                    pow2(102) * c +\n                    pow2(153) * c +\n                    pow2(204) * c\n                ) by {\n                    lemma_pow2_adds(51, 51);\n                    lemma_pow2_adds(51, 102);\n                    lemma_pow2_adds(51, 153);\n                    lemma_pow2_adds(51, 204);\n                }\n            }\n\n            let l0 = old(self).limbs[0];\n            let l1 = old(self).limbs[1];\n            let l2 = old(self).limbs[2];\n            let l3 = old(self).limbs[3];\n            let l4 = old(self).limbs[4];\n\n            assert(as_nat([\n                (16 * c0 - l0) as u64,\n                (16 * c - l1) as u64,\n                (16 * c - l2) as u64,\n                (16 * c - l3) as u64,\n                (16 * c - l4) as u64,\n                ]) == as_nat(v) - as_nat(old(self).limbs)\n            ) by {\n                lemma_as_nat_sub(v, old(self).limbs);\n            }\n        }\n        // See commentary in the Sub impl: (copied below)\n            // To avoid underflow, first add a multiple of p.\n            // Choose 16*p = p << 4 to be larger than 54-bit _rhs.\n            //\n            // If we could statically track the bitlengths of the limbs\n            // of every FieldElement51, we could choose a multiple of p\n            // just bigger than _rhs and avoid having to do a reduction.\n            //\n            // Since we don't yet have type-level integers to do this, we\n            // have to add an explicit reduction call here.\n        // Note on \"magic numbers\":\n        // 36028797018963664u64 = 2^55 - 304 = 16 * (2^51 - 19)\n        // 36028797018963952u64 = 2^55 - 16 =  16 * (2^51 - 1)\n        let neg = FieldElement51::reduce([\n            36028797018963664u64 - self.limbs[0],\n            36028797018963952u64 - self.limbs[1],\n            36028797018963952u64 - self.limbs[2],\n            36028797018963952u64 - self.limbs[3],\n            36028797018963952u64 - self.limbs[4],\n        ]);\n        self.limbs = neg.limbs;\n    }\n\n    /// Given 64-bit input limbs, reduce to enforce the bound 2^(51 + epsilon).\n    #[inline(always)]\n    fn reduce(mut limbs: [u64; 5]) -> (r: FieldElement51)\n        ensures\n            r.limbs == spec_reduce(limbs),\n            forall|i: int| 0 <= i < 5 ==> r.limbs[i] < (1u64 << 52),\n            (forall|i: int| 0 <= i < 5 ==> limbs[i] < (1u64 << 51)) ==> (r.limbs =~= limbs),\n            as_nat(r.limbs) == as_nat(limbs) - p() * (limbs[4] >> 51)\n    {\n        proof {\n            lemma_boundaries(limbs);\n            lemma_reduce(limbs);\n        }\n\n        // Since the input limbs are bounded by 2^64, the biggest\n        // carry-out is bounded by 2^13.\n        //\n        // The biggest carry-in is c4 * 19, resulting in\n        //\n        // 2^51 + 19*2^13 < 2^51.0000000001\n        //\n        // Because we don't need to canonicalize, only to reduce the\n        // limb sizes, it's OK to do a \"weak reduction\", where we\n        // compute the carry-outs in parallel.\n\n        let c0 = limbs[0] >> 51;\n        let c1 = limbs[1] >> 51;\n        let c2 = limbs[2] >> 51;\n        let c3 = limbs[3] >> 51;\n        let c4 = limbs[4] >> 51;\n\n        limbs[0] &= LOW_51_BIT_MASK;\n        limbs[1] &= LOW_51_BIT_MASK;\n        limbs[2] &= LOW_51_BIT_MASK;\n        limbs[3] &= LOW_51_BIT_MASK;\n        limbs[4] &= LOW_51_BIT_MASK;\n\n        limbs[0] += c4 * 19;\n        limbs[1] += c0;\n        limbs[2] += c1;\n        limbs[3] += c2;\n        limbs[4] += c3;\n\n        // ADAPTED CODE LINE: limbs is now a named field\n        FieldElement51{limbs}\n    }\n\n    /// Load a `FieldElement51` from the low 255 bits of a 256-bit\n    /// input.\n    ///\n    /// # Warning\n    ///\n    /// This function does not check that the input used the canonical\n    /// representative.  It masks the high bit, but it will happily\n    /// decode 2^255 - 18 to 1.  Applications that require a canonical\n    /// encoding of every field element should decode, re-encode to\n    /// the canonical encoding, and check that the input was\n    /// canonical.\n    ///\n    #[rustfmt::skip] // keep alignment of bit shifts\n    pub const fn from_bytes(bytes: &[u8; 32]) -> (r: FieldElement51)\n        ensures\n            true\n            // TODO:\n            // as_nat(r.limbs) =?= as_nat_32_u8(bytes)\n    {\n        proof {\n            l51_bit_mask_lt() // No over/underflow in the below let-def\n        }\n        let low_51_bit_mask = (1u64 << 51) - 1;\n        // ADAPTED CODE LINE: limbs is now a named field\n        FieldElement51{ limbs:\n        // load bits [  0, 64), no shift\n        [  load8_at(bytes,  0)        & low_51_bit_mask\n        // load bits [ 48,112), shift to [ 51,112)\n        , (load8_at(bytes,  6) >>  3) & low_51_bit_mask\n        // load bits [ 96,160), shift to [102,160)\n        , (load8_at(bytes, 12) >>  6) & low_51_bit_mask\n        // load bits [152,216), shift to [153,216)\n        , (load8_at(bytes, 19) >>  1) & low_51_bit_mask\n        // load bits [192,256), shift to [204,112)\n        , (load8_at(bytes, 24) >> 12) & low_51_bit_mask\n        ]}\n    }\n\n    /// Serialize this `FieldElement51` to a 32-byte array.  The\n    /// encoding is canonical.\n    #[rustfmt::skip] // keep alignment of s[*] calculations\n    #[allow(clippy::wrong_self_convention)]\n    pub fn to_bytes(self) -> (r: [u8; 32])\n        ensures\n            true // No overflow\n            // TODO:\n            // as_nat(self.limbs) =?= as_nat_32_u8(r),\n            // canonical encoding\n            // forall|i: int| 0 <= i < 5 ==> r[i] < (1u64 << 51)\n    {\n        proof {\n            let l = spec_reduce(self.limbs);\n            lemma_reduce(self.limbs);\n\n            let q0 = (l[0] + 19) as u64 >> 51;\n            let q1 = (l[1] + q0) as u64 >> 51;\n            let q2 = (l[2] + q1) as u64 >> 51;\n            let q3 = (l[3] + q2) as u64 >> 51;\n            let q4 = (l[4] + q3) as u64 >> 51;\n\n            assert(19 < (1u64 << 52)) by (bit_vector);\n            lemma_add_then_shift(l[0], 19);\n            lemma_add_then_shift(l[1], q0);\n            lemma_add_then_shift(l[2], q1);\n            lemma_add_then_shift(l[3], q2);\n            lemma_add_then_shift(l[4], q3);\n\n            let l0 = (l[0] + 19 * q4) as u64;\n            let l1 = (l[1] + (l0 >> 51)) as u64;\n            let l2 = (l[2] + (l1 >> 51)) as u64;\n            let l3 = (l[3] + (l2 >> 51)) as u64;\n            let l4 = (l[3] + (l3 >> 51)) as u64;\n\n            assert( 19 * q4 < 1u64 << 7) by {\n                // Explicit values for pow2(k) for k < 64\n                lemma2_to64();\n                shift_is_pow2(5); // now we know 19 < 1u64 << 5 for free\n                shift_is_pow2(2);\n                shift_is_pow2(7);\n                lemma_pow2_adds(5, 2);\n            }\n            assert(((1u64 << 7)) + (1u64 << 52) < (1u64 << 53)) by (bit_vector);\n            assert(((1u64 << 13)) + (1u64 << 52) < (1u64 << 53)) by (bit_vector);\n            shifted_lt(l0);\n            shifted_lt(l1);\n            shifted_lt(l2);\n            shifted_lt(l3);\n\n            l51_bit_mask_lt();\n\n            // TODO\n            // let rr = [\n            //     l0 & LOW_51_BIT_MASK,\n            //     l1 & LOW_51_BIT_MASK,\n            //     l2 & LOW_51_BIT_MASK,\n            //     l3 & LOW_51_BIT_MASK,\n            //     l4 & LOW_51_BIT_MASK\n            // ];\n\n            // let r = [\n            //     rr[0]                           as u8,\n            //     (rr[0] >>  8)                    as u8,\n            //     (rr[0] >> 16)                    as u8,\n            //     (rr[0] >> 24)                    as u8,\n            //     (rr[0] >> 32)                    as u8,\n            //     (rr[0] >> 40)                    as u8,\n            //     ((rr[0] >> 48) | (rr[1] << 3)) as u8,\n            //     (rr[1] >>  5)                    as u8,\n            //     (rr[1] >> 13)                    as u8,\n            //     (rr[1] >> 21)                    as u8,\n            //     (rr[1] >> 29)                    as u8,\n            //     (rr[1] >> 37)                    as u8,\n            //     ((rr[1] >> 45) | (rr[2] << 6)) as u8,\n            //     (rr[2] >>  2)                    as u8,\n            //     (rr[2] >> 10)                    as u8,\n            //     (rr[2] >> 18)                    as u8,\n            //     (rr[2] >> 26)                    as u8,\n            //     (rr[2] >> 34)                    as u8,\n            //     (rr[2] >> 42)                    as u8,\n            //     ((rr[2] >> 50) | (rr[3] << 1)) as u8,\n            //     (rr[3] >>  7)                    as u8,\n            //     (rr[3] >> 15)                    as u8,\n            //     (rr[3] >> 23)                    as u8,\n            //     (rr[3] >> 31)                    as u8,\n            //     (rr[3] >> 39)                    as u8,\n            //     ((rr[3] >> 47) | (rr[4] << 4)) as u8,\n            //     (rr[4] >>  4)                    as u8,\n            //     (rr[4] >> 12)                    as u8,\n            //     (rr[4] >> 20)                    as u8,\n            //     (rr[4] >> 28)                    as u8,\n            //     (rr[4] >> 36)                    as u8,\n            //     (rr[4] >> 44)                    as u8\n            // ];\n\n        }\n        // Let h = limbs[0] + limbs[1]*2^51 + ... + limbs[4]*2^204.\n        //\n        // Write h = pq + r with 0 <= r < p.\n        //\n        // We want to compute r = h mod p.\n        //\n        // If h < 2*p = 2^256 - 38,\n        // then q = 0 or 1,\n        //\n        // with q = 0 when h < p\n        //  and q = 1 when h >= p.\n        //\n        // Notice that h >= p <==> h + 19 >= p + 19 <==> h + 19 >= 2^255.\n        // Therefore q can be computed as the carry bit of h + 19.\n\n        // First, reduce the limbs to ensure h < 2*p.\n        let mut limbs = FieldElement51::reduce(self.limbs).limbs;\n\n        let mut q = (limbs[0] + 19) >> 51;\n        q = (limbs[1] + q) >> 51;\n        q = (limbs[2] + q) >> 51;\n        q = (limbs[3] + q) >> 51;\n        q = (limbs[4] + q) >> 51;\n\n        // Now we can compute r as r = h - pq = r - (2^255-19)q = r + 19q - 2^255q\n\n        limbs[0] += 19 * q;\n\n        // Now carry the result to compute r + 19q ...\n        let low_51_bit_mask = (1u64 << 51) - 1;\n        limbs[1] += limbs[0] >> 51;\n        limbs[0] &= low_51_bit_mask;\n        limbs[2] += limbs[1] >> 51;\n        limbs[1] &= low_51_bit_mask;\n        limbs[3] += limbs[2] >> 51;\n        limbs[2] &= low_51_bit_mask;\n        limbs[4] += limbs[3] >> 51;\n        limbs[3] &= low_51_bit_mask;\n        // ... but instead of carrying (limbs[4] >> 51) = 2^255q\n        // into another limb, discard it, subtracting the value\n        limbs[4] &= low_51_bit_mask;\n\n        // Now arrange the bits of the limbs.\n        let mut s = [0u8;32];\n        s[ 0] =   limbs[0]                           as u8;\n        s[ 1] =  (limbs[0] >>  8)                    as u8;\n        s[ 2] =  (limbs[0] >> 16)                    as u8;\n        s[ 3] =  (limbs[0] >> 24)                    as u8;\n        s[ 4] =  (limbs[0] >> 32)                    as u8;\n        s[ 5] =  (limbs[0] >> 40)                    as u8;\n        s[ 6] = ((limbs[0] >> 48) | (limbs[1] << 3)) as u8;\n        s[ 7] =  (limbs[1] >>  5)                    as u8;\n        s[ 8] =  (limbs[1] >> 13)                    as u8;\n        s[ 9] =  (limbs[1] >> 21)                    as u8;\n        s[10] =  (limbs[1] >> 29)                    as u8;\n        s[11] =  (limbs[1] >> 37)                    as u8;\n        s[12] = ((limbs[1] >> 45) | (limbs[2] << 6)) as u8;\n        s[13] =  (limbs[2] >>  2)                    as u8;\n        s[14] =  (limbs[2] >> 10)                    as u8;\n        s[15] =  (limbs[2] >> 18)                    as u8;\n        s[16] =  (limbs[2] >> 26)                    as u8;\n        s[17] =  (limbs[2] >> 34)                    as u8;\n        s[18] =  (limbs[2] >> 42)                    as u8;\n        s[19] = ((limbs[2] >> 50) | (limbs[3] << 1)) as u8;\n        s[20] =  (limbs[3] >>  7)                    as u8;\n        s[21] =  (limbs[3] >> 15)                    as u8;\n        s[22] =  (limbs[3] >> 23)                    as u8;\n        s[23] =  (limbs[3] >> 31)                    as u8;\n        s[24] =  (limbs[3] >> 39)                    as u8;\n        s[25] = ((limbs[3] >> 47) | (limbs[4] << 4)) as u8;\n        s[26] =  (limbs[4] >>  4)                    as u8;\n        s[27] =  (limbs[4] >> 12)                    as u8;\n        s[28] =  (limbs[4] >> 20)                    as u8;\n        s[29] =  (limbs[4] >> 28)                    as u8;\n        s[30] =  (limbs[4] >> 36)                    as u8;\n        s[31] =  (limbs[4] >> 44)                    as u8;\n\n        // High bit should be zero.\n        // DISABLED DUE TO NO VERUS SUPPORT FOR PANICS\n        // debug_assert!((s[31] & 0b1000_0000u8) == 0u8);\n\n        s\n    }\n\n    /// Given `k > 0`, return `self^(2^k)`.\n    #[rustfmt::skip] // keep alignment of c* calculations\n    pub fn pow2k(&self, mut k: u32) -> (r: FieldElement51)\n        ensures\n            true\n    {\n        // DISABLED DUE TO NO VERUS SUPPORT FOR PANICS\n        // debug_assert!( k > 0 );\n\n\n        let mut a: [u64; 5] = self.limbs;\n\n        loop\n            invariant\n                true\n            decreases k\n        {\n            proof {\n                assume(false);\n            }\n            // Precondition: assume input limbs a[i] are bounded as\n            //\n            // a[i] < 2^(51 + b)\n            //\n            // where b is a real parameter measuring the \"bit excess\" of the limbs.\n\n            // Precomputation: 64-bit multiply by 19.\n            //\n            // This fits into a u64 whenever 51 + b + lg(19) < 64.\n            //\n            // Since 51 + b + lg(19) < 51 + 4.25 + b\n            //                       = 55.25 + b,\n            // this fits if b < 8.75.\n            let a3_19 = 19 * a[3];\n            let a4_19 = 19 * a[4];\n\n            // Multiply to get 128-bit coefficients of output.\n            //\n            // The 128-bit multiplications by 2 turn into 1 slr + 1 slrd each,\n            // which doesn't seem any better or worse than doing them as precomputations\n            // on the 64-bit inputs.\n            let     c0: u128 = m(a[0],  a[0]) + 2*( m(a[1], a4_19) + m(a[2], a3_19) );\n            let mut c1: u128 = m(a[3], a3_19) + 2*( m(a[0],  a[1]) + m(a[2], a4_19) );\n            let mut c2: u128 = m(a[1],  a[1]) + 2*( m(a[0],  a[2]) + m(a[4], a3_19) );\n            let mut c3: u128 = m(a[4], a4_19) + 2*( m(a[0],  a[3]) + m(a[1],  a[2]) );\n            let mut c4: u128 = m(a[2],  a[2]) + 2*( m(a[0],  a[4]) + m(a[1],  a[3]) );\n\n            // Same bound as in multiply:\n            //    c[i] < 2^(102 + 2*b) * (1+i + (4-i)*19)\n            //         < 2^(102 + lg(1 + 4*19) + 2*b)\n            //         < 2^(108.27 + 2*b)\n            //\n            // The carry (c[i] >> 51) fits into a u64 when\n            //    108.27 + 2*b - 51 < 64\n            //    2*b < 6.73\n            //    b < 3.365.\n            //\n            // So we require b < 3 to ensure this fits.\n            // DISABLED DUE TO NO VERUS SUPPORT FOR PANICS\n            // debug_assert!(a[0] < (1 << 54));\n            // debug_assert!(a[1] < (1 << 54));\n            // debug_assert!(a[2] < (1 << 54));\n            // debug_assert!(a[3] < (1 << 54));\n            // debug_assert!(a[4] < (1 << 54));\n\n            // const LOW_51_BIT_MASK: u64 = (1u64 << 51) - 1; // already defined\n\n            // Casting to u64 and back tells the compiler that the carry is bounded by 2^64, so\n            // that the addition is a u128 + u64 rather than u128 + u128.\n            c1 += ((c0 >> 51) as u64) as u128;\n            a[0] = (c0 as u64) & LOW_51_BIT_MASK;\n\n            c2 += ((c1 >> 51) as u64) as u128;\n            a[1] = (c1 as u64) & LOW_51_BIT_MASK;\n\n            c3 += ((c2 >> 51) as u64) as u128;\n            a[2] = (c2 as u64) & LOW_51_BIT_MASK;\n\n            c4 += ((c3 >> 51) as u64) as u128;\n            a[3] = (c3 as u64) & LOW_51_BIT_MASK;\n\n            let carry: u64 = (c4 >> 51) as u64;\n            a[4] = (c4 as u64) & LOW_51_BIT_MASK;\n\n            // To see that this does not overflow, we need a[0] + carry * 19 < 2^64.\n            //\n            // c4 < a2^2 + 2*a0*a4 + 2*a1*a3 + (carry from c3)\n            //    < 2^(102 + 2*b + lg(5)) + 2^64.\n            //\n            // When b < 3 we get\n            //\n            // c4 < 2^110.33  so that carry < 2^59.33\n            //\n            // so that\n            //\n            // a[0] + carry * 19 < 2^51 + 19 * 2^59.33 < 2^63.58\n            //\n            // and there is no overflow.\n            a[0] += carry * 19;\n\n            // Now a[1] < 2^51 + 2^(64 -51) = 2^51 + 2^13 < 2^(51 + epsilon).\n            a[1] += a[0] >> 51;\n            a[0] &= LOW_51_BIT_MASK;\n\n            // Now all a[i] < 2^(51 + epsilon) and a = self^(2^k).\n\n            k -= 1;\n            if k == 0 {\n                break;\n            }\n        }\n\n        // ADAPTED CODE LINE: limbs is now a named field\n        FieldElement51{limbs: a}\n    }\n\n    /// Returns the square of this field element.\n    pub fn square(&self) -> FieldElement51 {\n        self.pow2k(1)\n    }\n\n    /// Returns 2 times the square of this field element.\n    pub fn square2(&self) -> (r: FieldElement51)\n        ensures\n            true\n    {\n        let mut square = self.pow2k(1);\n        for i in 0..5 {\n            proof {\n                assume(false);\n            }\n            square.limbs[i] *= 2;\n        }\n\n        square\n    }\n}\n\nfn main()\n{}\n\n}\n",
    "filename": "src/field_verus.rs",
    "filepath": null,
    "folder_id": 12249,
    "user_id": 460179
  },
  "51721": {
    "text": "mod field_verus;\nmod scalar_verus;\n",
    "filename": "src/lib.rs",
    "filepath": null,
    "folder_id": 12249,
    "user_id": 460179
  }
}